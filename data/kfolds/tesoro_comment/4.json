{"id":28,"comment_id":0,"comment":"\/\/todo: for each model create separate implementation.","code":"@override public void process(jcas jcas) throws analysisengineprocessexception { long starttime = system.currenttimemillis(); featureextractor fe = new nytentitysaliencefeatureextractor(); list<entityinstance> entityinstances; try { entityinstances = fe.getentityinstances(jcas, trainingsettings.featureextractor.entity_salience); final int featurevectorsize = featuresetfactory.createfeatureset(trainingsettings.featureextractor.entity_salience).getfeaturevectorsize(); \/\/todo: for each model create separate implementation. randomforestclassificationmodel rfm = (randomforestclassificationmodel)trainingmodel.stages()[2]; for(entityinstance ei : entityinstances) { vector vei = featurevalueinstanceutils.converttosparkmlvector(ei, featurevectorsize); double label = rfm.predict(vei); vector probabilities = rfm.predictprobability(vei); double salience = probabilities.toarray()[1]; saliententity saliententity = new saliententity(jcas, 0, 0); saliententity.setlabel(label); saliententity.setid(ei.getentityid()); saliententity.setsalience(salience); saliententity.addtoindexes(); } long endtime = system.currenttimemillis() - starttime; logger.debug(\"annotating salient entities finished in {}ms.\", endtime); } catch (exception e) { throw new analysisengineprocessexception(e); } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"entityinstances = fe.getentityinstances(jcas, trainingsettings.featureextractor.entity_salience); final int featurevectorsize = featuresetfactory.createfeatureset(trainingsettings.featureextractor.entity_salience).getfeaturevectorsize(); \/\/todo: for each model create separate implementation. randomforestclassificationmodel rfm = (randomforestclassificationmodel)trainingmodel.stages()[2]; for(entityinstance ei : entityinstances) {","code_context_10":"@override public void process(jcas jcas) throws analysisengineprocessexception { long starttime = system.currenttimemillis(); featureextractor fe = new nytentitysaliencefeatureextractor(); list<entityinstance> entityinstances; try { entityinstances = fe.getentityinstances(jcas, trainingsettings.featureextractor.entity_salience); final int featurevectorsize = featuresetfactory.createfeatureset(trainingsettings.featureextractor.entity_salience).getfeaturevectorsize(); \/\/todo: for each model create separate implementation. randomforestclassificationmodel rfm = (randomforestclassificationmodel)trainingmodel.stages()[2]; for(entityinstance ei : entityinstances) { vector vei = featurevalueinstanceutils.converttosparkmlvector(ei, featurevectorsize); double label = rfm.predict(vei); vector probabilities = rfm.predictprobability(vei); double salience = probabilities.toarray()[1]; saliententity saliententity = new saliententity(jcas, 0, 0); saliententity.setlabel(label); saliententity.setid(ei.getentityid()); saliententity.setsalience(salience);","code_context_20":"@override public void process(jcas jcas) throws analysisengineprocessexception { long starttime = system.currenttimemillis(); featureextractor fe = new nytentitysaliencefeatureextractor(); list<entityinstance> entityinstances; try { entityinstances = fe.getentityinstances(jcas, trainingsettings.featureextractor.entity_salience); final int featurevectorsize = featuresetfactory.createfeatureset(trainingsettings.featureextractor.entity_salience).getfeaturevectorsize(); \/\/todo: for each model create separate implementation. randomforestclassificationmodel rfm = (randomforestclassificationmodel)trainingmodel.stages()[2]; for(entityinstance ei : entityinstances) { vector vei = featurevalueinstanceutils.converttosparkmlvector(ei, featurevectorsize); double label = rfm.predict(vei); vector probabilities = rfm.predictprobability(vei); double salience = probabilities.toarray()[1]; saliententity saliententity = new saliententity(jcas, 0, 0); saliententity.setlabel(label); saliententity.setid(ei.getentityid()); saliententity.setsalience(salience); saliententity.addtoindexes(); } long endtime = system.currenttimemillis() - starttime; logger.debug(\"annotating salient entities finished in {}ms.\", endtime); } catch (exception e) { throw new analysisengineprocessexception(e); } }","repo":"dmilcevski\/ambiverse-nlu"}
{"id":32,"comment_id":0,"comment":"\/\/ todo: check here, may be necessary to remove the last space.","code":"private void generatearttribute() { string temp = \"\"; for (string s : connectedelements) temp = temp + s + \" \"; \/\/ todo: check here, may be necessary to remove the last space. linkingresourceelement.setattribute( \"connectedresourcecontainers_linkingresource\", temp); }","classification":"DESIGN","isFinished":true,"code_context_2":"for (string s : connectedelements) temp = temp + s + \" \"; \/\/ todo: check here, may be necessary to remove the last space. linkingresourceelement.setattribute( \"connectedresourcecontainers_linkingresource\", temp);","code_context_10":"private void generatearttribute() { string temp = \"\"; for (string s : connectedelements) temp = temp + s + \" \"; \/\/ todo: check here, may be necessary to remove the last space. linkingresourceelement.setattribute( \"connectedresourcecontainers_linkingresource\", temp); }","code_context_20":"private void generatearttribute() { string temp = \"\"; for (string s : connectedelements) temp = temp + s + \" \"; \/\/ todo: check here, may be necessary to remove the last space. linkingresourceelement.setattribute( \"connectedresourcecontainers_linkingresource\", temp); }","repo":"deib-polimi\/modaclouds-space4cloud"}
{"id":100,"comment_id":0,"comment":"\/\/ todo auto-generated method stub \/\/ return super.hasrole(principal, roleidentifier);","code":"@override public boolean hasrole(final principalcollection principal, final string roleidentifier) { \/\/ todo auto-generated method stub \/\/ return super.hasrole(principal, roleidentifier); for (final object p : principal.fromrealm(realm_name)) { if (p instanceof genericprincipal) { final genericprincipal gp = (genericprincipal) p; for (final string r : gp.getroles()) { if (r.equals(roleidentifier)) { return true; } } } } return false; }","classification":"DESIGN","isFinished":true,"code_context_2":"@override public boolean hasrole(final principalcollection principal, final string roleidentifier) { \/\/ todo auto-generated method stub \/\/ return super.hasrole(principal, roleidentifier); for (final object p : principal.fromrealm(realm_name)) { if (p instanceof genericprincipal) {","code_context_10":"@override public boolean hasrole(final principalcollection principal, final string roleidentifier) { \/\/ todo auto-generated method stub \/\/ return super.hasrole(principal, roleidentifier); for (final object p : principal.fromrealm(realm_name)) { if (p instanceof genericprincipal) { final genericprincipal gp = (genericprincipal) p; for (final string r : gp.getroles()) { if (r.equals(roleidentifier)) { return true; } } } }","code_context_20":"@override public boolean hasrole(final principalcollection principal, final string roleidentifier) { \/\/ todo auto-generated method stub \/\/ return super.hasrole(principal, roleidentifier); for (final object p : principal.fromrealm(realm_name)) { if (p instanceof genericprincipal) { final genericprincipal gp = (genericprincipal) p; for (final string r : gp.getroles()) { if (r.equals(roleidentifier)) { return true; } } } } return false; }","repo":"danieljue\/graphene"}
{"id":8408,"comment_id":0,"comment":"\/\/ todo load all expected impl classes, allowing fail fast rather than waiting for user to hit a certain use case","code":"private synchronized static void initializeimplclasses() { log.trace(\"exec\"); \/\/ todo load all expected impl classes, allowing fail fast rather than waiting for user to hit a certain use case }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"private synchronized static void initializeimplclasses() { log.trace(\"exec\"); \/\/ todo load all expected impl classes, allowing fail fast rather than waiting for user to hit a certain use case }","code_context_10":"private synchronized static void initializeimplclasses() { log.trace(\"exec\"); \/\/ todo load all expected impl classes, allowing fail fast rather than waiting for user to hit a certain use case }","code_context_20":"private synchronized static void initializeimplclasses() { log.trace(\"exec\"); \/\/ todo load all expected impl classes, allowing fail fast rather than waiting for user to hit a certain use case }","repo":"esasiela\/hc-log4tri"}
{"id":8468,"comment_id":0,"comment":"\/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology.","code":"private progresstracker createprogresstracker() { readoperation readoperation; remotegrpcportwriteoperation grpcwriteoperation; registerandprocessbundleoperation bundleprocessoperation; try { readoperation = getreadoperation(); } catch (exception exn) { readoperation = null; log.info(\"unable to get read operation.\", exn); return new nullprogresstracker(); } \/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation = iterables.getonlyelement( iterables.filter(operations, remotegrpcportwriteoperation.class)); bundleprocessoperation = iterables.getonlyelement( iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null; bundleprocessoperation = null; log.debug(\"does not have exactly one grpcwrite and bundleprocess operation.\", exn); } if (grpcwriteoperation != null && bundleprocessoperation != null) { return new singularprocessbundleprogresstracker( readoperation, grpcwriteoperation, bundleprocessoperation); } else { return new readoperationprogresstracker(readoperation); } }","classification":"NONSATD","isFinished":true,"code_context_2":"return new nullprogresstracker(); } \/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation =","code_context_10":"readoperation readoperation; remotegrpcportwriteoperation grpcwriteoperation; registerandprocessbundleoperation bundleprocessoperation; try { readoperation = getreadoperation(); } catch (exception exn) { readoperation = null; log.info(\"unable to get read operation.\", exn); return new nullprogresstracker(); } \/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation = iterables.getonlyelement( iterables.filter(operations, remotegrpcportwriteoperation.class)); bundleprocessoperation = iterables.getonlyelement( iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null;","code_context_20":"private progresstracker createprogresstracker() { readoperation readoperation; remotegrpcportwriteoperation grpcwriteoperation; registerandprocessbundleoperation bundleprocessoperation; try { readoperation = getreadoperation(); } catch (exception exn) { readoperation = null; log.info(\"unable to get read operation.\", exn); return new nullprogresstracker(); } \/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation = iterables.getonlyelement( iterables.filter(operations, remotegrpcportwriteoperation.class)); bundleprocessoperation = iterables.getonlyelement( iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null; bundleprocessoperation = null; log.debug(\"does not have exactly one grpcwrite and bundleprocess operation.\", exn); } if (grpcwriteoperation != null && bundleprocessoperation != null) { return new singularprocessbundleprogresstracker( readoperation, grpcwriteoperation, bundleprocessoperation); } else { return new readoperationprogresstracker(readoperation); } }","repo":"elwinarens\/beam"}
{"id":8468,"comment_id":1,"comment":"\/\/ todo: handle more than one sdk worker processing a single bundle.","code":"private progresstracker createprogresstracker() { readoperation readoperation; remotegrpcportwriteoperation grpcwriteoperation; registerandprocessbundleoperation bundleprocessoperation; try { readoperation = getreadoperation(); } catch (exception exn) { readoperation = null; log.info(\"unable to get read operation.\", exn); return new nullprogresstracker(); } \/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation = iterables.getonlyelement( iterables.filter(operations, remotegrpcportwriteoperation.class)); bundleprocessoperation = iterables.getonlyelement( iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null; bundleprocessoperation = null; log.debug(\"does not have exactly one grpcwrite and bundleprocess operation.\", exn); } if (grpcwriteoperation != null && bundleprocessoperation != null) { return new singularprocessbundleprogresstracker( readoperation, grpcwriteoperation, bundleprocessoperation); } else { return new readoperationprogresstracker(readoperation); } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null; bundleprocessoperation = null;","code_context_10":"\/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation = iterables.getonlyelement( iterables.filter(operations, remotegrpcportwriteoperation.class)); bundleprocessoperation = iterables.getonlyelement( iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null; bundleprocessoperation = null; log.debug(\"does not have exactly one grpcwrite and bundleprocess operation.\", exn); } if (grpcwriteoperation != null && bundleprocessoperation != null) { return new singularprocessbundleprogresstracker( readoperation, grpcwriteoperation, bundleprocessoperation); } else { return new readoperationprogresstracker(readoperation); }","code_context_20":"readoperation readoperation; remotegrpcportwriteoperation grpcwriteoperation; registerandprocessbundleoperation bundleprocessoperation; try { readoperation = getreadoperation(); } catch (exception exn) { readoperation = null; log.info(\"unable to get read operation.\", exn); return new nullprogresstracker(); } \/\/ if there is a exactly one of each of remotegrpcportwriteoperation and \/\/ registerandprocessbundleoperation we know they have the right topology. try { grpcwriteoperation = iterables.getonlyelement( iterables.filter(operations, remotegrpcportwriteoperation.class)); bundleprocessoperation = iterables.getonlyelement( iterables.filter(operations, registerandprocessbundleoperation.class)); } catch (illegalargumentexception | nosuchelementexception exn) { \/\/ todo: handle more than one sdk worker processing a single bundle. grpcwriteoperation = null; bundleprocessoperation = null; log.debug(\"does not have exactly one grpcwrite and bundleprocess operation.\", exn); } if (grpcwriteoperation != null && bundleprocessoperation != null) { return new singularprocessbundleprogresstracker( readoperation, grpcwriteoperation, bundleprocessoperation); } else { return new readoperationprogresstracker(readoperation); } }","repo":"elwinarens\/beam"}
{"id":16671,"comment_id":0,"comment":"\/\/todo - could we also check if the elevator is up and limit speed?","code":"@override public void execute() { double y = -constants.joystick.gety(); double z = constants.joystick.getz(); \/\/todo - could we also check if the elevator is up and limit speed? if(y < constants.reverse_max_speed) { y = constants.reverse_max_speed; } if(z < constants.left_max_speed ) { z = constants.left_max_speed; } else if(z > constants.right_max_speed){ z = constants.right_max_speed; } drivesubsystem.getdiffdrive().arcadedrive(y, z); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"double y = -constants.joystick.gety(); double z = constants.joystick.getz(); \/\/todo - could we also check if the elevator is up and limit speed? if(y < constants.reverse_max_speed) { y = constants.reverse_max_speed;","code_context_10":"@override public void execute() { double y = -constants.joystick.gety(); double z = constants.joystick.getz(); \/\/todo - could we also check if the elevator is up and limit speed? if(y < constants.reverse_max_speed) { y = constants.reverse_max_speed; } if(z < constants.left_max_speed ) { z = constants.left_max_speed; } else if(z > constants.right_max_speed){ z = constants.right_max_speed; } drivesubsystem.getdiffdrive().arcadedrive(y, z); }","code_context_20":"@override public void execute() { double y = -constants.joystick.gety(); double z = constants.joystick.getz(); \/\/todo - could we also check if the elevator is up and limit speed? if(y < constants.reverse_max_speed) { y = constants.reverse_max_speed; } if(z < constants.left_max_speed ) { z = constants.left_max_speed; } else if(z > constants.right_max_speed){ z = constants.right_max_speed; } drivesubsystem.getdiffdrive().arcadedrive(y, z); }","repo":"frc5826\/2022-production"}
{"id":8547,"comment_id":0,"comment":"\/** deletes all items in the given path list followed by all bucket items. *\/","code":"\/** deletes all items in the given path list followed by all bucket items. *\/ private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } } }","code_context_10":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } } }","code_context_20":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } } }","repo":"danielxjd\/hadoop-connectors"}
{"id":8547,"comment_id":1,"comment":"\/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future.","code":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } } }","classification":"DESIGN","isFinished":true,"code_context_2":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) {","code_context_10":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration()));","code_context_20":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames);","repo":"danielxjd\/hadoop-connectors"}
{"id":8547,"comment_id":2,"comment":"\/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos?","code":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid(","code_context_10":"\/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size());","code_context_20":"private void deleteinternal(list<fileinfo> itemstodelete, list<fileinfo> bucketstodelete) throws ioexception { \/\/ todo(user): we might need to separate out children into separate batches from parents to \/\/ avoid deleting a parent before somehow failing to delete a child. \/\/ delete children before their parents. \/\/ \/\/ note: we modify the input list, which is ok for current usage. \/\/ we should make a copy in case that changes in future. itemstodelete.sort(file_info_path_comparator.reversed()); if (!itemstodelete.isempty()) { list<storageresourceid> objectstodelete = new arraylist<>(itemstodelete.size()); for (fileinfo fileinfo : itemstodelete) { \/\/ todo(b\/110833109): populate generation id in storageresourceid when listing infos? objectstodelete.add( new storageresourceid( fileinfo.getiteminfo().getbucketname(), fileinfo.getiteminfo().getobjectname(), fileinfo.getiteminfo().getcontentgeneration())); } gcs.deleteobjects(objectstodelete); } if (!bucketstodelete.isempty()) { list<string> bucketnames = new arraylist<>(bucketstodelete.size()); for (fileinfo bucketinfo : bucketstodelete) { bucketnames.add(bucketinfo.getiteminfo().getresourceid().getbucketname()); } if (options.isbucketdeleteenabled()) { gcs.deletebuckets(bucketnames); } else { logger.atinfo().log( \"skipping deletion of buckets because enablebucketdelete is false: %s\", bucketnames); } }","repo":"danielxjd\/hadoop-connectors"}
{"id":16844,"comment_id":0,"comment":"\/\/ todo: script tests, should fail with defaultvaluessourcetype disabled.","code":"\/\/ todo: script tests, should fail with defaultvaluessourcetype disabled. public void testempty() throws ioexception { final mappedfieldtype ft = new numberfieldmapper.numberfieldtype(\"field\", numbertype.long); testcase( stats(\"_name\").field(ft.name()), iw -> {}, stats -> { assertequals(0d, stats.getcount(), 0); assertequals(0d, stats.getsum(), 0); assertequals(float.nan, stats.getavg(), 0); assertequals(double.positive_infinity, stats.getmin(), 0); assertequals(double.negative_infinity, stats.getmax(), 0); assertfalse(aggregationinspectionhelper.hasvalue(stats)); }, singleton(ft) ); }","classification":"TEST","isFinished":true,"code_context_2":"public void testempty() throws ioexception { final mappedfieldtype ft = new numberfieldmapper.numberfieldtype(\"field\", numbertype.long); testcase( stats(\"_name\").field(ft.name()), iw -> {}, stats -> { assertequals(0d, stats.getcount(), 0); assertequals(0d, stats.getsum(), 0); assertequals(float.nan, stats.getavg(), 0); assertequals(double.positive_infinity, stats.getmin(), 0); assertequals(double.negative_infinity, stats.getmax(), 0); assertfalse(aggregationinspectionhelper.hasvalue(stats)); }, singleton(ft) ); }","code_context_10":"public void testempty() throws ioexception { final mappedfieldtype ft = new numberfieldmapper.numberfieldtype(\"field\", numbertype.long); testcase( stats(\"_name\").field(ft.name()), iw -> {}, stats -> { assertequals(0d, stats.getcount(), 0); assertequals(0d, stats.getsum(), 0); assertequals(float.nan, stats.getavg(), 0); assertequals(double.positive_infinity, stats.getmin(), 0); assertequals(double.negative_infinity, stats.getmax(), 0); assertfalse(aggregationinspectionhelper.hasvalue(stats)); }, singleton(ft) ); }","code_context_20":"public void testempty() throws ioexception { final mappedfieldtype ft = new numberfieldmapper.numberfieldtype(\"field\", numbertype.long); testcase( stats(\"_name\").field(ft.name()), iw -> {}, stats -> { assertequals(0d, stats.getcount(), 0); assertequals(0d, stats.getsum(), 0); assertequals(float.nan, stats.getavg(), 0); assertequals(double.positive_infinity, stats.getmin(), 0); assertequals(double.negative_infinity, stats.getmax(), 0); assertfalse(aggregationinspectionhelper.hasvalue(stats)); }, singleton(ft) ); }","repo":"dial-workable\/elasticsearch"}
{"id":521,"comment_id":0,"comment":"\/\/ plain test","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access","code_context_10":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps","code_context_20":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\");","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":1,"comment":"\/\/ numeric field access","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access","code_context_10":"int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times","code_context_20":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\");","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":2,"comment":"\/\/ string field access","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list","code_context_10":".put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections","code_context_20":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")]","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":3,"comment":"\/\/ list","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges","code_context_10":".build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases:","code_context_20":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\");","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":4,"comment":"\/\/ ranges","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps","code_context_10":"client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\");","code_context_20":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" +","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":5,"comment":"\/\/ maps","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times","code_context_10":"\/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\");","code_context_20":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")]","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":6,"comment":"\/\/ times","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections","code_context_10":"\/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\");","code_context_20":"int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")]","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":7,"comment":"\/\/ groovycollections","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases:","code_context_10":"\/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")]","code_context_20":".put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not.","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":8,"comment":"\/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")]","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")]","code_context_10":"\/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")]","code_context_20":".build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) {","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":9,"comment":"\/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")]","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" +","code_context_10":"assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\");","code_context_20":"client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\");","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":10,"comment":"\/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")]","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\");","code_context_10":"\/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not.","code_context_20":"\/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":11,"comment":"\/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")]","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\");","code_context_10":"\/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) {","code_context_20":"assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":12,"comment":"\/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")]","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")]","code_context_10":"assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")]","code_context_20":"internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) {","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":13,"comment":"\/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")]","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not.","code_context_10":"\"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","code_context_20":"\/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":14,"comment":"\/\/ test a directory we normally have access to, but the groovy script does not.","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :)","code_context_10":"\/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","code_context_20":"\/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":15,"comment":"\/\/ todo: figure out the necessary escaping for windows paths here :)","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"DESIGN","isFinished":true,"code_context_2":"\/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\")","code_context_10":"assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","code_context_20":"\/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","repo":"drewr\/elasticsearch"}
{"id":521,"comment_id":16,"comment":"\/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\")","code":"public void testevilgroovyscripts() throws exception { int nodes = randomintbetween(1, 3); settings nodesettings = settings.builder() .put(\"script.inline\", true) .put(\"script.indexed\", true) .build(); internalcluster().startnodesasync(nodes, nodesettings).get(); client().admin().cluster().preparehealth().setwaitfornodes(nodes + \"\").get(); client().prepareindex(\"test\", \"doc\", \"1\").setsource(\"foo\", 5, \"bar\", \"baz\").setrefresh(true).get(); \/\/ plain test assertsuccess(\"\"); \/\/ numeric field access assertsuccess(\"def foo = doc['foo'].value; if (foo == null) { return 5; }\"); \/\/ string field access assertsuccess(\"def bar = doc['bar'].value; if (bar == null) { return 5; }\"); \/\/ list assertsuccess(\"def list = [doc['foo'].value, 3, 4]; def v = list.get(1); list.add(10)\"); \/\/ ranges assertsuccess(\"def range = 1..doc['foo'].value; def v = range.get(0)\"); \/\/ maps assertsuccess(\"def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)\"); \/\/ times assertsuccess(\"def t = instant.now().getmillis()\"); \/\/ groovycollections assertsuccess(\"def n = [1,2,3]; groovycollections.max(n)\"); \/\/ fail cases: \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); }","code_context_10":"assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","code_context_20":"assertfailure(\"pr = runtime.getruntime().exec(\\\"touch \/tmp\/gotcha\\\"); pr.waitfor()\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessclassinpackage.sun.reflect\")] assertfailure(\"d = new datetime(); d.getclass().getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); assertfailure(\"d = new datetime(); d.\\\"${'get' + 'class'}\\\"().\" + \"\\\"${'getdeclared' + 'method'}\\\"(\\\"year\\\").\\\"${'set' + 'accessible'}\\\"(false)\"); assertfailure(\"class.forname(\\\"org.joda.time.datetime\\\").getdeclaredmethod(\\\"year\\\").setaccessible(true)\"); \/\/ accesscontrolexception[access denied (\"groovy.security.groovycodesourcepermission\" \"\/groovy\/shell\")] assertfailure(\"eval.me('2 + 2')\"); assertfailure(\"eval.x(5, 'x + 2')\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"accessdeclaredmembers\")] assertfailure(\"d = new date(); java.lang.reflect.field f = date.class.getdeclaredfield(\\\"fasttime\\\");\" + \" f.setaccessible(true); f.get(\\\"fasttime\\\")\"); \/\/ accesscontrolexception[access denied (\"java.io.filepermission\" \"<<all files>>\" \"execute\")] assertfailure(\"def methodname = 'ex'; runtime.\\\"${'get' + 'runtime'}\\\"().\\\"${methodname}ec\\\"(\\\"touch \/tmp\/gotcha2\\\")\"); \/\/ accesscontrolexception[access denied (\"java.lang.runtimepermission\" \"modifythreadgroup\")] assertfailure(\"t = new thread({ println 3 });\"); \/\/ test a directory we normally have access to, but the groovy script does not. path dir = createtempdir(); \/\/ todo: figure out the necessary escaping for windows paths here :) if (!constants.windows) { \/\/ access denied (\"java.io.filepermission\" \"...\/tempdir-00n\" \"read\") assertfailure(\"new file(\\\"\" + dir + \"\\\").exists()\"); } }","repo":"drewr\/elasticsearch"}
{"id":33296,"comment_id":0,"comment":"\/* this really shouldn't happen *\/","code":"private byte[] md5(string data) { try { return this.getmd5digest().digest(data.getbytes(\"utf-8\")); } catch (unsupportedencodingexception e) { \/* this really shouldn't happen *\/ throw new runtimeexception(e); } }","classification":"DESIGN","isFinished":true,"code_context_2":"return this.getmd5digest().digest(data.getbytes(\"utf-8\")); } catch (unsupportedencodingexception e) { \/* this really shouldn't happen *\/ throw new runtimeexception(e); }","code_context_10":"private byte[] md5(string data) { try { return this.getmd5digest().digest(data.getbytes(\"utf-8\")); } catch (unsupportedencodingexception e) { \/* this really shouldn't happen *\/ throw new runtimeexception(e); } }","code_context_20":"private byte[] md5(string data) { try { return this.getmd5digest().digest(data.getbytes(\"utf-8\")); } catch (unsupportedencodingexception e) { \/* this really shouldn't happen *\/ throw new runtimeexception(e); } }","repo":"dontdrinkandroot\/cache.java"}
{"id":574,"comment_id":0,"comment":"\/\/ todo: customize your mock logic for s3 client","code":"@before public void setup() throws ioexception { event = testutils.parse(\"\/s3-event.put.json\", s3event.class); \/\/ todo: customize your mock logic for s3 client objectmetadata objectmetadata = new objectmetadata(); objectmetadata.setcontenttype(content_type); when(s3object.getobjectmetadata()).thenreturn(objectmetadata); when(s3client.getobject(getobjectrequest.capture())).thenreturn(s3object); }","classification":"DESIGN","isFinished":true,"code_context_2":"public void setup() throws ioexception { event = testutils.parse(\"\/s3-event.put.json\", s3event.class); \/\/ todo: customize your mock logic for s3 client objectmetadata objectmetadata = new objectmetadata(); objectmetadata.setcontenttype(content_type);","code_context_10":"@before public void setup() throws ioexception { event = testutils.parse(\"\/s3-event.put.json\", s3event.class); \/\/ todo: customize your mock logic for s3 client objectmetadata objectmetadata = new objectmetadata(); objectmetadata.setcontenttype(content_type); when(s3object.getobjectmetadata()).thenreturn(objectmetadata); when(s3client.getobject(getobjectrequest.capture())).thenreturn(s3object); }","code_context_20":"@before public void setup() throws ioexception { event = testutils.parse(\"\/s3-event.put.json\", s3event.class); \/\/ todo: customize your mock logic for s3 client objectmetadata objectmetadata = new objectmetadata(); objectmetadata.setcontenttype(content_type); when(s3object.getobjectmetadata()).thenreturn(objectmetadata); when(s3client.getobject(getobjectrequest.capture())).thenreturn(s3object); }","repo":"cpsloSecondScreen\/SecondScreen"}
{"id":16997,"comment_id":0,"comment":"\/** * handler for an embedded member. * @param mmds chain of member metadata to the embedded member * @param embcmd class metadata for the embedded member type * @param clr classloader resolver * @param embmd any embeddedmetadata defining column info * @param ownernested whether the owner is nested *\/","code":"\/** * handler for an embedded member. * @param mmds chain of member metadata to the embedded member * @param embcmd class metadata for the embedded member type * @param clr classloader resolver * @param embmd any embeddedmetadata defining column info * @param ownernested whether the owner is nested *\/ protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","code_context_10":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","code_context_20":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":1,"comment":"\/\/ go through all members of the embedded type","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++)","code_context_10":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname()))","code_context_20":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) {","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":2,"comment":"\/\/ don't need column if not persistent","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; }","code_context_10":"metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) {","code_context_20":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":3,"comment":"\/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; }","code_context_10":"for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) {","code_context_20":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd))","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":4,"comment":"\/\/ nested embedded pc, so recurse","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested))","code_context_10":"break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } }","code_context_20":"if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true);","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":5,"comment":"\/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0);","code_context_10":"string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) {","code_context_20":"relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":6,"comment":"\/\/ todo create mapping for the related info under the above column","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"} mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member","code_context_10":"{ col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested))","code_context_20":"} else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) {","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":7,"comment":"\/\/ recurse through the embedded member","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); }","code_context_10":"} membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds);","code_context_20":"{ col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":8,"comment":"\/\/ nested embedded collection, so recurse","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) {","code_context_10":"mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null)","code_context_20":"else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype());","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":9,"comment":"\/\/ add column for the collection (since the store needs a name to reference it by)","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0);","code_context_10":"} else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) {","code_context_20":"col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":10,"comment":"\/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"} mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested);","code_context_10":"{ col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) {","code_context_20":"} else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition());","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":11,"comment":"\/\/ nested embedded map, so recurse","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) {","code_context_10":"processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; }","code_context_20":"} membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata();","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":12,"comment":"\/\/ todo support nested embedded map key\/value","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); }","code_context_10":"{ nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) {","code_context_20":"if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true);","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":13,"comment":"\/\/ nested embedded array, so recurse","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) {","code_context_10":"nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null)","code_context_20":"nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype());","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":14,"comment":"\/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0);","code_context_10":"continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) {","code_context_20":"{ \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":15,"comment":"\/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"} mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested);","code_context_10":"{ col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) {","code_context_20":"} else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition());","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":16,"comment":"\/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null);","code_context_10":"} } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition());","code_context_20":"mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col);","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":17,"comment":"\/\/ todo pass in embedded colmds if they have jdbctype info?","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"DESIGN","isFinished":true,"code_context_2":"else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) {","code_context_10":"membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j);","code_context_20":"col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); }","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":18,"comment":"\/\/ create column(s) for this type using a typeconverter","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) {","code_context_10":"{ schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null)","code_context_20":"{ col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype());","repo":"dcheung2\/datanucleus-core"}
{"id":16997,"comment_id":19,"comment":"\/\/ create column for basic type","code":"protected void processembeddedmember(list<abstractmembermetadata> mmds, abstractclassmetadata embcmd, classloaderresolver clr, embeddedmetadata embmd, boolean ownernested) { typemanager typemgr = storemgr.getnucleuscontext().gettypemanager(); metadatamanager mmgr = storemgr.getmetadatamanager(); namingfactory namingfactory = storemgr.getnamingfactory(); abstractmembermetadata lastmmd = mmds.get(mmds.size()-1); \/\/ go through all members of the embedded type int[] memberpositions = embcmd.getallmemberpositions(); for (int i=0;i<memberpositions.length;i++) { abstractmembermetadata mmd = embcmd.getmetadataformanagedmemberatabsoluteposition(memberpositions[i]); if (mmd.getpersistencemodifier() != fieldpersistencemodifier.persistent) { \/\/ don't need column if not persistent continue; } if (mmds.size() == 1 && embmd != null && embmd.getownermember() != null && embmd.getownermember().equals(mmd.getname())) { \/\/ special case of this being a link back to the owner. todo repeat this for nested and their owners continue; } abstractmembermetadata embmdmmd = null; if (embmd != null) { abstractmembermetadata[] embmdmmds = embmd.getmembermetadata(); if (embmdmmds != null) { for (abstractmembermetadata thismmd : embmdmmds) { if (thismmd.getname().equals(mmd.getname())) { embmdmmd = thismmd; break; } } } } relationtype relationtype = mmd.getrelationtype(clr); if (relationtype != relationtype.none && metadatautils.getinstance().ismemberembedded(mmgr, clr, mmd, relationtype, lastmmd)) { if (relationtype.isrelationsinglevalued(relationtype)) { \/\/ nested embedded pc, so recurse boolean nested = false; if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_pc_nested)) { nested = !storemgr.getnucleuscontext().getconfiguration().getbooleanproperty(propertynames.property_metadata_embedded_pc_flat); string nestedstr = mmd.getvalueforextension(\"nested\"); if (nestedstr != null && nestedstr.equalsignorecase(\"\" + !nested)) { nested = !nested; } } list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); if (nested) { \/\/ embedded object stored as nested under this in the owner table (where the datastore supports that) \/\/ add column for the owner of the embedded object, typically for the column name only columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column } \/\/ recurse through the embedded member processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.gettype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, nested); } else { if (mmd.hascollection()) { \/\/ nested embedded collection, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_collection_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the collection (since the store needs a name to reference it by) columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded collection element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getcollection().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded collection. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasmap()) { \/\/ nested embedded map, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_map_nested)) { \/\/ todo support nested embedded map key\/value nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not yet supported so ignoring\"); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded map. not supported for this datastore so ignoring\"); continue; } } else if (mmd.hasarray()) { \/\/ nested embedded array, so recurse if (storemgr.getsupportedoptions().contains(storemanager.option_orm_embedded_array_nested)) { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); \/\/ add column for the array (since the store needs a name to reference it by) todo extract this block out and reuse it columnmetadata[] colmds = mmd.getcolumnmetadata(); string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(true); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); \/\/ todo create mapping for the related info under the above column \/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { typeconverter typeconv = gettypeconverterformember(mmd, colmds, typemgr); \/\/ todo pass in embedded colmds if they have jdbctype info? if (typeconv != null) { \/\/ create column(s) for this type using a typeconverter if (typeconv instanceof multicolumnconverter) { class[] coljavatypes = ((multicolumnconverter)typeconv).getdatastorecolumntypes(); column[] cols = new column[coljavatypes.length]; for (int j=0;j<coljavatypes.length;j++) { string colname = namingfactory.getcolumnname(embmmds, j); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[j].getposition()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getposition() != null) { col.setposition(colmds[j].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == coljavatypes.length && embmdmmd.getcolumnmetadata()[j].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[j].getjdbctype()); } else if (colmds != null && colmds.length == coljavatypes.length && colmds[j].getjdbctype() != null) { col.setjdbctype(colmds[j].getjdbctype()); } cols[j] = col; } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, cols, typeconv); for (int j=0;j<coljavatypes.length;j++) { ((columnimpl)cols[j]).setmembercolumnmapping(mapping); } if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } else { string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, typeconv); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); mapping.settypeconverter(typeconv); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } else { \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); abstractmembermetadata themmd = embmmds.get(0); if (themmd.isprimarykey()) { col.setprimarykey(); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col); col.setmembercolumnmapping(mapping); if (schemaverifier != null) { schemaverifier.attributeembeddedmember(mapping, embmmds); } mappingbyembeddedmember.put(getembeddedmembernavigatedpath(embmmds), mapping); } } } } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null);","code_context_10":"} else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition());","code_context_20":"\/\/ recurse through the embedded array element processembeddedmember(embmmds, mmgr.getmetadataforclass(mmd.getarray().getelementtype(), clr), clr, embmdmmd != null ? embmdmmd.getembeddedmetadata() : null, true); } else { nucleuslogger.datastore_schema.warn(\"member \" + mmd.getfullfieldname() + \" is a (nested) embedded array. not supported for this datastore so ignoring\"); continue; } } } } else { list<abstractmembermetadata> embmmds = new arraylist<abstractmembermetadata>(mmds); embmmds.add(mmd); columnmetadata[] colmds = mmd.getcolumnmetadata(); if (relationtype != relationtype.none) { \/\/ 1-1\/n-1 stored as single column with persistable-id \/\/ 1-n\/m-n stored as single column with collection<persistable-id> \/\/ create column for basic type string colname = namingfactory.getcolumnname(embmmds, 0); columnimpl col = addembeddedcolumn(colname, null); col.setnested(ownernested); if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getposition() != null) { col.setposition(embmdmmd.getcolumnmetadata()[0].getposition()); } else if (colmds != null && colmds.length == 1 && colmds[0].getposition() != null) { col.setposition(colmds[0].getposition()); } if (embmdmmd != null && embmdmmd.getcolumnmetadata() != null && embmdmmd.getcolumnmetadata().length == 1 && embmdmmd.getcolumnmetadata()[0].getjdbctype() != null) { col.setjdbctype(embmdmmd.getcolumnmetadata()[0].getjdbctype()); } else if (colmds != null && colmds.length == 1 && colmds[0].getjdbctype() != null) { col.setjdbctype(colmds[0].getjdbctype()); } membercolumnmapping mapping = new membercolumnmappingimpl(mmd, col);","repo":"dcheung2\/datanucleus-core"}
{"id":17020,"comment_id":0,"comment":"\/\/ todo: remove these probing methods, and test through mocked scriptingcontainerdelegate.","code":"\/\/ todo: remove these probing methods, and test through mocked scriptingcontainerdelegate. string probegemhomefortesting() { return this.gemhome; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"string probegemhomefortesting() { return this.gemhome; }","code_context_10":"string probegemhomefortesting() { return this.gemhome; }","code_context_20":"string probegemhomefortesting() { return this.gemhome; }","repo":"dmikurube\/embulk"}
{"id":8997,"comment_id":0,"comment":"\/\/ this is the name of the rule, not a reference to it!!","code":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","classification":"NONSATD","isFinished":true,"code_context_2":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename,","code_context_10":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) {","code_context_20":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","repo":"etirelli\/kie-wb-common"}
{"id":8997,"comment_id":1,"comment":"\/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance)","code":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","classification":"NONSATD","isFinished":true,"code_context_2":"addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) {","code_context_10":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this..","code_context_20":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","repo":"etirelli\/kie-wb-common"}
{"id":8997,"comment_id":2,"comment":"\/\/ need compilation for this..","code":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","classification":"DOCUMENTATION","isFinished":true,"code_context_2":"} visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo","code_context_10":"\/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","code_context_20":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","repo":"etirelli\/kie-wb-common"}
{"id":8997,"comment_id":3,"comment":"\/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence);","code":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","classification":"NONSATD","isFinished":true,"code_context_2":"visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr);","code_context_10":"if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","code_context_20":"protected void visit(final ruledescr descr) { \/\/ this is the name of the rule, not a reference to it!! string rulename = getpackageprefix() + descr.getname(); addresource(rulename, resourcetype.rule); \/\/ this is, on other hand, is a reference to the parent rule (because it's used in inheritance) string parentrulename = descr.getparentname(); if (parentrulename != null) { addresourcereference(parentrulename, resourcetype.rule); } for (attributedescr d : descr.getattributes().values()) { visit(d); } visit(descr.getlhs()); visitconsequence(descr.getconsequence()); \/\/ need compilation for this.. for (string namedconsequence : descr.getnamedconsequences().keyset()) { \/\/ todo \/\/ ? addresourcereference(namedconsequence, parttype.named_consequence); } visitannos(descr); }","repo":"etirelli\/kie-wb-common"}
{"id":832,"comment_id":0,"comment":"\/\/ todo: eventually use caching here","code":"public raster getraster(int xoffset, int yoffset, int w, int h) { colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; } rectangle2d destrect = new rectangle2d.double(); rectangle2d.intersect(childrect, devicebounds, destrect); int dx = (int)(destrect.getx()-devicebounds.getx()); int dy = (int)(destrect.gety()-devicebounds.gety()); int dw = (int)destrect.getwidth(); int dh = (int)destrect.getheight(); object data = raster.getdataelements(dx, dy, dw, dh, null); dx = (int)(destrect.getx()-childrect.getx()); dy = (int)(destrect.gety()-childrect.gety()); childraster.setdataelements(dx, dy, dw, dh, data); return childraster; }","classification":"DESIGN","isFinished":true,"code_context_2":"colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h);","code_context_10":"public raster getraster(int xoffset, int yoffset, int w, int h) { colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; } rectangle2d destrect = new rectangle2d.double(); rectangle2d.intersect(childrect, devicebounds, destrect); int dx = (int)(destrect.getx()-devicebounds.getx()); int dy = (int)(destrect.gety()-devicebounds.gety());","code_context_20":"public raster getraster(int xoffset, int yoffset, int w, int h) { colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; } rectangle2d destrect = new rectangle2d.double(); rectangle2d.intersect(childrect, devicebounds, destrect); int dx = (int)(destrect.getx()-devicebounds.getx()); int dy = (int)(destrect.gety()-devicebounds.gety()); int dw = (int)destrect.getwidth(); int dh = (int)destrect.getheight(); object data = raster.getdataelements(dx, dy, dw, dh, null); dx = (int)(destrect.getx()-childrect.getx()); dy = (int)(destrect.gety()-childrect.gety()); childraster.setdataelements(dx, dy, dw, dh, data); return childraster; }","repo":"fomich-artem\/poi"}
{"id":832,"comment_id":1,"comment":"\/\/ usually doesn't happen ...","code":"public raster getraster(int xoffset, int yoffset, int w, int h) { colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; } rectangle2d destrect = new rectangle2d.double(); rectangle2d.intersect(childrect, devicebounds, destrect); int dx = (int)(destrect.getx()-devicebounds.getx()); int dy = (int)(destrect.gety()-devicebounds.gety()); int dw = (int)destrect.getwidth(); int dh = (int)destrect.getheight(); object data = raster.getdataelements(dx, dy, dw, dh, null); dx = (int)(destrect.getx()-childrect.getx()); dy = (int)(destrect.gety()-childrect.gety()); childraster.setdataelements(dx, dy, dw, dh, data); return childraster; }","classification":"NONSATD","isFinished":true,"code_context_2":"rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; }","code_context_10":"public raster getraster(int xoffset, int yoffset, int w, int h) { colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; } rectangle2d destrect = new rectangle2d.double(); rectangle2d.intersect(childrect, devicebounds, destrect); int dx = (int)(destrect.getx()-devicebounds.getx()); int dy = (int)(destrect.gety()-devicebounds.gety()); int dw = (int)destrect.getwidth(); int dh = (int)destrect.getheight(); object data = raster.getdataelements(dx, dy, dw, dh, null); dx = (int)(destrect.getx()-childrect.getx());","code_context_20":"public raster getraster(int xoffset, int yoffset, int w, int h) { colormodel cm = getcolormodel(); if (raster == null) createraster(); \/\/ todo: eventually use caching here writableraster childraster = cm.createcompatiblewritableraster(w, h); rectangle2d childrect = new rectangle2d.double(xoffset, yoffset, w, h); if (!childrect.intersects(devicebounds)) { \/\/ usually doesn't happen ... return childraster; } rectangle2d destrect = new rectangle2d.double(); rectangle2d.intersect(childrect, devicebounds, destrect); int dx = (int)(destrect.getx()-devicebounds.getx()); int dy = (int)(destrect.gety()-devicebounds.gety()); int dw = (int)destrect.getwidth(); int dh = (int)destrect.getheight(); object data = raster.getdataelements(dx, dy, dw, dh, null); dx = (int)(destrect.getx()-childrect.getx()); dy = (int)(destrect.gety()-childrect.gety()); childraster.setdataelements(dx, dy, dw, dh, data); return childraster; }","repo":"fomich-artem\/poi"}
{"id":17336,"comment_id":0,"comment":"\/\/ todo: need to rewrite.","code":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenanothertaskisbeingexecuted() throws exception { thread.execute(() -> { try { thread.sleep(200); } catch (interruptedexception e) { thread.interrupt(); } }); thread.execute(mock(itask.class)); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenanothertaskisbeingexecuted() throws exception {","code_context_10":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenanothertaskisbeingexecuted() throws exception { thread.execute(() -> { try { thread.sleep(200); } catch (interruptedexception e) { thread.interrupt(); } }); thread.execute(mock(itask.class));","code_context_20":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenanothertaskisbeingexecuted() throws exception { thread.execute(() -> { try { thread.sleep(200); } catch (interruptedexception e) { thread.interrupt(); } }); thread.execute(mock(itask.class)); }","repo":"d-protsenko\/smartactors-core"}
{"id":17337,"comment_id":0,"comment":"\/\/ todo: need to rewrite.","code":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenthreadisnotalive() throws exception { itask taskmock = mock(itask.class); thread.interrupt(); thread.execute(taskmock); fail(); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenthreadisnotalive() throws exception {","code_context_10":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenthreadisnotalive() throws exception { itask taskmock = mock(itask.class); thread.interrupt(); thread.execute(taskmock); fail(); }","code_context_20":"@test(expected = taskexecutionexception.class) @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_throwwhenthreadisnotalive() throws exception { itask taskmock = mock(itask.class); thread.interrupt(); thread.execute(taskmock); fail(); }","repo":"d-protsenko\/smartactors-core"}
{"id":17338,"comment_id":0,"comment":"\/\/ todo: need to rewrite.","code":"@test @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_ignoreexceptionsfromtask() throws exception { itask taskmock1 = mock(itask.class), taskmock2 = mock(itask.class); dothrow(new taskexecutionexception(\"whoops!\")).when(taskmock1).execute(); thread.execute(taskmock1); verify(taskmock1, timeout(100)).execute(); thread.sleep(200); thread.execute(taskmock2); verify(taskmock2, timeout(100)).execute(); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@test @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_ignoreexceptionsfromtask() throws exception {","code_context_10":"@test @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_ignoreexceptionsfromtask() throws exception { itask taskmock1 = mock(itask.class), taskmock2 = mock(itask.class); dothrow(new taskexecutionexception(\"whoops!\")).when(taskmock1).execute(); thread.execute(taskmock1); verify(taskmock1, timeout(100)).execute(); thread.sleep(200); thread.execute(taskmock2); verify(taskmock2, timeout(100)).execute(); }","code_context_20":"@test @ignore(\"test execution is depending on server technical characteristics.\") \/\/ todo: need to rewrite. public void should_ignoreexceptionsfromtask() throws exception { itask taskmock1 = mock(itask.class), taskmock2 = mock(itask.class); dothrow(new taskexecutionexception(\"whoops!\")).when(taskmock1).execute(); thread.execute(taskmock1); verify(taskmock1, timeout(100)).execute(); thread.sleep(200); thread.execute(taskmock2); verify(taskmock2, timeout(100)).execute(); }","repo":"d-protsenko\/smartactors-core"}
{"id":25637,"comment_id":0,"comment":"\/\/ todo: maybe sine curve is more appropriate for turning? \/\/ pass the raw turn value through an input curve, then apply the turn sensitivity.","code":"@override double getturninput(humaninput humaninput) { \/\/ todo: maybe sine curve is more appropriate for turning? \/\/ pass the raw turn value through an input curve, then apply the turn sensitivity. return util.applyinputcurve(humaninput.getgamepad().rs().gethorizontal(), .75, 3) * turnsensitivity; }","classification":"DESIGN","isFinished":true,"code_context_2":"@override double getturninput(humaninput humaninput) { \/\/ todo: maybe sine curve is more appropriate for turning? \/\/ pass the raw turn value through an input curve, then apply the turn sensitivity. return util.applyinputcurve(humaninput.getgamepad().rs().gethorizontal(), .75, 3) * turnsensitivity; }","code_context_10":"@override double getturninput(humaninput humaninput) { \/\/ todo: maybe sine curve is more appropriate for turning? \/\/ pass the raw turn value through an input curve, then apply the turn sensitivity. return util.applyinputcurve(humaninput.getgamepad().rs().gethorizontal(), .75, 3) * turnsensitivity; }","code_context_20":"@override double getturninput(humaninput humaninput) { \/\/ todo: maybe sine curve is more appropriate for turning? \/\/ pass the raw turn value through an input curve, then apply the turn sensitivity. return util.applyinputcurve(humaninput.getgamepad().rs().gethorizontal(), .75, 3) * turnsensitivity; }","repo":"first-1251\/exp-drive"}
{"id":17495,"comment_id":0,"comment":"\/** * return the reloadabletype representing the superclass of this type. if the supertype is not reloadable, this * method will return null. the reloadabletype that is returned may not be within the same type registry, if the * supertype was loaded by a different classloader. * * @return the reloadabletype for the supertype or null if it is not reloadable *\/","code":"\/** * return the reloadabletype representing the superclass of this type. if the supertype is not reloadable, this * method will return null. the reloadabletype that is returned may not be within the same type registry, if the * supertype was loaded by a different classloader. * * @return the reloadabletype for the supertype or null if it is not reloadable *\/ public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } } else { classloader superclassloader = superclazz.getclassloader(); typeregistry supertyperegistry = typeregistry.gettyperegistryfor(superclassloader); superrtype = supertyperegistry.getreloadabletype(superclazz); return superrtype; } }","classification":"NONSATD","isFinished":true,"code_context_2":"public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } } else { classloader superclassloader = superclazz.getclassloader(); typeregistry supertyperegistry = typeregistry.gettyperegistryfor(superclassloader); superrtype = supertyperegistry.getreloadabletype(superclazz); return superrtype; } }","code_context_10":"public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } } else { classloader superclassloader = superclazz.getclassloader(); typeregistry supertyperegistry = typeregistry.gettyperegistryfor(superclassloader); superrtype = supertyperegistry.getreloadabletype(superclazz); return superrtype; } }","code_context_20":"public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } } else { classloader superclassloader = superclazz.getclassloader(); typeregistry supertyperegistry = typeregistry.gettyperegistryfor(superclassloader); superrtype = supertyperegistry.getreloadabletype(superclazz); return superrtype; } }","repo":"crudolf\/spring-loaded"}
{"id":17495,"comment_id":1,"comment":"\/\/ not filled in yet? why is this code different to the interface case?","code":"public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } } else { classloader superclassloader = superclazz.getclassloader(); typeregistry supertyperegistry = typeregistry.gettyperegistryfor(superclassloader); superrtype = supertyperegistry.getreloadabletype(superclazz); return superrtype; } }","classification":"DESIGN","isFinished":true,"code_context_2":"} if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) {","code_context_10":"public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } }","code_context_20":"public reloadabletype getsuperrtype() { if (superrtype != null) { return superrtype; } if (superclazz == null) { \/\/ not filled in yet? why is this code different to the interface case? string name = this.getslashedsupertypename(); if (name == null) { return null; } else { reloadabletype rtype = typeregistry.getreloadablesupertype(name); superrtype = rtype; return superrtype; } } else { classloader superclassloader = superclazz.getclassloader(); typeregistry supertyperegistry = typeregistry.gettyperegistryfor(superclassloader); superrtype = supertyperegistry.getreloadabletype(superclazz); return superrtype; } }","repo":"crudolf\/spring-loaded"}
{"id":33952,"comment_id":0,"comment":"\/** * check if the replica count per ts matches the expected, returns true if it does within * timeoutms, false otherwise. * todo(rahul): follow similar style for this type of function will do with affinitized * leaders tests. * @param timeoutms number of milliseconds before timing out. * @param table the table to wait for load balancing. * @param replicamapexpected the expected map between cluster uuid and live, read replica count. * @return true if the read only replica count for the table matches the expected within the * expected time frame, false otherwise. *\/","code":"\/** * check if the replica count per ts matches the expected, returns true if it does within * timeoutms, false otherwise. * todo(rahul): follow similar style for this type of function will do with affinitized * leaders tests. * @param timeoutms number of milliseconds before timing out. * @param table the table to wait for load balancing. * @param replicamapexpected the expected map between cluster uuid and live, read replica count. * @return true if the read only replica count for the table matches the expected within the * expected time frame, false otherwise. *\/ public boolean waitforexpectedreplicamap(final long timeoutms, ybtable table, map<string, list<list<integer>>> replicamapexpected) { condition replicamapcondition = new replicamapcondition(table, replicamapexpected, timeoutms); return waitforcondition(replicamapcondition, timeoutms); }","classification":"DESIGN","isFinished":true,"code_context_2":"public boolean waitforexpectedreplicamap(final long timeoutms, ybtable table, map<string, list<list<integer>>> replicamapexpected) { condition replicamapcondition = new replicamapcondition(table, replicamapexpected, timeoutms); return waitforcondition(replicamapcondition, timeoutms); }","code_context_10":"public boolean waitforexpectedreplicamap(final long timeoutms, ybtable table, map<string, list<list<integer>>> replicamapexpected) { condition replicamapcondition = new replicamapcondition(table, replicamapexpected, timeoutms); return waitforcondition(replicamapcondition, timeoutms); }","code_context_20":"public boolean waitforexpectedreplicamap(final long timeoutms, ybtable table, map<string, list<list<integer>>> replicamapexpected) { condition replicamapcondition = new replicamapcondition(table, replicamapexpected, timeoutms); return waitforcondition(replicamapcondition, timeoutms); }","repo":"def-\/yugabyte-db"}
{"id":25790,"comment_id":0,"comment":"\/** * this is really testing how the nonproductionconfig works - how can this be * targeted to the init config? *\/","code":"\/** * this is really testing how the nonproductionconfig works - how can this be * targeted to the init config? *\/ @test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); } }","classification":"DESIGN","isFinished":true,"code_context_2":"@test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); } }","code_context_10":"@test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); } }","code_context_20":"@test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); } }","repo":"emafazillah\/andhow"}
{"id":25790,"comment_id":1,"comment":"\/\/the line above should throw an error","code":"@test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); } }","classification":"NONSATD","isFinished":true,"code_context_2":"andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size());","code_context_10":"@test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); }","code_context_20":"@test public void testblowingupwithduplicateloaders() { keyvaluepairloader kvpl = new keyvaluepairloader(); kvpl.setkeyvaluepairs(cmdlineargswfullclassname); try { andhowconfiguration config = andhowtestconfig.instance() .setloaders(kvpl, kvpl) .addoverridegroups(configptgroups); andhow.setconfig(config); andhow.instance(); fail(); \/\/the line above should throw an error } catch (appfatalexception ce) { assertequals(1, ce.getproblems().filter(constructionproblem.class).size()); asserttrue(ce.getproblems().filter(constructionproblem.class).get(0) instanceof constructionproblem.duplicateloader); constructionproblem.duplicateloader dl = (constructionproblem.duplicateloader)ce.getproblems().filter(constructionproblem.class).get(0); assertequals(kvpl, dl.getloader()); asserttrue(ce.getsampledirectory().length() > 0); file sampledir = new file(ce.getsampledirectory()); asserttrue(sampledir.exists()); asserttrue(sampledir.listfiles().length > 0); } }","repo":"emafazillah\/andhow"}
{"id":17643,"comment_id":0,"comment":"\/** * creates a new connection and a new session to activemq *\/","code":"\/** * creates a new connection and a new session to activemq *\/ public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","code_context_10":"public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","code_context_20":"public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","repo":"dbraga\/solr2activemq"}
{"id":17643,"comment_id":1,"comment":"\/\/todo: handle topics, not just queues","code":"public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer();","code_context_10":"public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","code_context_20":"public void bootstrapmessagingsystem(){ try{ messagingsystem = new messagingsystem(activemq_broker_uri, activemq_broker_port); messagingsystem.createconnection(); messagingsystem.createsession(); \/\/todo: handle topics, not just queues messagingsystem.createdestination(activemq_destination_name); messagingsystem.createproducer(); system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system done.\"); messagingsystem.validateconnection(); } catch (jmsexception e){ system.out.println(\"solrtoactivemqcomponent: bootstrapping messaging system failed.\\n\" + e); messagingsystem.invalidateconnection(); } }","repo":"dbraga\/solr2activemq"}
{"id":9486,"comment_id":0,"comment":"\/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck.","code":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","classification":"DESIGN","isFinished":true,"code_context_2":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot);","code_context_10":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true;","code_context_20":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\")","repo":"ecgreb\/robolectric"}
{"id":9486,"comment_id":1,"comment":"\/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521","code":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","classification":"DESIGN","isFinished":true,"code_context_2":"} if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !(","code_context_10":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\")","code_context_20":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","repo":"ecgreb\/robolectric"}
{"id":9486,"comment_id":2,"comment":"\/\/ allows for android projects with mixed scala\\java tests to be","code":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","classification":"NONSATD","isFinished":true,"code_context_2":"|| name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still?","code_context_10":"name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","code_context_20":"string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","repo":"ecgreb\/robolectric"}
{"id":9486,"comment_id":3,"comment":"\/\/ run with maven surefire (see the robospecs project on github)","code":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","classification":"NONSATD","isFinished":true,"code_context_2":"|| name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? );","code_context_10":"|| classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","code_context_20":"if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","repo":"ecgreb\/robolectric"}
{"id":9486,"comment_id":4,"comment":"\/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still?","code":"public boolean shouldacquire(string name) { \/\/ the org.robolectric.res package lives in the base classloader, but not its tests; yuck. int lastdot = name.lastindexof('.'); string pkgname = name.substring(0, lastdot == -1 ? 0 : lastdot); if (pkgname.equals(\"org.robolectric.res\")) { return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","classification":"DESIGN","isFinished":true,"code_context_2":"|| name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","code_context_10":"|| name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","code_context_20":"return name.contains(\"test\"); } if (name.matches(\"com\\\\.android\\\\.internal\\\\.r(\\\\$.*)?\")) return true; \/\/ android sdk code almost universally refers to com.android.internal.r, except \/\/ when refering to android.r.stylable, as in horizontalscrollview. arghgh. \/\/ see https:\/\/github.com\/robolectric\/robolectric\/issues\/521 if (name.equals(\"android.r$styleable\")) return true; return !( name.matches(\".*\\\\.r(|\\\\$[a-z]+)$\") || classes_to_always_delegate.contains(name) || name.startswith(\"java.\") || name.startswith(\"javax.\") || name.startswith(\"sun.\") || name.startswith(\"com.sun.\") || name.startswith(\"org.w3c.\") || name.startswith(\"org.xml.\") || name.startswith(\"org.junit\") || name.startswith(\"org.hamcrest\") || name.startswith(\"org.specs2\") \/\/ allows for android projects with mixed scala\\java tests to be || name.startswith(\"scala.\") \/\/ run with maven surefire (see the robospecs project on github) || name.startswith(\"org.sqlite.\") \/\/ ugh, we're barfing while loading org.sqlite now for some reason?!? todo: still? ); }","repo":"ecgreb\/robolectric"}
{"id":17714,"comment_id":0,"comment":"\/\/ todo: optimize","code":"public void writebytes(int stream, byte[] b, int offset, int len) { \/\/ todo: optimize final int end = offset + len; for(int i=offset;i<end;i++) writebyte(stream, b[i]); }","classification":"DESIGN","isFinished":true,"code_context_2":"public void writebytes(int stream, byte[] b, int offset, int len) { \/\/ todo: optimize final int end = offset + len; for(int i=offset;i<end;i++)","code_context_10":"public void writebytes(int stream, byte[] b, int offset, int len) { \/\/ todo: optimize final int end = offset + len; for(int i=offset;i<end;i++) writebyte(stream, b[i]); }","code_context_20":"public void writebytes(int stream, byte[] b, int offset, int len) { \/\/ todo: optimize final int end = offset + len; for(int i=offset;i<end;i++) writebyte(stream, b[i]); }","repo":"dannycho7\/RTP_Latest"}
{"id":9527,"comment_id":0,"comment":"\/** * todo: option - create via idea or via java.io. in latter case no need in project parameter. * creates directory inside a write action and returns the resulting reference to it. * if the directory already exists, does nothing. * @param parent parent directory. * @param name name of the directory. * @return reference to the created or already existing directory. *\/","code":"\/** * todo: option - create via idea or via java.io. in latter case no need in project parameter. * creates directory inside a write action and returns the resulting reference to it. * if the directory already exists, does nothing. * @param parent parent directory. * @param name name of the directory. * @return reference to the created or already existing directory. *\/ public static virtualfile createdir(project project, final virtualfile parent, final string name) { final ref<virtualfile> result = new ref<virtualfile>(); new writecommandaction.simple(project) { @override protected void run() throws throwable { try { virtualfile dir = parent.findchild(name); if (dir == null) { dir = parent.createchilddirectory(this, name); } result.set(dir); } catch (ioexception e) { throw new runtimeexception(e); } } }.execute(); return result.get(); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public static virtualfile createdir(project project, final virtualfile parent, final string name) { final ref<virtualfile> result = new ref<virtualfile>(); new writecommandaction.simple(project) { @override protected void run() throws throwable { try { virtualfile dir = parent.findchild(name); if (dir == null) { dir = parent.createchilddirectory(this, name); } result.set(dir); } catch (ioexception e) { throw new runtimeexception(e); } } }.execute(); return result.get(); }","code_context_10":"public static virtualfile createdir(project project, final virtualfile parent, final string name) { final ref<virtualfile> result = new ref<virtualfile>(); new writecommandaction.simple(project) { @override protected void run() throws throwable { try { virtualfile dir = parent.findchild(name); if (dir == null) { dir = parent.createchilddirectory(this, name); } result.set(dir); } catch (ioexception e) { throw new runtimeexception(e); } } }.execute(); return result.get(); }","code_context_20":"public static virtualfile createdir(project project, final virtualfile parent, final string name) { final ref<virtualfile> result = new ref<virtualfile>(); new writecommandaction.simple(project) { @override protected void run() throws throwable { try { virtualfile dir = parent.findchild(name); if (dir == null) { dir = parent.createchilddirectory(this, name); } result.set(dir); } catch (ioexception e) { throw new runtimeexception(e); } } }.execute(); return result.get(); }","repo":"consulo\/consulo-git"}
{"id":9528,"comment_id":0,"comment":"\/\/ ignored. todo: do i have to handle this?","code":"@centrypoint(name = \"execute\") public static int execute(isolatethread thread, int statement) { try { boolean hasresult = statements.get(statement).execute(); if (hasresult) { throw new sqlexception(\"unexpected results on statement execute\"); } return statements.get(statement).getupdatecount(); } catch (throwable e) { seterror(e); return -1; } finally { try { statements.get(statement).close(); } catch(throwable t) { \/\/ ignored. todo: do i have to handle this? } } }","classification":"DESIGN","isFinished":true,"code_context_2":"statements.get(statement).close(); } catch(throwable t) { \/\/ ignored. todo: do i have to handle this? } }","code_context_10":"throw new sqlexception(\"unexpected results on statement execute\"); } return statements.get(statement).getupdatecount(); } catch (throwable e) { seterror(e); return -1; } finally { try { statements.get(statement).close(); } catch(throwable t) { \/\/ ignored. todo: do i have to handle this? } } }","code_context_20":"@centrypoint(name = \"execute\") public static int execute(isolatethread thread, int statement) { try { boolean hasresult = statements.get(statement).execute(); if (hasresult) { throw new sqlexception(\"unexpected results on statement execute\"); } return statements.get(statement).getupdatecount(); } catch (throwable e) { seterror(e); return -1; } finally { try { statements.get(statement).close(); } catch(throwable t) { \/\/ ignored. todo: do i have to handle this? } } }","repo":"csmu-cenr\/gdbc"}
{"id":17766,"comment_id":0,"comment":"\/** * when server receives a request with unknown session_id it must recognize that as request for a new session. when * server opens a new session it must immediately send an frame containing a letter o. * <\/p> * note: this test may periodically fail as we're relying on a multicore processor an non-blocking io being * reliable. this isn't ideal as tests should be determinate! * * @throws executionexception * @throws interruptedexception *\/","code":"\/** * when server receives a request with unknown session_id it must recognize that as request for a new session. when * server opens a new session it must immediately send an frame containing a letter o. * <\/p> * note: this test may periodically fail as we're relying on a multicore processor an non-blocking io being * reliable. this isn't ideal as tests should be determinate! * * @throws executionexception * @throws interruptedexception *\/ @test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"TEST","isFinished":true,"code_context_2":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","code_context_10":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","code_context_20":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","repo":"dansiviter\/cito-sockjs"}
{"id":17766,"comment_id":1,"comment":"\/\/ new line is a frame delimiter specific for xhr-polling\"","code":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"NONSATD","isFinished":true,"code_context_2":"final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class));","code_context_10":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'.","code_context_20":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be","repo":"dansiviter\/cito-sockjs"}
{"id":17766,"comment_id":2,"comment":"\/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings.","code":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"NONSATD","isFinished":true,"code_context_2":"assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo());","code_context_10":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid.","code_context_20":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) {","repo":"dansiviter\/cito-sockjs"}
{"id":17766,"comment_id":3,"comment":"\/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'.","code":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"NONSATD","isFinished":true,"code_context_2":"verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo());","code_context_10":"\/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be","code_context_20":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the","repo":"dansiviter\/cito-sockjs"}
{"id":17766,"comment_id":4,"comment":"\/\/ sending messages to not existing sessions is invalid.","code":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"NONSATD","isFinished":true,"code_context_2":"assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res);","code_context_10":"\/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) {","code_context_20":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class);","repo":"dansiviter\/cito-sockjs"}
{"id":17766,"comment_id":5,"comment":"\/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection.","code":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"DESIGN","isFinished":true,"code_context_2":"verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) {","code_context_10":"} \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo());","code_context_20":"try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\";","repo":"dansiviter\/cito-sockjs"}
{"id":17766,"comment_id":6,"comment":"\/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both.","code":"@test @runasclient public void simplesession() throws interruptedexception, executionexception { final string uuid = uuid(); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { \/\/ new line is a frame delimiter specific for xhr-polling\" assertequals(status.ok, res.getstatusinfo()); assertequals(\"o\\n\", res.readentity(string.class)); } \/\/ after a session was established the server needs to accept requests for sending messages. \/\/ xhr-polling accepts messages as a list of json-encoded strings. try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"a\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); verifyemptyentity(res); } \/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) {","code_context_10":"\/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) {","code_context_20":"\/\/ we're using an echo service - we'll receive our message back. the message is encoded as an array 'a'. try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); assertequals(\"a[\\\"a\\\"]\\n\", res.readentity(string.class)); } \/\/ sending messages to not existing sessions is invalid. try (closableresponse res = post(target(\"000\", \"bad_session\", xhr_send), json(\"[\\\"a\\\"]\"))) { verify404(xhr_send, res); } \/\/ the session must time out after 5 seconds of not having a receiving connection. the server must send a \/\/ heartbeat frame every 25 seconds. the heartbeat frame contains a single h character. this delay may be \/\/ configurable. \/\/ todo \/\/ the server must not allow two receiving connections to wait on a single session. in such case the server must \/\/ send a close frame to the new connection. for (int i = 0; i < 10; i++) { try (closableresponse res = post(target(\"000\", uuid, xhr_send), json(\"[\\\"xxxxxx\\\"]\"))) { assertequals(status.no_content, res.getstatusinfo()); } } \/\/ due to the time it takes for an async request to be scheduled it might actually be the one that returns the \/\/ 'another connection still open' error. therefore we need to check both. final future<response> asyncfuture = target(\"000\", uuid, xhr).request().async().post(json(null)); try (closableresponse res = post(target(\"000\", uuid, xhr), json(null))) { assertequals(status.ok, res.getstatusinfo()); final string respayload = res.readentity(string.class); try (closableresponse asyncres = closable(asyncfuture.get())) { assertequals(status.ok, asyncres.getstatusinfo()); final string asyncrespayload = asyncres.readentity(string.class); if (enable_concurrent_requests_test) { final string expectederror = \"c[2010,\\\"another connection still open\\\"]\\n\"; if (!expectederror.equals(respayload) && !expectederror.equals(asyncrespayload)) { fail(\"neither response had '\" + expectederror + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } final string expected = \"a[\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\",\\\"xxxxxx\\\"]\\n\"; if (!expected.equals(respayload) && !expected.equals(asyncrespayload)) { fail(\"neither response had '\" + expected + \"'! [blocking=\" + respayload + \",async=\" + asyncrespayload + \"]\"); } } } } finally { asyncfuture.cancel(true);","repo":"dansiviter\/cito-sockjs"}
{"id":9593,"comment_id":0,"comment":"\/\/ maybe we could support concurrent some time in the future","code":"@override public boolean isconcurrentaccesssupported() { \/\/ maybe we could support concurrent some time in the future return false; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public boolean isconcurrentaccesssupported() { \/\/ maybe we could support concurrent some time in the future return false; }","code_context_10":"@override public boolean isconcurrentaccesssupported() { \/\/ maybe we could support concurrent some time in the future return false; }","code_context_20":"@override public boolean isconcurrentaccesssupported() { \/\/ maybe we could support concurrent some time in the future return false; }","repo":"elharo\/plexus-archiver"}
{"id":9601,"comment_id":0,"comment":"\/\/todo:until the listener is set, content writing happens in i\/o thread. if writability changed \/\/while in i\/o thread and defaultbackpressurelistener is engaged, there's a chance of i\/o thread \/\/getting blocked. cannot recreate, only a possibility.","code":"public void writecontent(httpcarbonmessage httpoutboundrequest) { if (handlerexecutor != null) { handlerexecutor.executeattargetrequestreceiving(httpoutboundrequest); } backpressurehandler backpressurehandler = util.getbackpressurehandler(targethandler.getcontext()); util.setbackpressurelistener(httpoutboundrequest, backpressurehandler, httpoutboundrequest.getsourcecontext()); resettargetchannelstate(); httpoutboundrequest.gethttpcontentasync().setmessagelistener((httpcontent -> { \/\/todo:until the listener is set, content writing happens in i\/o thread. if writability changed \/\/while in i\/o thread and defaultbackpressurelistener is engaged, there's a chance of i\/o thread \/\/getting blocked. cannot recreate, only a possibility. util.checkunwritabilityandnotify(targethandler.getcontext(), backpressurehandler); this.channel.eventloop().execute(() -> { try { senderreqrespstatemanager.writeoutboundrequestentity(httpoutboundrequest, httpcontent); } catch (exception exception) { string errormsg = \"failed to send the request : \" + exception.getmessage().tolowercase(locale.english); log.error(errormsg, exception); this.targethandler.gethttpresponsefuture().notifyhttplistener(exception); } }); })); }","classification":"DEFECT","isFinished":true,"code_context_2":"resettargetchannelstate(); httpoutboundrequest.gethttpcontentasync().setmessagelistener((httpcontent -> { \/\/todo:until the listener is set, content writing happens in i\/o thread. if writability changed \/\/while in i\/o thread and defaultbackpressurelistener is engaged, there's a chance of i\/o thread \/\/getting blocked. cannot recreate, only a possibility. util.checkunwritabilityandnotify(targethandler.getcontext(), backpressurehandler); this.channel.eventloop().execute(() -> {","code_context_10":"public void writecontent(httpcarbonmessage httpoutboundrequest) { if (handlerexecutor != null) { handlerexecutor.executeattargetrequestreceiving(httpoutboundrequest); } backpressurehandler backpressurehandler = util.getbackpressurehandler(targethandler.getcontext()); util.setbackpressurelistener(httpoutboundrequest, backpressurehandler, httpoutboundrequest.getsourcecontext()); resettargetchannelstate(); httpoutboundrequest.gethttpcontentasync().setmessagelistener((httpcontent -> { \/\/todo:until the listener is set, content writing happens in i\/o thread. if writability changed \/\/while in i\/o thread and defaultbackpressurelistener is engaged, there's a chance of i\/o thread \/\/getting blocked. cannot recreate, only a possibility. util.checkunwritabilityandnotify(targethandler.getcontext(), backpressurehandler); this.channel.eventloop().execute(() -> { try { senderreqrespstatemanager.writeoutboundrequestentity(httpoutboundrequest, httpcontent); } catch (exception exception) { string errormsg = \"failed to send the request : \" + exception.getmessage().tolowercase(locale.english); log.error(errormsg, exception); this.targethandler.gethttpresponsefuture().notifyhttplistener(exception); }","code_context_20":"public void writecontent(httpcarbonmessage httpoutboundrequest) { if (handlerexecutor != null) { handlerexecutor.executeattargetrequestreceiving(httpoutboundrequest); } backpressurehandler backpressurehandler = util.getbackpressurehandler(targethandler.getcontext()); util.setbackpressurelistener(httpoutboundrequest, backpressurehandler, httpoutboundrequest.getsourcecontext()); resettargetchannelstate(); httpoutboundrequest.gethttpcontentasync().setmessagelistener((httpcontent -> { \/\/todo:until the listener is set, content writing happens in i\/o thread. if writability changed \/\/while in i\/o thread and defaultbackpressurelistener is engaged, there's a chance of i\/o thread \/\/getting blocked. cannot recreate, only a possibility. util.checkunwritabilityandnotify(targethandler.getcontext(), backpressurehandler); this.channel.eventloop().execute(() -> { try { senderreqrespstatemanager.writeoutboundrequestentity(httpoutboundrequest, httpcontent); } catch (exception exception) { string errormsg = \"failed to send the request : \" + exception.getmessage().tolowercase(locale.english); log.error(errormsg, exception); this.targethandler.gethttpresponsefuture().notifyhttplistener(exception); } }); })); }","repo":"gabilang\/module-ballerina-http"}
{"id":1413,"comment_id":0,"comment":"\/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller","code":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","classification":"NONSATD","isFinished":true,"code_context_2":"@pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id.","code_context_10":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","code_context_20":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","repo":"focadiz\/metasfresh"}
{"id":1413,"comment_id":1,"comment":"\/\/ just validate the windowid and viewid","code":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","classification":"NONSATD","isFinished":true,"code_context_2":"{ \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname);","code_context_10":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","code_context_20":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","repo":"focadiz\/metasfresh"}
{"id":1413,"comment_id":2,"comment":"\/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id.","code":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","classification":"DESIGN","isFinished":true,"code_context_2":"\/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","code_context_10":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","code_context_20":"@getmapping(\"\/{viewid}\/{rowid}\/field\/{fieldname}\/zoominto\") public jsonzoominto getrowfieldzoominto( @pathvariable(\"windowid\") final string windowidstr, @pathvariable(param_viewid) final string viewidstr, @pathvariable(\"rowid\") final string rowid, @pathvariable(\"fieldname\") final string fieldname) { \/\/ usersession.assertloggedin(); \/\/ note: not needed because we are forwarding to windowrestcontroller viewid.ofviewidstring(viewidstr, windowid.fromjson(windowidstr)); \/\/ just validate the windowid and viewid \/\/ todo: atm we are forwarding all calls to windowrestcontroller hoping the document existing and has the same id as view's row id. return windowrestcontroller.getdocumentfieldzoominto(windowidstr, rowid, fieldname); }","repo":"focadiz\/metasfresh"}
{"id":26006,"comment_id":0,"comment":"\/* potential flaw: use password directly in passwordauthentication() *\/","code":"public void bad() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).badsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","classification":"DEFECT","isFinished":true,"code_context_2":"if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring());","code_context_10":"public void bad() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).badsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","code_context_20":"public void bad() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).badsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","repo":"diktat-static-analysis\/juliet-benchmark-java"}
{"id":26007,"comment_id":0,"comment":"\/* goodg2b() - use goodsource and badsink *\/","code":"\/* goodg2b() - use goodsource and badsink *\/ private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","classification":"NONSATD","isFinished":true,"code_context_2":"private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","code_context_10":"private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","code_context_20":"private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","repo":"diktat-static-analysis\/juliet-benchmark-java"}
{"id":26007,"comment_id":1,"comment":"\/* potential flaw: use password directly in passwordauthentication() *\/","code":"private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","classification":"DEFECT","isFinished":true,"code_context_2":"if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring());","code_context_10":"private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","code_context_20":"private void goodg2b() throws throwable { string password = (new cwe319_cleartext_tx_sensitive_info__urlconnection_passwordauth_61b()).goodg2bsource(); if (password != null) { \/* potential flaw: use password directly in passwordauthentication() *\/ passwordauthentication credentials = new passwordauthentication(\"user\", password.tochararray()); io.writeline(credentials.tostring()); } }","repo":"diktat-static-analysis\/juliet-benchmark-java"}
{"id":34231,"comment_id":0,"comment":"\/\/ add tracing for this operation","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan();","code_context_10":"int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; }","code_context_20":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next();","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":1,"comment":"\/\/ at this point we are going through mutations for each table","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref);","code_context_10":"tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable();","code_context_20":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null)","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":2,"comment":"\/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely)","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn();","code_context_10":"try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst();","code_context_20":"long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":3,"comment":"\/\/ build map from physical table to mutation list","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) {","code_context_10":"valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false;","code_context_20":"} map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); }","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":4,"comment":"\/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure.","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values());","code_context_10":"while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed.","code_context_20":"continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue();","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":5,"comment":"\/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed.","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); }","code_context_10":"} \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue();","code_context_20":"boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0;","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":6,"comment":"\/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here?","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"DESIGN","isFinished":true,"code_context_2":"byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0;","code_context_10":"joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref();","code_context_20":"if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try {","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":7,"comment":"\/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers.","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null;","code_context_10":"long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback","code_context_20":"while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount);","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":8,"comment":"\/\/ track tables to which we've sent uncommitted data","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that","code_context_10":"table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size();","code_context_20":"int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename));","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":9,"comment":"\/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref);","code_context_10":"\/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount);","code_context_20":"long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime;","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":10,"comment":"\/\/ remove batches as we process them","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) {","code_context_10":"if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry.","code_context_20":"nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede;","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":11,"comment":"\/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry.","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection));","code_context_10":"numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede;","code_context_20":"batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric);","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":12,"comment":"\/\/ add a new child span as this one failed","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop();","code_context_10":"servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes();","code_context_20":"mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally {","repo":"cruisehz\/phoenix"}
{"id":34231,"comment_id":13,"comment":"\/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully.","code":"@suppresswarnings(\"deprecation\") private void send(iterator<tableref> tablerefiterator) throws sqlexception { int i = 0; long[] servertimestamps = null; boolean sendall = false; if (tablerefiterator == null) { servertimestamps = validateall(); tablerefiterator = mutations.keyset().iterator(); sendall = true; } map<immutablebytesptr, rowmutationstate> valuesmap; map<tableinfo,list<mutation>> physicaltablemutationmap = maps.newlinkedhashmap(); \/\/ add tracing for this operation try (tracescope trace = tracing.startnewspan(connection, \"committing mutations to tables\")) { span span = trace.getspan(); immutablebyteswritable indexmetadataptr = new immutablebyteswritable(); while (tablerefiterator.hasnext()) { \/\/ at this point we are going through mutations for each table final tableref tableref = tablerefiterator.next(); valuesmap = mutations.get(tableref); if (valuesmap == null || valuesmap.isempty()) { continue; } \/\/ validate as we go if transactional since we can undo if a problem occurs (which is unlikely) long servertimestamp = servertimestamps == null ? validateandgetservertimestamp(tableref, valuesmap) : servertimestamps[i++]; long scn = connection.getscn(); long mutationtimestamp = scn == null ? hconstants.latest_timestamp : scn; final ptable table = tableref.gettable(); iterator<pair<pname,list<mutation>>> mutationsiterator = addrowmutations(tableref, valuesmap, mutationtimestamp, servertimestamp, false, sendall); \/\/ build map from physical table to mutation list boolean isdatatable = true; while (mutationsiterator.hasnext()) { pair<pname,list<mutation>> pair = mutationsiterator.next(); pname htablename = pair.getfirst(); list<mutation> mutationlist = pair.getsecond(); tableinfo tableinfo = new tableinfo(isdatatable, htablename, tableref); list<mutation> oldmutationlist = physicaltablemutationmap.put(tableinfo, mutationlist); if (oldmutationlist!=null) mutationlist.addall(0, oldmutationlist); isdatatable = false; } \/\/ for transactions, track the statement indexes as we send data \/\/ over because our commitexception should include all statements \/\/ involved in the transaction since none of them would have been \/\/ committed in the event of a failure. if (table.istransactional()) { adduncommittedstatementindexes(valuesmap.values()); if (txmutations.isempty()) { txmutations = maps.newhashmapwithexpectedsize(mutations.size()); } \/\/ keep all mutations we've encountered until a commit or rollback. \/\/ this is not ideal, but there's not good way to get the values back \/\/ in the event that we need to replay the commit. \/\/ copy tableref so we have the original ptable and know when the \/\/ indexes have changed. joinmutationstate(new tableref(tableref), valuesmap, txmutations); } } long servertimestamp = hconstants.latest_timestamp; iterator<entry<tableinfo, list<mutation>>> mutationsiterator = physicaltablemutationmap.entryset().iterator(); while (mutationsiterator.hasnext()) { entry<tableinfo, list<mutation>> pair = mutationsiterator.next(); tableinfo tableinfo = pair.getkey(); byte[] htablename = tableinfo.gethtablename().getbytes(); list<mutation> mutationlist = pair.getvalue(); \/\/create a span per target table \/\/todo maybe we can be smarter about the table name to string here? span child = tracing.child(span,\"writing mutation batch for table: \"+bytes.tostring(htablename)); int retrycount = 0; boolean shouldretry = false; long nummutations = 0; long mutationsizebytes = 0; long mutationcommittime = 0; long numfailedmutations = 0;; long starttime = 0; do { tableref origtableref = tableinfo.getorigtableref(); ptable table = origtableref.gettable(); table.getindexmaintainers(indexmetadataptr, connection); final servercache cache = tableinfo.isdatatable() ? setmetadataonmutations(origtableref, mutationlist, indexmetadataptr) : null; \/\/ if we haven't retried yet, retry for this case only, as it's possible that \/\/ a split will occur after we send the index metadata cache to all known \/\/ region servers. shouldretry = cache!=null; sqlexception sqle = null; htableinterface htable = connection.getqueryservices().gettable(htablename); try { if (table.istransactional()) { \/\/ track tables to which we've sent uncommitted data uncommittedphysicalnames.add(table.getphysicalname().getstring()); \/\/ if we have indexes, wrap the htable in a delegate htable that \/\/ will attach the necessary index meta data in the event of a \/\/ rollback if (!table.getindexes().isempty()) { htable = new metadataawarehtable(htable, origtableref); } htable = transactionutil.getphoenixtransactiontable(phoenixtransactioncontext, htable, table); } nummutations = mutationlist.size(); global_mutation_batch_size.update(nummutations); mutationsizebytes = calculatemutationsize(mutationlist); starttime = system.currenttimemillis(); child.addtimelineannotation(\"attempt \" + retrycount); list<list<mutation>> mutationbatchlist = getmutationbatchlist(batchsize, batchsizebytes, mutationlist); for (list<mutation> mutationbatch : mutationbatchlist) { htable.batch(mutationbatch); batchcount++; } if (logger.isdebugenabled()) logger.debug(\"sent batch of \" + nummutations + \" for \" + bytes.tostring(htablename)); child.stop(); child.stop(); shouldretry = false; mutationcommittime = system.currenttimemillis() - starttime; global_mutation_commit_time.update(mutationcommittime); numfailedmutations = 0; if (tableinfo.isdatatable()) { numrows -= nummutations; } \/\/ remove batches as we process them mutations.remove(origtableref); } catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); } } if (sqle != null) { throw sqle; } } } } while (shouldretry && retrycount++ < 1); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp);","code_context_10":"logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close();","code_context_20":"} catch (exception e) { mutationcommittime = system.currenttimemillis() - starttime; servertimestamp = serverutil.parseservertimestamp(e); sqlexception inferrede = serverutil.parseserverexceptionornull(e); if (inferrede != null) { if (shouldretry && retrycount == 0 && inferrede.geterrorcode() == sqlexceptioncode.index_metadata_not_found.geterrorcode()) { \/\/ swallow this exception once, as it's possible that we split after sending the index metadata \/\/ and one of the region servers doesn't have it. this will cause it to have it the next go around. \/\/ if it fails again, we don't retry. string msg = \"swallowing exception and retrying after clearing meta cache on connection. \" + inferrede; logger.warn(logutil.addcustomannotations(msg, connection)); connection.getqueryservices().cleartableregioncache(htablename); \/\/ add a new child span as this one failed child.addtimelineannotation(msg); child.stop(); child = tracing.child(span,\"failed batch, attempting retry\"); continue; } e = inferrede; } \/\/ throw to client an exception that indicates the statements that \/\/ were not committed successfully. int[] uncommittedstatementindexes = getuncommittedstatementindexes(); sqle = new commitexception(e, uncommittedstatementindexes, servertimestamp); numfailedmutations = uncommittedstatementindexes.length; global_mutation_batch_failed_count.update(numfailedmutations); } finally { mutationmetric mutationsmetric = new mutationmetric(nummutations, mutationsizebytes, mutationcommittime, numfailedmutations); mutationmetricqueue.addmetricsfortable(bytes.tostring(htablename), mutationsmetric); try { if (cache!=null) cache.close(); } finally { try { htable.close(); } catch (ioexception e) { if (sqle != null) { sqle.setnextexception(serverutil.parseserverexception(e)); } else { sqle = serverutil.parseserverexception(e); }","repo":"cruisehz\/phoenix"}
{"id":1597,"comment_id":0,"comment":"\/\/ todo(#496): replace with actual study's org name.","code":"@transactional() @override public emailresponse resendconfirmationthroughemail( string applicationid, string securitytoken, string emailid, string appname) { logger.info(\"usermanagementprofileserviceimpl - resendconfirmationthroughemail() - starts\"); appentity apppropertiesdetails = null; string content = \"\"; string subject = \"\"; apporginfobean apporginfobean = null; apporginfobean = commondao.getuserappdetailsbyallapi(\"\", applicationid); apppropertiesdetails = userprofilemanagementdao.getapppropertiesdetailsbyappid(apporginfobean.getappinfoid()); map<string, string> templateargs = new hashmap<>(); if ((apppropertiesdetails == null) || (apppropertiesdetails.getregemailsub() == null) || (apppropertiesdetails.getregemailbody() == null) || apppropertiesdetails.getregemailbody().equalsignorecase(\"\") || apppropertiesdetails.getregemailsub().equalsignorecase(\"\")) { subject = appconfig.getconfirmationmailsubject(); content = appconfig.getconfirmationmail(); } else { content = apppropertiesdetails.getregemailbody(); subject = apppropertiesdetails.getregemailsub(); } templateargs.put(\"appname\", appname); \/\/ todo(#496): replace with actual study's org name. templateargs.put(\"orgname\", appconfig.getorgname()); templateargs.put(\"contactemail\", appconfig.getcontactemail()); templateargs.put(\"securitytoken\", securitytoken); emailrequest emailrequest = new emailrequest( appconfig.getfromemail(), new string[] {emailid}, null, null, subject, content, templateargs); logger.info(\"usermanagementprofileserviceimpl - resendconfirmationthroughemail() - ends\"); return emailservice.sendmimemail(emailrequest); }","classification":"DESIGN","isFinished":true,"code_context_2":"} templateargs.put(\"appname\", appname); \/\/ todo(#496): replace with actual study's org name. templateargs.put(\"orgname\", appconfig.getorgname()); templateargs.put(\"contactemail\", appconfig.getcontactemail());","code_context_10":"|| (apppropertiesdetails.getregemailbody() == null) || apppropertiesdetails.getregemailbody().equalsignorecase(\"\") || apppropertiesdetails.getregemailsub().equalsignorecase(\"\")) { subject = appconfig.getconfirmationmailsubject(); content = appconfig.getconfirmationmail(); } else { content = apppropertiesdetails.getregemailbody(); subject = apppropertiesdetails.getregemailsub(); } templateargs.put(\"appname\", appname); \/\/ todo(#496): replace with actual study's org name. templateargs.put(\"orgname\", appconfig.getorgname()); templateargs.put(\"contactemail\", appconfig.getcontactemail()); templateargs.put(\"securitytoken\", securitytoken); emailrequest emailrequest = new emailrequest( appconfig.getfromemail(), new string[] {emailid}, null, null, subject,","code_context_20":"appentity apppropertiesdetails = null; string content = \"\"; string subject = \"\"; apporginfobean apporginfobean = null; apporginfobean = commondao.getuserappdetailsbyallapi(\"\", applicationid); apppropertiesdetails = userprofilemanagementdao.getapppropertiesdetailsbyappid(apporginfobean.getappinfoid()); map<string, string> templateargs = new hashmap<>(); if ((apppropertiesdetails == null) || (apppropertiesdetails.getregemailsub() == null) || (apppropertiesdetails.getregemailbody() == null) || apppropertiesdetails.getregemailbody().equalsignorecase(\"\") || apppropertiesdetails.getregemailsub().equalsignorecase(\"\")) { subject = appconfig.getconfirmationmailsubject(); content = appconfig.getconfirmationmail(); } else { content = apppropertiesdetails.getregemailbody(); subject = apppropertiesdetails.getregemailsub(); } templateargs.put(\"appname\", appname); \/\/ todo(#496): replace with actual study's org name. templateargs.put(\"orgname\", appconfig.getorgname()); templateargs.put(\"contactemail\", appconfig.getcontactemail()); templateargs.put(\"securitytoken\", securitytoken); emailrequest emailrequest = new emailrequest( appconfig.getfromemail(), new string[] {emailid}, null, null, subject, content, templateargs); logger.info(\"usermanagementprofileserviceimpl - resendconfirmationthroughemail() - ends\"); return emailservice.sendmimemail(emailrequest); }","repo":"deepsinghchouhan\/fdamystudies"}
{"id":18031,"comment_id":0,"comment":"\/** * @fix the illegal group reference issue *\/","code":"private string replaceparam(string message, object... parameters) { int startsize = 0; int parametersindex = 0; int index; string tmpmessage = message; while ((index = message.indexof(\"{}\", startsize)) != -1) { if (parametersindex >= parameters.length) { break; } \/** * @fix the illegal group reference issue *\/ tmpmessage = tmpmessage.replacefirst(\"\\\\{\\\\}\", matcher.quotereplacement(string.valueof(parameters[parametersindex++]))); startsize = index + 2; } return tmpmessage; }","classification":"DEFECT","isFinished":true,"code_context_2":"break; } \/** * @fix the illegal group reference issue *\/ tmpmessage = tmpmessage.replacefirst(\"\\\\{\\\\}\", matcher.quotereplacement(string.valueof(parameters[parametersindex++]))); startsize = index + 2;","code_context_10":"private string replaceparam(string message, object... parameters) { int startsize = 0; int parametersindex = 0; int index; string tmpmessage = message; while ((index = message.indexof(\"{}\", startsize)) != -1) { if (parametersindex >= parameters.length) { break; } \/** * @fix the illegal group reference issue *\/ tmpmessage = tmpmessage.replacefirst(\"\\\\{\\\\}\", matcher.quotereplacement(string.valueof(parameters[parametersindex++]))); startsize = index + 2; } return tmpmessage; }","code_context_20":"private string replaceparam(string message, object... parameters) { int startsize = 0; int parametersindex = 0; int index; string tmpmessage = message; while ((index = message.indexof(\"{}\", startsize)) != -1) { if (parametersindex >= parameters.length) { break; } \/** * @fix the illegal group reference issue *\/ tmpmessage = tmpmessage.replacefirst(\"\\\\{\\\\}\", matcher.quotereplacement(string.valueof(parameters[parametersindex++]))); startsize = index + 2; } return tmpmessage; }","repo":"erda-project\/erda-java-extensions"}
{"id":26234,"comment_id":0,"comment":"\/\/todo replace this with java.util.collections!","code":"@get @permitall @path(\"\/server\/info\") @produces(mediatype.application_json) public response getserverinformation() throws apiexception { map<string, object> returnmap = new hashmap<string, object>(); list<object> configurations = new arraylist<object>(); appconstants appconstants = appconstants.getinstance(); for (configuration configuration : getibismanager().getconfigurations()) { map<string, object> cfg = new hashmap<string, object>(); cfg.put(\"name\", configuration.getname()); cfg.put(\"version\", configuration.getversion()); cfg.put(\"stubbed\", configuration.isstubbed()); cfg.put(\"type\", configuration.getclassloadertype()); if(configuration.getconfigurationexception() != null) { cfg.put(\"exception\", configuration.getconfigurationexception().getmessage()); } classloader classloader = configuration.getclassloader(); if(classloader instanceof databaseclassloader) { cfg.put(\"filename\", ((databaseclassloader) classloader).getfilename()); cfg.put(\"created\", ((databaseclassloader) classloader).getcreationdate()); cfg.put(\"user\", ((databaseclassloader) classloader).getuser()); } string parentconfig = appconstants.getinstance().getstring(\"configurations.\" + configuration.getname() + \".parentconfig\", null); if(parentconfig != null) cfg.put(\"parent\", parentconfig); configurations.add(cfg); } \/\/todo replace this with java.util.collections! collections.sort(configurations, new comparator<map<string, string>>() { @override public int compare(map<string, string> lhs, map<string, string> rhs) { string name1 = lhs.get(\"name\"); string name2 = rhs.get(\"name\"); return name1.startswith(\"iaf_\") ? -1 : name2.startswith(\"iaf_\") ? 1 : name1.compareto(name2); } }); returnmap.put(\"configurations\", configurations); map<string, object> framework = new hashmap<string, object>(2); framework.put(\"name\", \"ff!\"); framework.put(\"version\", appconstants.getproperty(\"application.version\")); returnmap.put(\"framework\", framework); map<string, object> instance = new hashmap<string, object>(2); instance.put(\"version\", appconstants.getproperty(\"instance.version\")); instance.put(\"name\", getibiscontext().getapplicationname()); returnmap.put(\"instance\", instance); string dtapstage = appconstants.getproperty(\"dtap.stage\"); returnmap.put(\"dtap.stage\", dtapstage); string dtapside = appconstants.getproperty(\"dtap.side\"); returnmap.put(\"dtap.side\", dtapside); returnmap.put(\"applicationserver\", servletconfig.getservletcontext().getserverinfo()); returnmap.put(\"javaversion\", system.getproperty(\"java.runtime.name\") + \" (\" + system.getproperty(\"java.runtime.version\") + \")\"); map<string, object> filesystem = new hashmap<string, object>(2); filesystem.put(\"totalspace\", misc.getfilesystemtotalspace()); filesystem.put(\"freespace\", misc.getfilesystemfreespace()); returnmap.put(\"filesystem\", filesystem); returnmap.put(\"processmetrics\", processmetrics.tomap()); date date = new date(); returnmap.put(\"servertime\", date.gettime()); returnmap.put(\"machinename\" , misc.gethostname()); applicationmetrics metrics = getibiscontext().getbean(\"metrics\", applicationmetrics.class); returnmap.put(\"uptime\", (metrics != null) ? metrics.getuptimedate() : \"\"); return response.status(response.status.ok).entity(returnmap).build(); }","classification":"DESIGN","isFinished":true,"code_context_2":"configurations.add(cfg); } \/\/todo replace this with java.util.collections! collections.sort(configurations, new comparator<map<string, string>>() { @override","code_context_10":"if(classloader instanceof databaseclassloader) { cfg.put(\"filename\", ((databaseclassloader) classloader).getfilename()); cfg.put(\"created\", ((databaseclassloader) classloader).getcreationdate()); cfg.put(\"user\", ((databaseclassloader) classloader).getuser()); } string parentconfig = appconstants.getinstance().getstring(\"configurations.\" + configuration.getname() + \".parentconfig\", null); if(parentconfig != null) cfg.put(\"parent\", parentconfig); configurations.add(cfg); } \/\/todo replace this with java.util.collections! collections.sort(configurations, new comparator<map<string, string>>() { @override public int compare(map<string, string> lhs, map<string, string> rhs) { string name1 = lhs.get(\"name\"); string name2 = rhs.get(\"name\"); return name1.startswith(\"iaf_\") ? -1 : name2.startswith(\"iaf_\") ? 1 : name1.compareto(name2); } }); returnmap.put(\"configurations\", configurations); map<string, object> framework = new hashmap<string, object>(2);","code_context_20":"for (configuration configuration : getibismanager().getconfigurations()) { map<string, object> cfg = new hashmap<string, object>(); cfg.put(\"name\", configuration.getname()); cfg.put(\"version\", configuration.getversion()); cfg.put(\"stubbed\", configuration.isstubbed()); cfg.put(\"type\", configuration.getclassloadertype()); if(configuration.getconfigurationexception() != null) { cfg.put(\"exception\", configuration.getconfigurationexception().getmessage()); } classloader classloader = configuration.getclassloader(); if(classloader instanceof databaseclassloader) { cfg.put(\"filename\", ((databaseclassloader) classloader).getfilename()); cfg.put(\"created\", ((databaseclassloader) classloader).getcreationdate()); cfg.put(\"user\", ((databaseclassloader) classloader).getuser()); } string parentconfig = appconstants.getinstance().getstring(\"configurations.\" + configuration.getname() + \".parentconfig\", null); if(parentconfig != null) cfg.put(\"parent\", parentconfig); configurations.add(cfg); } \/\/todo replace this with java.util.collections! collections.sort(configurations, new comparator<map<string, string>>() { @override public int compare(map<string, string> lhs, map<string, string> rhs) { string name1 = lhs.get(\"name\"); string name2 = rhs.get(\"name\"); return name1.startswith(\"iaf_\") ? -1 : name2.startswith(\"iaf_\") ? 1 : name1.compareto(name2); } }); returnmap.put(\"configurations\", configurations); map<string, object> framework = new hashmap<string, object>(2); framework.put(\"name\", \"ff!\"); framework.put(\"version\", appconstants.getproperty(\"application.version\")); returnmap.put(\"framework\", framework); map<string, object> instance = new hashmap<string, object>(2); instance.put(\"version\", appconstants.getproperty(\"instance.version\")); instance.put(\"name\", getibiscontext().getapplicationname()); returnmap.put(\"instance\", instance); string dtapstage = appconstants.getproperty(\"dtap.stage\"); returnmap.put(\"dtap.stage\", dtapstage); string dtapside = appconstants.getproperty(\"dtap.side\");","repo":"francisgw\/iaf"}
{"id":18158,"comment_id":0,"comment":"\/** * creates a db and populates it with the sql statements in sqlstatements. * * @param context the context to use to create the db * @param dbname the name of the db to create * @param dbversion the version to set on the db * @param sqlstatements the statements to use to populate the db. this should be a single string * of the form returned by sqlite3's <tt>.dump<\/tt> command (statements separated by * semicolons) *\/ \/* static public void createdbfromsqlstatements( context context, string dbname, int dbversion, string sqlstatements) { sqlitedatabase db = context.openorcreatedatabase(dbname, 0, null); \/\/ todo: this is not quite safe since it assumes that all semicolons at the end of a line \/\/ terminate statements. it is possible that a text field contains ;\\n. we will have to fix \/\/ this if that turns out to be a problem. string[] statements = textutils.split(sqlstatements, \";\\n\"); for (string statement : statements) { if (textutils.isempty(statement)) continue; db.execsql(statement); } db.setversion(dbversion); db.close(); } *\/ \/** * returns one of the following which represent the type of the given sql statement. * <ol> * <li>{@link #statement_select}<\/li> * <li>{@link #statement_update}<\/li> * <li>{@link #statement_attach}<\/li> * <li>{@link #statement_begin}<\/li> * <li>{@link #statement_commit}<\/li> * <li>{@link #statement_abort}<\/li> * <li>{@link #statement_other}<\/li> * <\/ol> * @param sql the sql statement whose type is returned by this method * @return one of the values listed above *\/","code":"\/** * creates a db and populates it with the sql statements in sqlstatements. * * @param context the context to use to create the db * @param dbname the name of the db to create * @param dbversion the version to set on the db * @param sqlstatements the statements to use to populate the db. this should be a single string * of the form returned by sqlite3's <tt>.dump<\/tt> command (statements separated by * semicolons) *\/ \/* static public void createdbfromsqlstatements( context context, string dbname, int dbversion, string sqlstatements) { sqlitedatabase db = context.openorcreatedatabase(dbname, 0, null); \/\/ todo: this is not quite safe since it assumes that all semicolons at the end of a line \/\/ terminate statements. it is possible that a text field contains ;\\n. we will have to fix \/\/ this if that turns out to be a problem. string[] statements = textutils.split(sqlstatements, \";\\n\"); for (string statement : statements) { if (textutils.isempty(statement)) continue; db.execsql(statement); } db.setversion(dbversion); db.close(); } *\/ \/** * returns one of the following which represent the type of the given sql statement. * <ol> * <li>{@link #statement_select}<\/li> * <li>{@link #statement_update}<\/li> * <li>{@link #statement_attach}<\/li> * <li>{@link #statement_begin}<\/li> * <li>{@link #statement_commit}<\/li> * <li>{@link #statement_abort}<\/li> * <li>{@link #statement_other}<\/li> * <\/ol> * @param sql the sql statement whose type is returned by this method * @return one of the values listed above *\/ public static int getsqlstatementtype(string sql) { sql = sql.trim(); if (sql.length() < 3) { return statement_other; } string prefixsql = sql.substring(0, 3).touppercase(locale.root); if (prefixsql.equals(\"sel\")) { return statement_select; } else if (prefixsql.equals(\"ins\") || prefixsql.equals(\"upd\") || prefixsql.equals(\"rep\") || prefixsql.equals(\"del\")) { return statement_update; } else if (prefixsql.equals(\"att\")) { return statement_attach; } else if (prefixsql.equals(\"com\")) { return statement_commit; } else if (prefixsql.equals(\"end\")) { return statement_commit; } else if (prefixsql.equals(\"rol\")) { return statement_abort; } else if (prefixsql.equals(\"beg\")) { return statement_begin; } else if (prefixsql.equals(\"pra\")) { return statement_pragma; } else if (prefixsql.equals(\"cre\") || prefixsql.equals(\"dro\") || prefixsql.equals(\"alt\")) { return statement_ddl; } else if (prefixsql.equals(\"ana\") || prefixsql.equals(\"det\")) { return statement_unprepared; } return statement_other; }","classification":"NONSATD","isFinished":true,"code_context_2":"public static int getsqlstatementtype(string sql) { sql = sql.trim(); if (sql.length() < 3) { return statement_other; } string prefixsql = sql.substring(0, 3).touppercase(locale.root); if (prefixsql.equals(\"sel\")) { return statement_select; } else if (prefixsql.equals(\"ins\") || prefixsql.equals(\"upd\") || prefixsql.equals(\"rep\") || prefixsql.equals(\"del\")) { return statement_update; } else if (prefixsql.equals(\"att\")) { return statement_attach; } else if (prefixsql.equals(\"com\")) { return statement_commit; } else if (prefixsql.equals(\"end\")) { return statement_commit; } else if (prefixsql.equals(\"rol\")) { return statement_abort; } else if (prefixsql.equals(\"beg\")) { return statement_begin; } else if (prefixsql.equals(\"pra\")) { return statement_pragma; } else if (prefixsql.equals(\"cre\") || prefixsql.equals(\"dro\") || prefixsql.equals(\"alt\")) { return statement_ddl; } else if (prefixsql.equals(\"ana\") || prefixsql.equals(\"det\")) { return statement_unprepared; } return statement_other; }","code_context_10":"public static int getsqlstatementtype(string sql) { sql = sql.trim(); if (sql.length() < 3) { return statement_other; } string prefixsql = sql.substring(0, 3).touppercase(locale.root); if (prefixsql.equals(\"sel\")) { return statement_select; } else if (prefixsql.equals(\"ins\") || prefixsql.equals(\"upd\") || prefixsql.equals(\"rep\") || prefixsql.equals(\"del\")) { return statement_update; } else if (prefixsql.equals(\"att\")) { return statement_attach; } else if (prefixsql.equals(\"com\")) { return statement_commit; } else if (prefixsql.equals(\"end\")) { return statement_commit; } else if (prefixsql.equals(\"rol\")) { return statement_abort; } else if (prefixsql.equals(\"beg\")) { return statement_begin; } else if (prefixsql.equals(\"pra\")) { return statement_pragma; } else if (prefixsql.equals(\"cre\") || prefixsql.equals(\"dro\") || prefixsql.equals(\"alt\")) { return statement_ddl; } else if (prefixsql.equals(\"ana\") || prefixsql.equals(\"det\")) { return statement_unprepared; } return statement_other; }","code_context_20":"public static int getsqlstatementtype(string sql) { sql = sql.trim(); if (sql.length() < 3) { return statement_other; } string prefixsql = sql.substring(0, 3).touppercase(locale.root); if (prefixsql.equals(\"sel\")) { return statement_select; } else if (prefixsql.equals(\"ins\") || prefixsql.equals(\"upd\") || prefixsql.equals(\"rep\") || prefixsql.equals(\"del\")) { return statement_update; } else if (prefixsql.equals(\"att\")) { return statement_attach; } else if (prefixsql.equals(\"com\")) { return statement_commit; } else if (prefixsql.equals(\"end\")) { return statement_commit; } else if (prefixsql.equals(\"rol\")) { return statement_abort; } else if (prefixsql.equals(\"beg\")) { return statement_begin; } else if (prefixsql.equals(\"pra\")) { return statement_pragma; } else if (prefixsql.equals(\"cre\") || prefixsql.equals(\"dro\") || prefixsql.equals(\"alt\")) { return statement_ddl; } else if (prefixsql.equals(\"ana\") || prefixsql.equals(\"det\")) { return statement_unprepared; } return statement_other; }","repo":"finki001\/couchbase-lite-java-core"}
{"id":34557,"comment_id":0,"comment":"\/\/ probably incorrect - comparing object[] arrays with arrays.equals","code":"@override public boolean equals(object o) { if (this == o) return true; if (o == null || getclass() != o.getclass()) return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(jobs, that.jobs)) return false; if (list != null ? !list.equals(that.list) : that.list != null) return false; if (map != null ? !map.equals(that.map) : that.map != null) return false; return clazz != null ? clazz.equals(that.clazz) : that.clazz == null; }","classification":"DEFECT","isFinished":true,"code_context_2":"return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false;","code_context_10":"@override public boolean equals(object o) { if (this == o) return true; if (o == null || getclass() != o.getclass()) return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(jobs, that.jobs)) return false; if (list != null ? !list.equals(that.list) : that.list != null) return false; if (map != null ? !map.equals(that.map) : that.map != null) return false; return clazz != null ? clazz.equals(that.clazz) : that.clazz == null;","code_context_20":"@override public boolean equals(object o) { if (this == o) return true; if (o == null || getclass() != o.getclass()) return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(jobs, that.jobs)) return false; if (list != null ? !list.equals(that.list) : that.list != null) return false; if (map != null ? !map.equals(that.map) : that.map != null) return false; return clazz != null ? clazz.equals(that.clazz) : that.clazz == null; }","repo":"dbl-x\/sofa-rpc"}
{"id":34557,"comment_id":1,"comment":"\/\/ probably incorrect - comparing object[] arrays with arrays.equals","code":"@override public boolean equals(object o) { if (this == o) return true; if (o == null || getclass() != o.getclass()) return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(jobs, that.jobs)) return false; if (list != null ? !list.equals(that.list) : that.list != null) return false; if (map != null ? !map.equals(that.map) : that.map != null) return false; return clazz != null ? clazz.equals(that.clazz) : that.clazz == null; }","classification":"DEFECT","isFinished":true,"code_context_2":"return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false;","code_context_10":"@override public boolean equals(object o) { if (this == o) return true; if (o == null || getclass() != o.getclass()) return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(jobs, that.jobs)) return false; if (list != null ? !list.equals(that.list) : that.list != null) return false; if (map != null ? !map.equals(that.map) : that.map != null) return false; return clazz != null ? clazz.equals(that.clazz) : that.clazz == null;","code_context_20":"@override public boolean equals(object o) { if (this == o) return true; if (o == null || getclass() != o.getclass()) return false; complexbean that = (complexbean) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(strs, that.strs)) return false; \/\/ probably incorrect - comparing object[] arrays with arrays.equals if (!arrays.equals(jobs, that.jobs)) return false; if (list != null ? !list.equals(that.list) : that.list != null) return false; if (map != null ? !map.equals(that.map) : that.map != null) return false; return clazz != null ? clazz.equals(that.clazz) : that.clazz == null; }","repo":"dbl-x\/sofa-rpc"}
{"id":10052,"comment_id":0,"comment":"\/\/ first see if we can access the clipboard","code":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb; cb = toolkit.getdefaulttoolkit().getsystemclipboard(); stringselection sel; sel = new stringselection(strsel); cb.setcontents(sel, sel); } }.start(); }","classification":"NONSATD","isFinished":true,"code_context_2":"return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard.","code_context_10":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) {","code_context_20":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb;","repo":"css4j\/echosvg"}
{"id":10052,"comment_id":1,"comment":"\/\/ can't access clipboard.","code":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb; cb = toolkit.getdefaulttoolkit().getsystemclipboard(); stringselection sel; sel = new stringselection(strsel); cb.setcontents(sel, sel); } }.start(); }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; }","code_context_10":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); }","code_context_20":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb; cb = toolkit.getdefaulttoolkit().getsystemclipboard(); stringselection sel;","repo":"css4j\/echosvg"}
{"id":10052,"comment_id":2,"comment":"\/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that.","code":"@override public void selectiondone(selectionevent evt) { if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb; cb = toolkit.getdefaulttoolkit().getsystemclipboard(); stringselection sel; sel = new stringselection(strsel); cb.setcontents(sel, sel); } }.start(); }","classification":"DESIGN","isFinished":true,"code_context_2":"} final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override","code_context_10":"} int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb; cb = toolkit.getdefaulttoolkit().getsystemclipboard(); stringselection sel; sel = new stringselection(strsel); cb.setcontents(sel, sel); } }.start();","code_context_20":"if (!useunixtextselection) return; object o = evt.getselection(); if (!(o instanceof characteriterator)) return; characteriterator iter = (characteriterator) o; \/\/ first see if we can access the clipboard if (!permissionchecker.getinstance().checkpermission(new awtpermission(\"accessclipboard\"))) { \/\/ can't access clipboard. return; } int sz = iter.getendindex() - iter.getbeginindex(); if (sz == 0) return; char[] cbuff = new char[sz]; cbuff[0] = iter.first(); for (int i = 1; i < cbuff.length; ++i) { cbuff[i] = iter.next(); } final string strsel = new string(cbuff); \/\/ hack: getsystemclipboard sometimes deadlocks on \/\/ linux when called from the awt thread. the thread \/\/ creation prevents that. new thread() { @override public void run() { clipboard cb; cb = toolkit.getdefaulttoolkit().getsystemclipboard(); stringselection sel; sel = new stringselection(strsel); cb.setcontents(sel, sel); } }.start(); }","repo":"css4j\/echosvg"}
{"id":10102,"comment_id":0,"comment":"\/\/ string contextrelativepath = httprequest.getservletpath();","code":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","classification":"NONSATD","isFinished":true,"code_context_2":"return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath);","code_context_10":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) {","code_context_20":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try {","repo":"csrster\/openwayback-csrdev"}
{"id":10102,"comment_id":1,"comment":"\/\/ string abspath = getservletcontext().getrealpath(contextrelativepath);","code":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) {","code_context_10":"httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; }","code_context_20":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true;","repo":"csrster\/openwayback-csrdev"}
{"id":10102,"comment_id":2,"comment":"\/\/ik: added null check for abspath, it may be null (ex. on jetty)","code":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","classification":"NONSATD","isFinished":true,"code_context_2":"mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath);","code_context_10":"if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this);","code_context_20":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","repo":"csrster\/openwayback-csrdev"}
{"id":10102,"comment_id":3,"comment":"\/\/ wbrequest.setcontextprefix(geturlroot());","code":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","classification":"NONSATD","isFinished":true,"code_context_2":"string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest);","code_context_10":"} \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false;","code_context_20":"} if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","repo":"csrster\/openwayback-csrdev"}
{"id":10102,"comment_id":4,"comment":"\/\/ todo: figure out if we got io because of a missing dispatcher","code":"protected boolean dispatchlocal(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { if (logger.isloggable(level.fine)) { logger.fine(\"local dispatch \/\" + translaterequestpath(httprequest)); } if (!servestatic) { return false; } \/\/ string contextrelativepath = httprequest.getservletpath(); string translatednoquery = \"\/\" + translaterequestpath(httprequest); \/\/ string abspath = getservletcontext().getrealpath(contextrelativepath); string abspath = getservletcontext().getrealpath(translatednoquery); if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","classification":"DESIGN","isFinished":true,"code_context_2":"return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false;","code_context_10":"string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","code_context_20":"if (this.isenablememento()) { mementoutils.adddonotnegotiateheader(httpresponse); } \/\/ik: added null check for abspath, it may be null (ex. on jetty) if (abspath != null) { file test = new file(abspath); if((test != null) && !test.exists()) { return false; } } string translatedq = \"\/\" + translaterequestpathquery(httprequest); waybackrequest wbrequest = new waybackrequest(); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.setaccesspoint(this); wbrequest.extracthttprequestinfo(httprequest); uiresults uiresults = new uiresults(wbrequest,uriconverter); try { uiresults.forward(httprequest, httpresponse, translatedq); return true; } catch(ioexception e) { \/\/ todo: figure out if we got io because of a missing dispatcher } return false; }","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":0,"comment":"\/** * @param httprequest httpservletrequest which is being handled * @param httpresponse httpservletresponse which is being handled * @return true if the request was actually handled * @throws servletexception per usual * @throws ioexception per usual *\/","code":"\/** * @param httprequest httpservletrequest which is being handled * @param httpresponse httpservletresponse which is being handled * @return true if the request was actually handled * @throws servletexception per usual * @throws ioexception per usual *\/ public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","code_context_10":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","code_context_20":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":1,"comment":"\/\/ todo: refactor this code into requestparser implementations","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"DESIGN","isFinished":true,"code_context_2":"if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest));","code_context_10":"perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); }","code_context_20":"httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch());","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":2,"comment":"\/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot());","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor","code_context_10":"string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) {","code_context_20":"waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":3,"comment":"\/\/ end of refactor","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) {","code_context_10":"\" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); }","code_context_20":"perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest);","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":4,"comment":"\/\/ set exclusionfilter on wbrequest only if not set externally","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"} } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter());","code_context_10":"\/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) {","code_context_20":"string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) {","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":5,"comment":"\/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"DESIGN","isFinished":true,"code_context_2":"wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) {","code_context_10":"if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; }","code_context_20":"logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; }","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":6,"comment":"\/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place:","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix;","code_context_10":"if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place:","code_context_20":"\/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse);","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":7,"comment":"\/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place:","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix;","code_context_10":"if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place:","code_context_20":"\/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse);","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":8,"comment":"\/\/ cause perf headers to be committed","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"} catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) {","code_context_10":"return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest);","code_context_20":"return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); }","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":9,"comment":"\/\/ if not liveweb redirected, then render current exception","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found);","code_context_10":"if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders();","code_context_20":"handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","repo":"csrster\/openwayback-csrdev"}
{"id":10103,"comment_id":10,"comment":"\/\/slightly hacky, but ensures that all block loaders are closed","code":"public boolean handlerequest(httpservletrequest httprequest, httpservletresponse httpresponse) throws servletexception, ioexception { waybackrequest wbrequest = null; boolean handled = false; try { perfstats.clearall(); if (this.isenableperfstatsheader() && (perfstatsheader != null)) { perfstats.timestart(perfstat.total); httpresponse = new perfwritinghttpservletresponse(httprequest, httpresponse, perfstat.total, perfstatsheader, perfstatsheaderformat); } string inputpath = translaterequestpathquery(httprequest); thread.currentthread().setname(\"thread \" + thread.currentthread().getid() + \" \" + getbeanname() + \" handling: \" + inputpath); logger.fine(\"handling translated: \" + inputpath); wbrequest = getparser().parse(httprequest, this); if (wbrequest != null) { handled = true; \/\/ todo: refactor this code into requestparser implementations wbrequest.setaccesspoint(this); \/\/ wbrequest.setcontextprefix(getabsolutelocalprefix(httprequest)); \/\/ wbrequest.setcontextprefix(geturlroot()); wbrequest.extracthttprequestinfo(httprequest); \/\/ end of refactor if (getauthentication() != null) { if (!getauthentication().istrue(wbrequest)) { throw new authenticationcontrolexception( \"unauthorized\", isrequestauth()); } } \/\/ set exclusionfilter on wbrequest only if not set externally if (wbrequest.getexclusionfilter() == null) { wbrequest.setexclusionfilter(createexclusionfilter()); } \/\/ todo: refactor this into requestparser implementations, so a \/\/ user could alter requests to change the behavior within a \/\/ single accesspoint. for now, this is a simple way to expose \/\/ the feature to configuration.g wbrequest.setexactscheme(isexactschemematch()); if (wbrequest.isreplayrequest()) { if (bouncetoreplayprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = replayprefix + suffix; httpresponse.sendredirect(replayurl); return true; } handlereplay(wbrequest, httprequest, httpresponse); } else { if (bouncetoqueryprefix) { \/\/ we don't accept replay requests on this accesspoint \/\/ bounce the user to the right place: string suffix = translaterequestpathquery(httprequest); string replayurl = queryprefix + suffix; httpresponse.sendredirect(replayurl); return true; } wbrequest.setexacthost(isexacthostmatch()); handlequery(wbrequest, httprequest, httpresponse); } } else { handled = dispatchlocal(httprequest, httpresponse); } } catch (betterrequestexception e) { e.generateresponse(httpresponse, wbrequest); httpresponse.getwriter(); \/\/ cause perf headers to be committed handled = true; } catch (waybackexception e) { if (httpresponse.iscommitted()) { return true; } if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","classification":"NONSATD","isFinished":true,"code_context_2":"logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); }","code_context_10":"} \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","code_context_20":"} if (wbrequest == null) { wbrequest = new waybackrequest(); wbrequest.setaccesspoint(this); } logerror(httpresponse, errormsgheader, e, wbrequest); livewebstate livewebstate = livewebstate.not_found; if ((getlivewebredirector() != null) && !wbrequest.hasmementoacceptdatetime() && !wbrequest.ismementotimemaprequest()) { livewebstate = getlivewebredirector().handleredirect(e, wbrequest, httprequest, httpresponse); } \/\/ if not liveweb redirected, then render current exception if (livewebstate != livewebstate.redirected) { e.setlivewebavailable(livewebstate == livewebstate.found); getexception().renderexception(httprequest, httpresponse, wbrequest, e, geturiconverter()); } handled = true; } catch (exception other) { logerror(httpresponse, errormsgheader, other, wbrequest); } finally { \/\/slightly hacky, but ensures that all block loaders are closed zipnumblockloader.closeallreaders(); } return handled; }","repo":"csrster\/openwayback-csrdev"}
{"id":1957,"comment_id":0,"comment":"\/\/ delete associated relation annotation","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) {","code_context_10":"jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset,","code_context_20":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset,","repo":"debovis\/webanno"}
{"id":1957,"comment_id":1,"comment":"\/\/ begin hack - issue 933","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"} } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false);","code_context_10":"if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber);","code_context_20":"if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model))","repo":"debovis\/webanno"}
{"id":1957,"comment_id":2,"comment":"\/\/ end hack - issue 933","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(),","code_context_10":"sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset);","code_context_20":"sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); }","repo":"debovis\/webanno"}
{"id":1957,"comment_id":3,"comment":"\/\/ update timestamp now","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber);","code_context_10":"} } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations","code_context_20":"} string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer);","repo":"debovis\/webanno"}
{"id":1957,"comment_id":4,"comment":"\/\/ store latest annotations","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model))","code_context_10":"\/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer);","code_context_20":"} } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","repo":"debovis\/webanno"}
{"id":1957,"comment_id":5,"comment":"\/\/ a hack to rememeber the visural dropdown display \/\/ value","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"} info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues);","code_context_10":"bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","code_context_20":"repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","repo":"debovis\/webanno"}
{"id":1957,"comment_id":6,"comment":"\/\/ setlayerandfeaturemodels(jcas);","code":"private void actiondelete(ajaxrequesttarget atarget) throws ioexception, uimaexception, classnotfoundexception { bratannotatormodel bratannotatormodel = getmodelobject(); jcas jcas = getcas(bratannotatormodel); annotationfs fs = selectbyaddr(jcas, selectedannotationid); typeadapter adapter = getadapter(selectedannotationlayer); string attachfeaturename = adapter.getattachfeaturename(); string attachtypename = adapter.getannotationtypename(); set<typeadapter> typeadapters = new hashset<typeadapter>(); for (annotationlayer layer : annotationservice.listannotationlayer(bratannotatormodel .getproject())) { typeadapters.add(getadapter(layer)); } \/\/ delete associated relation annotation for (typeadapter ad : typeadapters) { if (adapter.getannotationtypename().equals(ad.getannotationtypename())) { continue; } string tn = ad.getattachtypename(); if (tn == null) { continue; } if (tn.equals(attachtypename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } string fn = ad.getattachfeaturename(); if (fn == null) { continue; } if (fn.equals(attachfeaturename)) { sentence thissentence = bratajaxcasutil.getcurrentsentence(jcas, beginoffset, endoffset); ad.deletebyspan(jcas, fs, thissentence.getbegin(), thissentence.getend()); break; } } \/\/ begin hack - issue 933 if (adapter instanceof chainadapter) { ((chainadapter) adapter).setarc(false); } \/\/ end hack - issue 933 adapter.delete(jcas, selectedannotationid); repository.updatejcas(bratannotatormodel.getmode(), bratannotatormodel.getdocument(), bratannotatormodel.getuser(), jcas); \/\/ update timestamp now int sentencenumber = bratajaxcasutil.getsentencenumber(jcas, beginoffset); bratannotatormodel.getdocument().setsentenceaccessed(sentencenumber); repository.updatetimestamp(bratannotatormodel.getdocument(), bratannotatormodel.getuser(), bratannotatormodel.getmode()); if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","classification":"NONSATD","isFinished":true,"code_context_2":"selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject());","code_context_10":"selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","code_context_20":"if (bratannotatormodel.isscrollpage()) { updatesentenceaddressandoffsets(jcas, beginoffset); } bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setannotate(false); \/\/ store latest annotations for (imodel<string> model : featurevaluemodels) { annotationfeature feature = featuresmodel.get(featurevaluemodels.indexof(model)) .getobject().feature; selectedannotationlayer = feature.getlayer(); selectedfeaturevalues.put(feature, model.getobject()); } info(generatemessage(selectedannotationlayer, null, true)); \/\/ a hack to rememeber the visural dropdown display \/\/ value bratannotatormodel.setrememberedspanlayer(selectedannotationlayer); bratannotatormodel.setrememberedspanfeatures(selectedfeaturevalues); selectedspantext = \"\"; selectedannotationid = -1; atarget.add(annotationfeatureform); \/\/ setlayerandfeaturemodels(jcas); bratrender(atarget, jcas); onchange(atarget, getmodelobject()); }","repo":"debovis\/webanno"}
{"id":34792,"comment_id":0,"comment":"\/\/ sort the list \/\/ todo understand logits!! \/\/ remove 1 counts!","code":"private list<pair<object, double>> getmostinfluentialfeatures(label clazz, featurevector featurevector) { assert this.model instanceof naivebayesmodel; naivebayesmodel mymodel = (naivebayesmodel) this.model; list<pair<object, double>> mostinfluentialfeatures = new arraylist<>(); for (object feature : featurevector) { double classconditional = mymodel.computeclassconditionalprobability(feature, clazz); if (uselogits) classconditional = -math.log(classconditional); mostinfluentialfeatures.add(pair.of(feature, classconditional)); } \/\/ sort the list \/\/ todo understand logits!! \/\/ remove 1 counts! if (uselogits) mostinfluentialfeatures.sort(comparator.comparing(pair::getright)); else mostinfluentialfeatures.sort((o1, o2) -> o2.getright().compareto(o1.getright())); return mostinfluentialfeatures.sublist(0, mostinfluentialfeatures.size() <= numberofinfluentialfeatures ? mostinfluentialfeatures.size() - 1 : numberofinfluentialfeatures); }","classification":"DESIGN","isFinished":true,"code_context_2":"mostinfluentialfeatures.add(pair.of(feature, classconditional)); } \/\/ sort the list \/\/ todo understand logits!! \/\/ remove 1 counts! if (uselogits) mostinfluentialfeatures.sort(comparator.comparing(pair::getright));","code_context_10":"private list<pair<object, double>> getmostinfluentialfeatures(label clazz, featurevector featurevector) { assert this.model instanceof naivebayesmodel; naivebayesmodel mymodel = (naivebayesmodel) this.model; list<pair<object, double>> mostinfluentialfeatures = new arraylist<>(); for (object feature : featurevector) { double classconditional = mymodel.computeclassconditionalprobability(feature, clazz); if (uselogits) classconditional = -math.log(classconditional); mostinfluentialfeatures.add(pair.of(feature, classconditional)); } \/\/ sort the list \/\/ todo understand logits!! \/\/ remove 1 counts! if (uselogits) mostinfluentialfeatures.sort(comparator.comparing(pair::getright)); else mostinfluentialfeatures.sort((o1, o2) -> o2.getright().compareto(o1.getright())); return mostinfluentialfeatures.sublist(0, mostinfluentialfeatures.size() <= numberofinfluentialfeatures ? mostinfluentialfeatures.size() - 1 : numberofinfluentialfeatures); }","code_context_20":"private list<pair<object, double>> getmostinfluentialfeatures(label clazz, featurevector featurevector) { assert this.model instanceof naivebayesmodel; naivebayesmodel mymodel = (naivebayesmodel) this.model; list<pair<object, double>> mostinfluentialfeatures = new arraylist<>(); for (object feature : featurevector) { double classconditional = mymodel.computeclassconditionalprobability(feature, clazz); if (uselogits) classconditional = -math.log(classconditional); mostinfluentialfeatures.add(pair.of(feature, classconditional)); } \/\/ sort the list \/\/ todo understand logits!! \/\/ remove 1 counts! if (uselogits) mostinfluentialfeatures.sort(comparator.comparing(pair::getright)); else mostinfluentialfeatures.sort((o1, o2) -> o2.getright().compareto(o1.getright())); return mostinfluentialfeatures.sublist(0, mostinfluentialfeatures.size() <= numberofinfluentialfeatures ? mostinfluentialfeatures.size() - 1 : numberofinfluentialfeatures); }","repo":"floschne\/NLP_ThumbnailAnnotator"}
{"id":34835,"comment_id":0,"comment":"\/\/ todo: change me when implemented","code":"@override public boolean allowsnewvlancreation() throws cloudexception, internalexception { \/\/ todo: change me when implemented return false; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@override public boolean allowsnewvlancreation() throws cloudexception, internalexception { \/\/ todo: change me when implemented return false; }","code_context_10":"@override public boolean allowsnewvlancreation() throws cloudexception, internalexception { \/\/ todo: change me when implemented return false; }","code_context_20":"@override public boolean allowsnewvlancreation() throws cloudexception, internalexception { \/\/ todo: change me when implemented return false; }","repo":"erik-johnson\/dasein-cloud-vcloud"}
{"id":34840,"comment_id":0,"comment":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ \/\/ test alignmentutils.trimcigarbyreference() \/\/ \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/","code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ \/\/ test alignmentutils.trimcigarbyreference() \/\/ \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ @dataprovider(name = \"trimcigardata\") public object[][] maketrimcigardata() { list<object[]> tests = new arraylist<>(); for ( final cigaroperator op : arrays.aslist(cigaroperator.d, cigaroperator.eq, cigaroperator.x, cigaroperator.m) ) { for ( int mylength = 1; mylength < 6; mylength++ ) { for ( int start = 0; start < mylength - 1; start++ ) { for ( int end = start; end < mylength; end++ ) { final int length = end - start + 1; final list<cigaroperator> padops = arrays.aslist(cigaroperator.d, cigaroperator.m); for ( final cigaroperator padop: padops) { for ( int leftpad = 0; leftpad < 2; leftpad++ ) { for ( int rightpad = 0; rightpad < 2; rightpad++ ) { tests.add(new object[]{ (leftpad > 0 ? leftpad + padop.tostring() : \"\") + mylength + op.tostring() + (rightpad > 0 ? rightpad + padop.tostring() : \"\"), start + leftpad, end + leftpad, length + op.tostring()}); } } } } } } } for ( final int leftpad : arrays.aslist(0, 1, 2, 5) ) { for ( final int rightpad : arrays.aslist(0, 1, 2, 5) ) { final int length = leftpad + rightpad; if ( length > 0 ) { for ( final int inssize : arrays.aslist(1, 10) ) { for ( int start = 0; start <= leftpad; start++ ) { for ( int stop = leftpad; stop < length; stop++ ) { final int leftpadremaining = leftpad - start; final int rightpadremaining = stop - leftpad + 1; final string insc = inssize + \"i\"; tests.add(new object[]{ leftpad + \"m\" + insc + rightpad + \"m\", start, stop, (leftpadremaining > 0 ? leftpadremaining + \"m\" : \"\") + insc + (rightpadremaining > 0 ? rightpadremaining + \"m\" : \"\") }); } } } } } } tests.add(new object[]{\"3m2d4m\", 0, 8, \"3m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 8, \"1m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 6, \"1m2d2m\"}); tests.add(new object[]{\"3m2d4m\", 3, 6, \"2d2m\"}); tests.add(new object[]{\"3m2d4m\", 4, 6, \"1d2m\"}); tests.add(new object[]{\"3m2d4m\", 5, 6, \"2m\"}); tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","classification":"NONSATD","isFinished":true,"code_context_2":"@dataprovider(name = \"trimcigardata\") public object[][] maketrimcigardata() { list<object[]> tests = new arraylist<>(); for ( final cigaroperator op : arrays.aslist(cigaroperator.d, cigaroperator.eq, cigaroperator.x, cigaroperator.m) ) { for ( int mylength = 1; mylength < 6; mylength++ ) { for ( int start = 0; start < mylength - 1; start++ ) { for ( int end = start; end < mylength; end++ ) { final int length = end - start + 1; final list<cigaroperator> padops = arrays.aslist(cigaroperator.d, cigaroperator.m); for ( final cigaroperator padop: padops) { for ( int leftpad = 0; leftpad < 2; leftpad++ ) { for ( int rightpad = 0; rightpad < 2; rightpad++ ) { tests.add(new object[]{ (leftpad > 0 ? leftpad + padop.tostring() : \"\") + mylength + op.tostring() + (rightpad > 0 ? rightpad + padop.tostring() : \"\"), start + leftpad, end + leftpad, length + op.tostring()}); } } } } } } } for ( final int leftpad : arrays.aslist(0, 1, 2, 5) ) { for ( final int rightpad : arrays.aslist(0, 1, 2, 5) ) { final int length = leftpad + rightpad; if ( length > 0 ) { for ( final int inssize : arrays.aslist(1, 10) ) { for ( int start = 0; start <= leftpad; start++ ) { for ( int stop = leftpad; stop < length; stop++ ) { final int leftpadremaining = leftpad - start; final int rightpadremaining = stop - leftpad + 1; final string insc = inssize + \"i\"; tests.add(new object[]{ leftpad + \"m\" + insc + rightpad + \"m\", start, stop, (leftpadremaining > 0 ? leftpadremaining + \"m\" : \"\") + insc + (rightpadremaining > 0 ? rightpadremaining + \"m\" : \"\") }); } } } } } } tests.add(new object[]{\"3m2d4m\", 0, 8, \"3m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 8, \"1m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 6, \"1m2d2m\"}); tests.add(new object[]{\"3m2d4m\", 3, 6, \"2d2m\"}); tests.add(new object[]{\"3m2d4m\", 4, 6, \"1d2m\"}); tests.add(new object[]{\"3m2d4m\", 5, 6, \"2m\"}); tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","code_context_10":"@dataprovider(name = \"trimcigardata\") public object[][] maketrimcigardata() { list<object[]> tests = new arraylist<>(); for ( final cigaroperator op : arrays.aslist(cigaroperator.d, cigaroperator.eq, cigaroperator.x, cigaroperator.m) ) { for ( int mylength = 1; mylength < 6; mylength++ ) { for ( int start = 0; start < mylength - 1; start++ ) { for ( int end = start; end < mylength; end++ ) { final int length = end - start + 1; final list<cigaroperator> padops = arrays.aslist(cigaroperator.d, cigaroperator.m); for ( final cigaroperator padop: padops) { for ( int leftpad = 0; leftpad < 2; leftpad++ ) { for ( int rightpad = 0; rightpad < 2; rightpad++ ) { tests.add(new object[]{ (leftpad > 0 ? leftpad + padop.tostring() : \"\") + mylength + op.tostring() + (rightpad > 0 ? rightpad + padop.tostring() : \"\"), start + leftpad, end + leftpad, length + op.tostring()}); } } } } } } } for ( final int leftpad : arrays.aslist(0, 1, 2, 5) ) { for ( final int rightpad : arrays.aslist(0, 1, 2, 5) ) { final int length = leftpad + rightpad; if ( length > 0 ) { for ( final int inssize : arrays.aslist(1, 10) ) { for ( int start = 0; start <= leftpad; start++ ) { for ( int stop = leftpad; stop < length; stop++ ) { final int leftpadremaining = leftpad - start; final int rightpadremaining = stop - leftpad + 1; final string insc = inssize + \"i\"; tests.add(new object[]{ leftpad + \"m\" + insc + rightpad + \"m\", start, stop, (leftpadremaining > 0 ? leftpadremaining + \"m\" : \"\") + insc + (rightpadremaining > 0 ? rightpadremaining + \"m\" : \"\") }); } } } } } } tests.add(new object[]{\"3m2d4m\", 0, 8, \"3m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 8, \"1m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 6, \"1m2d2m\"}); tests.add(new object[]{\"3m2d4m\", 3, 6, \"2d2m\"}); tests.add(new object[]{\"3m2d4m\", 4, 6, \"1d2m\"}); tests.add(new object[]{\"3m2d4m\", 5, 6, \"2m\"}); tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","code_context_20":"@dataprovider(name = \"trimcigardata\") public object[][] maketrimcigardata() { list<object[]> tests = new arraylist<>(); for ( final cigaroperator op : arrays.aslist(cigaroperator.d, cigaroperator.eq, cigaroperator.x, cigaroperator.m) ) { for ( int mylength = 1; mylength < 6; mylength++ ) { for ( int start = 0; start < mylength - 1; start++ ) { for ( int end = start; end < mylength; end++ ) { final int length = end - start + 1; final list<cigaroperator> padops = arrays.aslist(cigaroperator.d, cigaroperator.m); for ( final cigaroperator padop: padops) { for ( int leftpad = 0; leftpad < 2; leftpad++ ) { for ( int rightpad = 0; rightpad < 2; rightpad++ ) { tests.add(new object[]{ (leftpad > 0 ? leftpad + padop.tostring() : \"\") + mylength + op.tostring() + (rightpad > 0 ? rightpad + padop.tostring() : \"\"), start + leftpad, end + leftpad, length + op.tostring()}); } } } } } } } for ( final int leftpad : arrays.aslist(0, 1, 2, 5) ) { for ( final int rightpad : arrays.aslist(0, 1, 2, 5) ) { final int length = leftpad + rightpad; if ( length > 0 ) { for ( final int inssize : arrays.aslist(1, 10) ) { for ( int start = 0; start <= leftpad; start++ ) { for ( int stop = leftpad; stop < length; stop++ ) { final int leftpadremaining = leftpad - start; final int rightpadremaining = stop - leftpad + 1; final string insc = inssize + \"i\"; tests.add(new object[]{ leftpad + \"m\" + insc + rightpad + \"m\", start, stop, (leftpadremaining > 0 ? leftpadremaining + \"m\" : \"\") + insc + (rightpadremaining > 0 ? rightpadremaining + \"m\" : \"\") }); } } } } } } tests.add(new object[]{\"3m2d4m\", 0, 8, \"3m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 8, \"1m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 6, \"1m2d2m\"}); tests.add(new object[]{\"3m2d4m\", 3, 6, \"2d2m\"}); tests.add(new object[]{\"3m2d4m\", 4, 6, \"1d2m\"}); tests.add(new object[]{\"3m2d4m\", 5, 6, \"2m\"}); tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","repo":"ga4gh\/gatk"}
{"id":34840,"comment_id":1,"comment":"\/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"});","code":"@dataprovider(name = \"trimcigardata\") public object[][] maketrimcigardata() { list<object[]> tests = new arraylist<>(); for ( final cigaroperator op : arrays.aslist(cigaroperator.d, cigaroperator.eq, cigaroperator.x, cigaroperator.m) ) { for ( int mylength = 1; mylength < 6; mylength++ ) { for ( int start = 0; start < mylength - 1; start++ ) { for ( int end = start; end < mylength; end++ ) { final int length = end - start + 1; final list<cigaroperator> padops = arrays.aslist(cigaroperator.d, cigaroperator.m); for ( final cigaroperator padop: padops) { for ( int leftpad = 0; leftpad < 2; leftpad++ ) { for ( int rightpad = 0; rightpad < 2; rightpad++ ) { tests.add(new object[]{ (leftpad > 0 ? leftpad + padop.tostring() : \"\") + mylength + op.tostring() + (rightpad > 0 ? rightpad + padop.tostring() : \"\"), start + leftpad, end + leftpad, length + op.tostring()}); } } } } } } } for ( final int leftpad : arrays.aslist(0, 1, 2, 5) ) { for ( final int rightpad : arrays.aslist(0, 1, 2, 5) ) { final int length = leftpad + rightpad; if ( length > 0 ) { for ( final int inssize : arrays.aslist(1, 10) ) { for ( int start = 0; start <= leftpad; start++ ) { for ( int stop = leftpad; stop < length; stop++ ) { final int leftpadremaining = leftpad - start; final int rightpadremaining = stop - leftpad + 1; final string insc = inssize + \"i\"; tests.add(new object[]{ leftpad + \"m\" + insc + rightpad + \"m\", start, stop, (leftpadremaining > 0 ? leftpadremaining + \"m\" : \"\") + insc + (rightpadremaining > 0 ? rightpadremaining + \"m\" : \"\") }); } } } } } } tests.add(new object[]{\"3m2d4m\", 0, 8, \"3m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 8, \"1m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 6, \"1m2d2m\"}); tests.add(new object[]{\"3m2d4m\", 3, 6, \"2d2m\"}); tests.add(new object[]{\"3m2d4m\", 4, 6, \"1d2m\"}); tests.add(new object[]{\"3m2d4m\", 5, 6, \"2m\"}); tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","classification":"DEFECT","isFinished":true,"code_context_2":"tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","code_context_10":"tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","code_context_20":"} } } } tests.add(new object[]{\"3m2d4m\", 0, 8, \"3m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 8, \"1m2d4m\"}); tests.add(new object[]{\"3m2d4m\", 2, 6, \"1m2d2m\"}); tests.add(new object[]{\"3m2d4m\", 3, 6, \"2d2m\"}); tests.add(new object[]{\"3m2d4m\", 4, 6, \"1d2m\"}); tests.add(new object[]{\"3m2d4m\", 5, 6, \"2m\"}); tests.add(new object[]{\"3m2d4m\", 6, 6, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 0, 5, \"2m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 5, \"1m3i4m\"}); tests.add(new object[]{\"2m3i4m\", 1, 4, \"1m3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 4, \"3i3m\"}); tests.add(new object[]{\"2m3i4m\", 2, 3, \"3i2m\"}); tests.add(new object[]{\"2m3i4m\", 2, 2, \"3i1m\"}); tests.add(new object[]{\"2m3i4m\", 3, 4, \"2m\"}); tests.add(new object[]{\"2m3i4m\", 3, 3, \"1m\"}); tests.add(new object[]{\"2m3i4m\", 4, 4, \"1m\"}); \/\/ this doesn't work -- but i'm not sure it should \/\/ tests.add(new object[]{\"2m3i4m\", 2, 1, \"3i\"}); return tests.toarray(new object[][]{}); }","repo":"ga4gh\/gatk"}
{"id":18558,"comment_id":0,"comment":"\/\/todo set last message time","code":"@override public void onbindviewholder(recyclerview.viewholder holder, int position) { \/\/todo set last message time final chatdescription description = descriptionlist.get(position); user mainuser = ((globalvars)(inboxfragment.getactivity()).getapplication()).getuser(); final user otheruser; if (objects.equals(mainuser.getuid(), description.getuserid1())) otheruser = usermap.get(description.getuserid2()); else otheruser = usermap.get(description.getuserid1()); final inboxviewholder viewholder = (inboxviewholder) holder; glide.with(context).load(otheruser.getprofileurl()).asbitmap().centercrop().dontanimate(). into(new bitmapimageviewtarget(viewholder.ivprofile) { @override protected void setresource(bitmap resource) { roundedbitmapdrawable circularbitmapdrawable = roundedbitmapdrawablefactory.create(context.getresources(), resource); circularbitmapdrawable.setcircular(true); viewholder.ivprofile.setimagedrawable(circularbitmapdrawable); viewholder.ivprofile.setbordercolor(otheruser.getcolorhexdark(context)); } }); string lastmessage; if (description.lastmessage != null) lastmessage = description.lastmessage.gettext(); else lastmessage = \"\"; viewholder.tvlastmessage.settext(lastmessage); viewholder.tvlastmessage.setellipsize(textutils.truncateat.end); viewholder.tvlastmessage.setmaxlines(2); viewholder.tvotherusername.settext(otheruser.getfirstname()); long seconds = description.getlastmessage().getsenttime().gettime(); viewholder.tvtime.settext(timeformatter.gettimedifference(seconds)); viewholder.itemview.setonclicklistener(new view.onclicklistener() { @override public void onclick(view view) { intent i = new intent(view.getcontext(), messageactivity.class); i.putextra(\"otheruser\", parcels.wrap(otheruser)); i.putextra(\"description\", parcels.wrap(description)); inboxfragment.startactivityforresult(i, inboxfragment.changedescription); } }); if (position == 0) holder.itemview.setpadding(0, 200, 0, 0); \/\/not the right way to do this }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@override public void onbindviewholder(recyclerview.viewholder holder, int position) { \/\/todo set last message time final chatdescription description = descriptionlist.get(position); user mainuser = ((globalvars)(inboxfragment.getactivity()).getapplication()).getuser();","code_context_10":"@override public void onbindviewholder(recyclerview.viewholder holder, int position) { \/\/todo set last message time final chatdescription description = descriptionlist.get(position); user mainuser = ((globalvars)(inboxfragment.getactivity()).getapplication()).getuser(); final user otheruser; if (objects.equals(mainuser.getuid(), description.getuserid1())) otheruser = usermap.get(description.getuserid2()); else otheruser = usermap.get(description.getuserid1()); final inboxviewholder viewholder = (inboxviewholder) holder; glide.with(context).load(otheruser.getprofileurl()).asbitmap().centercrop().dontanimate(). into(new bitmapimageviewtarget(viewholder.ivprofile) { @override","code_context_20":"@override public void onbindviewholder(recyclerview.viewholder holder, int position) { \/\/todo set last message time final chatdescription description = descriptionlist.get(position); user mainuser = ((globalvars)(inboxfragment.getactivity()).getapplication()).getuser(); final user otheruser; if (objects.equals(mainuser.getuid(), description.getuserid1())) otheruser = usermap.get(description.getuserid2()); else otheruser = usermap.get(description.getuserid1()); final inboxviewholder viewholder = (inboxviewholder) holder; glide.with(context).load(otheruser.getprofileurl()).asbitmap().centercrop().dontanimate(). into(new bitmapimageviewtarget(viewholder.ivprofile) { @override protected void setresource(bitmap resource) { roundedbitmapdrawable circularbitmapdrawable = roundedbitmapdrawablefactory.create(context.getresources(), resource); circularbitmapdrawable.setcircular(true); viewholder.ivprofile.setimagedrawable(circularbitmapdrawable); viewholder.ivprofile.setbordercolor(otheruser.getcolorhexdark(context)); } }); string lastmessage; if (description.lastmessage != null) lastmessage = description.lastmessage.gettext();","repo":"dgisser\/Ripple"}
{"id":18558,"comment_id":1,"comment":"\/\/not the right way to do this","code":"@override public void onbindviewholder(recyclerview.viewholder holder, int position) { \/\/todo set last message time final chatdescription description = descriptionlist.get(position); user mainuser = ((globalvars)(inboxfragment.getactivity()).getapplication()).getuser(); final user otheruser; if (objects.equals(mainuser.getuid(), description.getuserid1())) otheruser = usermap.get(description.getuserid2()); else otheruser = usermap.get(description.getuserid1()); final inboxviewholder viewholder = (inboxviewholder) holder; glide.with(context).load(otheruser.getprofileurl()).asbitmap().centercrop().dontanimate(). into(new bitmapimageviewtarget(viewholder.ivprofile) { @override protected void setresource(bitmap resource) { roundedbitmapdrawable circularbitmapdrawable = roundedbitmapdrawablefactory.create(context.getresources(), resource); circularbitmapdrawable.setcircular(true); viewholder.ivprofile.setimagedrawable(circularbitmapdrawable); viewholder.ivprofile.setbordercolor(otheruser.getcolorhexdark(context)); } }); string lastmessage; if (description.lastmessage != null) lastmessage = description.lastmessage.gettext(); else lastmessage = \"\"; viewholder.tvlastmessage.settext(lastmessage); viewholder.tvlastmessage.setellipsize(textutils.truncateat.end); viewholder.tvlastmessage.setmaxlines(2); viewholder.tvotherusername.settext(otheruser.getfirstname()); long seconds = description.getlastmessage().getsenttime().gettime(); viewholder.tvtime.settext(timeformatter.gettimedifference(seconds)); viewholder.itemview.setonclicklistener(new view.onclicklistener() { @override public void onclick(view view) { intent i = new intent(view.getcontext(), messageactivity.class); i.putextra(\"otheruser\", parcels.wrap(otheruser)); i.putextra(\"description\", parcels.wrap(description)); inboxfragment.startactivityforresult(i, inboxfragment.changedescription); } }); if (position == 0) holder.itemview.setpadding(0, 200, 0, 0); \/\/not the right way to do this }","classification":"NONSATD","isFinished":true,"code_context_2":"} }); if (position == 0) holder.itemview.setpadding(0, 200, 0, 0); \/\/not the right way to do this }","code_context_10":"viewholder.tvtime.settext(timeformatter.gettimedifference(seconds)); viewholder.itemview.setonclicklistener(new view.onclicklistener() { @override public void onclick(view view) { intent i = new intent(view.getcontext(), messageactivity.class); i.putextra(\"otheruser\", parcels.wrap(otheruser)); i.putextra(\"description\", parcels.wrap(description)); inboxfragment.startactivityforresult(i, inboxfragment.changedescription); } }); if (position == 0) holder.itemview.setpadding(0, 200, 0, 0); \/\/not the right way to do this }","code_context_20":"} }); string lastmessage; if (description.lastmessage != null) lastmessage = description.lastmessage.gettext(); else lastmessage = \"\"; viewholder.tvlastmessage.settext(lastmessage); viewholder.tvlastmessage.setellipsize(textutils.truncateat.end); viewholder.tvlastmessage.setmaxlines(2); viewholder.tvotherusername.settext(otheruser.getfirstname()); long seconds = description.getlastmessage().getsenttime().gettime(); viewholder.tvtime.settext(timeformatter.gettimedifference(seconds)); viewholder.itemview.setonclicklistener(new view.onclicklistener() { @override public void onclick(view view) { intent i = new intent(view.getcontext(), messageactivity.class); i.putextra(\"otheruser\", parcels.wrap(otheruser)); i.putextra(\"description\", parcels.wrap(description)); inboxfragment.startactivityforresult(i, inboxfragment.changedescription); } }); if (position == 0) holder.itemview.setpadding(0, 200, 0, 0); \/\/not the right way to do this }","repo":"dgisser\/Ripple"}
{"id":18576,"comment_id":0,"comment":"\/\/ todo fix","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"DEFECT","isFinished":true,"code_context_2":"int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); }","code_context_10":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx;","code_context_20":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern;","repo":"enoxos\/r5"}
{"id":18576,"comment_id":1,"comment":"\/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below.","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern","code_context_10":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) {","code_context_20":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]);","repo":"enoxos\/r5"}
{"id":18576,"comment_id":2,"comment":"\/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination]","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) {","code_context_10":"if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern;","code_context_20":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0]));","repo":"enoxos\/r5"}
{"id":18576,"comment_id":3,"comment":"\/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"DESIGN","isFinished":true,"code_context_2":"int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++;","code_context_10":"trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern","code_context_20":"if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0];","repo":"enoxos\/r5"}
{"id":18576,"comment_id":4,"comment":"\/\/ sort by departure time of each trip, within each pattern","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":".toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window","code_context_10":"\/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0];","code_context_20":"\/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) {","repo":"enoxos\/r5"}
{"id":18576,"comment_id":5,"comment":"\/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++;","code_context_10":"tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0;","code_context_20":"int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime;","repo":"enoxos\/r5"}
{"id":18576,"comment_id":6,"comment":"\/\/ now interleave times","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) {","code_context_10":"times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible","code_context_20":"while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) {","repo":"enoxos\/r5"}
{"id":18576,"comment_id":7,"comment":"\/\/ linear scan over timetable to do interleaving","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0;","code_context_10":"stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time","code_context_20":"if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; }","repo":"enoxos\/r5"}
{"id":18576,"comment_id":8,"comment":"\/\/ we've found the end of the times at which this path is possible","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0];","code_context_10":"\/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) {","code_context_20":"times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } }","repo":"enoxos\/r5"}
{"id":18576,"comment_id":9,"comment":"\/\/ find the transfer time","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime;","code_context_10":"\/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; }","code_context_20":"stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time;","repo":"enoxos\/r5"}
{"id":18576,"comment_id":10,"comment":"\/\/ no transfer time, we are at the same stop (board slack applied below)","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"NONSATD","isFinished":true,"code_context_2":"} } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics","code_context_10":"if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries();","code_context_20":"} itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","repo":"enoxos\/r5"}
{"id":18576,"comment_id":11,"comment":"\/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics","code":"private void computetimes (transportnetwork network, profilerequest req, tintintmap accesstimes, tintintmap egresstimes) { if (!accesstimes.containskey(this.boardstops[0])) throw new illegalargumentexception(\"access times do not contain first stop of path!\"); if (!egresstimes.containskey(this.alightstops[this.length - 1])) throw new illegalargumentexception(\"egress times do not contain last stop of path!\"); int accesstime = accesstimes.get(this.boardstops[0]); int egresstime = egresstimes.get(this.alightstops[this.length - 1]); if (network.transitlayer.hasfrequencies) { \/\/ todo fix throw new unsupportedoperationexception(\"frequency-based trips are not yet supported in customer-facing profile routing\"); } \/\/ we now know what patterns are being used, interleave to find times \/\/ nb no need to reverse-optimize these itineraries; we'll just filter them below. trippattern[] patterns = intstream.of(this.patterns).maptoobj(p -> network.transitlayer.trippatterns.get(p)).toarray(s -> new trippattern[s]); \/\/ find all possible times to board and alight each pattern \/\/ for each trip pattern, for each trip on that pattern, array of [depart origin, arrive destination] int[][][] times = new int[patterns.length][][]; for (int patidx = 0; patidx < patterns.length; patidx++) { final int pidx = patidx; int fromstopinpattern = 0; while (patterns[patidx].stops[fromstopinpattern] != this.boardstops[pidx]) fromstopinpattern++; int tostopinpattern = fromstopinpattern; while (patterns[patidx].stops[tostopinpattern] != this.alightstops[pidx]) { \/\/ if we visit the board stop multiple times, board at the one closest to the alight stop \/\/ todo better handle duplicated stops\/loop routes if (patterns[patidx].stops[tostopinpattern] == this.boardstops[pidx]) fromstopinpattern = tostopinpattern; tostopinpattern++; } final int finalfromstopinpattern = fromstopinpattern; final int finaltostopinpattern = tostopinpattern; times[patidx] = patterns[patidx].tripschedules.stream() .map(ts -> new int[] { ts.departures[finalfromstopinpattern], ts.arrivals[finaltostopinpattern] }) .toarray(s -> new int[s][]); } \/\/ sort by departure time of each trip, within each pattern stream.of(times).foreach(t -> arrays.sort(t, (t1, t2) -> t1[0] - t2[0])); \/\/ loop over departures within the time window \/\/ firsttrip is the trip on the first pattern int firsttrip = 0; while (times[0][firsttrip][0] < req.fromtime + accesstime + fastraptorworker.board_slack_seconds) firsttrip++; \/\/ now interleave times double walkspeedmillimeterspersecond = req.walkspeed * 1000; times: while (firsttrip < times[0].length) { itinerary itin = new itinerary(this.patterns.length); int time = times[0][firsttrip][0]; \/\/ linear scan over timetable to do interleaving for (int patidx = 0; patidx < this.patterns.length; patidx++) { int trip = 0; while (times[patidx][trip][0] < time) { trip++; if (trip >= times[patidx].length) break times; \/\/ we've found the end of the times at which this path is possible } itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","classification":"DESIGN","isFinished":true,"code_context_2":"} else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time;","code_context_10":"int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","code_context_20":"itin.boardtimes[patidx] = times[patidx][trip][0]; itin.alighttimes[patidx] = times[patidx][trip][1]; if (patidx < this.length - 1) { \/\/ find the transfer time tintlist transfers = network.transitlayer.transfersforstop.get(this.alightstops[patidx]); int transfertime; if (this.alightstops[patidx] != this.boardstops[patidx + 1]) { transfertime = -1; for (int i = 0; i < transfers.size(); i += 2) { if (transfers.get(i) == this.boardstops[patidx + 1]) { int transferdistancemillimeters = transfers.get(i + 1); transfertime = (int)(transferdistancemillimeters \/ walkspeedmillimeterspersecond); break; } } if (transfertime == -1) { throw new illegalstateexception(\"did not find transfer in transit network, indicates an internal error\"); } } else transfertime = 0; \/\/ no transfer time, we are at the same stop (board slack applied below) \/\/ todo should board slack be applied at the origin stop? is this done in raptorworker? \/\/ see also below in computestatistics time = times[patidx][trip][1] + transfertime + fastraptorworker.board_slack_seconds; itin.arriveatboardstoptimes[patidx + 1] = time; } } this.itineraries.add(itin); firsttrip++; } sortandfilteritineraries(); computestatistics(req, accesstime, egresstime); }","repo":"enoxos\/r5"}
{"id":18691,"comment_id":0,"comment":"\/** * this listener hides the popup when the focus is lost. it also repaints * when focus is gained or lost. * * this override is necessary because the basic l&f for the combo box is working * around a solaris-only bug that we don't have on mac os x. so, remove the lightweight * popup check here. rdar:\/\/problem\/3518582 *\/","code":"\/** * this listener hides the popup when the focus is lost. it also repaints * when focus is gained or lost. * * this override is necessary because the basic l&f for the combo box is working * around a solaris-only bug that we don't have on mac os x. so, remove the lightweight * popup check here. rdar:\/\/problem\/3518582 *\/ protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","classification":"DESIGN","isFinished":true,"code_context_2":"protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","code_context_10":"protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","code_context_20":"protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","repo":"dbac\/jdk8"}
{"id":18691,"comment_id":1,"comment":"\/\/ notify assistive technologies that the combo box lost focus","code":"protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"} combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) {","code_context_10":"protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","code_context_20":"protected focuslistener createfocuslistener() { return new basiccomboboxui.focushandler() { public void focuslost(final focusevent e) { hasfocus = false; if (!e.istemporary()) { setpopupvisible(combobox, false); } combobox.repaint(); \/\/ notify assistive technologies that the combo box lost focus final accessiblecontext ac = ((accessible)combobox).getaccessiblecontext(); if (ac != null) { ac.firepropertychange(accessiblecontext.accessible_state_property, accessiblestate.focused, null); } } }; }","repo":"dbac\/jdk8"}
{"id":10532,"comment_id":0,"comment":"\/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u);","code_context_10":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\");","code_context_20":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring());","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":1,"comment":"\/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) {","code_context_10":"final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\");","code_context_20":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser);","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":2,"comment":"\/\/ notify logged users about the new player","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper();","code_context_10":"public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) {","code_context_20":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break;","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":3,"comment":"\/\/logger.debug(\"unknown user connected\");","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"} } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e);","code_context_10":"}*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser);","code_context_20":"@override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break;","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":4,"comment":"\/\/ todo: add error handling","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser);","code_context_10":"} } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\":","code_context_20":"\/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser);","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":5,"comment":"\/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket","code_context_10":"break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try {","code_context_20":"onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else {","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":6,"comment":"\/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100);","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) {","code_context_10":"break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\");","code_context_20":"onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } });","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":7,"comment":"\/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) {","code_context_10":"final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\");","code_context_20":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser);","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":8,"comment":"\/\/ notify logged users about the leaving player","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try {","code_context_10":"if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); }","code_context_20":"onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","repo":"daniel-sandro\/WTEC1516"}
{"id":10532,"comment_id":9,"comment":"\/\/logger.debug(\"unknown user disconnected\");","code":"public websocket<jsonnode> socket() { final http.session session = session(); final user currentuser = application.getlocaluser(session()); return new websocket<jsonnode>() { @override public void onready(in<jsonnode> in, out<jsonnode> out) { try { user u = application.getlocaluser(session); \/\/logger.debug(\"user \" + u.id + \" connected\"); \/\/ add the new user to the data structures boolean alreadyonline = onlineusers.containsvalue(u); onlineusers.put(out, u); \/*try (jedis j = jedispool.getresource()) { j.sadd(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the new player if (!alreadyonline) { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"newuser\"); notification.put(\"newuser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } } catch (runtimeexception e) { \/\/logger.debug(\"unknown user connected\"); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } in.onmessage((data) -> { logger.debug(currentuser.id + \" - \" + data.tostring()); string action = data.findpath(\"action\").textvalue(); switch (action) { \/\/ todo: add error handling case \"newgame\": onnewgameresponse(data, currentuser); break; case \"ready\": onuserready(data, currentuser); break; case \"setrowboat\": onsetship(data, currentuser, new rowboat()); break; case \"setdestructor\": onsetship(data, currentuser, new destructor()); break; case \"setflattop\": onsetship(data, currentuser, new flattop()); break; case \"shoot\": onshoot(data, currentuser); break; case \"userleaves\": onuserleaves(data, currentuser); break; case \"getonlineusers\": ongetonlineusers(data, currentuser); break; default: } }); in.onclose(() -> { user u = onlineusers.get(out); if (u != null) { \/\/logger.debug(\"user \" + u.id + \" disconnected\"); \/\/ remove user from the data structures onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","classification":"NONSATD","isFinished":true,"code_context_2":"} } else { \/\/logger.debug(\"unknown user disconnected\"); } });","code_context_10":"objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","code_context_20":"onlineusers.remove(out); \/\/ wait for 100 ms to see if the user connects through another websocket \/\/thread.sleep(100); boolean stillonline = onlineusers.containsvalue(u); \/*try (jedis j = jedispool.getresource()) { j.srem(\"online_users\", long.tostring(u.id)); }*\/ \/\/ notify logged users about the leaving player if (!stillonline) { try { objectmapper mapper = new objectmapper(); objectnode notification = jsonnodefactory.instance.objectnode(); notification.put(\"action\", \"userleaves\"); notification.put(\"leavinguser\", mapper.writevalueasstring(currentuser)); broadcastmessage(notification, new hashset<user>() {{ add(u); }}); } catch (jsonprocessingexception e) { logger.error(e.getmessage(), e); } } } else { \/\/logger.debug(\"unknown user disconnected\"); } }); } }; }","repo":"daniel-sandro\/WTEC1516"}
{"id":10559,"comment_id":0,"comment":"\/\/ todo: alternatively could use the same app wide if want only one entry in notifications","code":"private void postnotification(context context, answerset answerset, string message) { int surveyid = answerset.getdbsurveyid(); notificationmanager notificationmanager = (notificationmanager)context.getsystemservice(context.notification_service); intent intent = new intent(context, surveyactivity.class); intent.putextra(\"programid\", answerset.getdbprogramid()); intent.putextra(\"answersetuuid\", answerset.getuuid()); intent.setaction(\"com.sema.notification.\" + surveyid); pendingintent pintent = pendingintent.getactivity(context, 0, intent, pendingintent.flag_update_current); survey survey = answerset.getsurvey(); program program = survey != null ? survey.getprogram() : null; if (survey != null && program != null) { string programname = program.getdisplayname(); notification n = new notificationcompat.builder(context) .setcontenttitle(programname) .setcontenttext(message) .setsmallicon(r.drawable.ic_launcher) .setcontentintent(pintent) .setautocancel(true) .setsound(uri.parse(\"android.resource:\/\/\" + context.getpackagename() + \"\/\" + r.raw.notification_extended)) .build(); notificationmanager.notify(answerset.getstartalarmrequestcode(), n); \/\/ todo: alternatively could use the same app wide if want only one entry in notifications } }","classification":"DESIGN","isFinished":true,"code_context_2":".setsound(uri.parse(\"android.resource:\/\/\" + context.getpackagename() + \"\/\" + r.raw.notification_extended)) .build(); notificationmanager.notify(answerset.getstartalarmrequestcode(), n); \/\/ todo: alternatively could use the same app wide if want only one entry in notifications } }","code_context_10":"if (survey != null && program != null) { string programname = program.getdisplayname(); notification n = new notificationcompat.builder(context) .setcontenttitle(programname) .setcontenttext(message) .setsmallicon(r.drawable.ic_launcher) .setcontentintent(pintent) .setautocancel(true) .setsound(uri.parse(\"android.resource:\/\/\" + context.getpackagename() + \"\/\" + r.raw.notification_extended)) .build(); notificationmanager.notify(answerset.getstartalarmrequestcode(), n); \/\/ todo: alternatively could use the same app wide if want only one entry in notifications } }","code_context_20":"private void postnotification(context context, answerset answerset, string message) { int surveyid = answerset.getdbsurveyid(); notificationmanager notificationmanager = (notificationmanager)context.getsystemservice(context.notification_service); intent intent = new intent(context, surveyactivity.class); intent.putextra(\"programid\", answerset.getdbprogramid()); intent.putextra(\"answersetuuid\", answerset.getuuid()); intent.setaction(\"com.sema.notification.\" + surveyid); pendingintent pintent = pendingintent.getactivity(context, 0, intent, pendingintent.flag_update_current); survey survey = answerset.getsurvey(); program program = survey != null ? survey.getprogram() : null; if (survey != null && program != null) { string programname = program.getdisplayname(); notification n = new notificationcompat.builder(context) .setcontenttitle(programname) .setcontenttext(message) .setsmallicon(r.drawable.ic_launcher) .setcontentintent(pintent) .setautocancel(true) .setsound(uri.parse(\"android.resource:\/\/\" + context.getpackagename() + \"\/\" + r.raw.notification_extended)) .build(); notificationmanager.notify(answerset.getstartalarmrequestcode(), n); \/\/ todo: alternatively could use the same app wide if want only one entry in notifications } }","repo":"eorygen\/sema2_android"}
{"id":18865,"comment_id":0,"comment":"\/\/ ** base case **","code":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","classification":"NONSATD","isFinished":true,"code_context_2":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); }","code_context_10":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) {","code_context_20":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute)","repo":"danigiri\/particle"}
{"id":18865,"comment_id":1,"comment":"\/\/ ** recursive case **","code":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","classification":"NONSATD","isFinished":true,"code_context_2":"throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^'","code_context_10":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) {","code_context_20":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { }","repo":"danigiri\/particle"}
{"id":18865,"comment_id":2,"comment":"\/\/ todo: notice this is not ordered like the input ^^'","code":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","classification":"DESIGN","isFinished":true,"code_context_2":"\/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i);","code_context_10":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri)","code_context_20":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes;","repo":"danigiri\/particle"}
{"id":18865,"comment_id":3,"comment":"\/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model)","code":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","classification":"DESIGN","isFinished":true,"code_context_2":"string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) {","code_context_10":"log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue());","code_context_20":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","repo":"danigiri\/particle"}
{"id":18865,"comment_id":4,"comment":"\/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue());","code":"private static attributes<cell> attributesfrom(node node, uri uri, complexcellmodel cellmodel, predicate<string> attributefilter) { if (!node.hasattributes()) { \/\/ ** base case ** return new orderedmap<cell>(0); } if (cellmodel.issimple()) { log.error(\"cellmodel '{}' does not allow attributes but the elem does\", cellmodel.getname()); throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","classification":"NONSATD","isFinished":true,"code_context_2":".createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { }","code_context_10":"\/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","code_context_20":"throw new runtimeexception(\"element and model attribute mismatch\", new illegalargumentexception()); } \/\/ ** recursive case ** orderedmap<cell> attributes = new orderedmap<cell>(node.getattributes().getlength()); namednodemap elemattributes = node.getattributes(); \/\/ todo: notice this is not ordered like the input ^^' for (int i=0; i<elemattributes.getlength(); i++) { node attribute = elemattributes.item(i); string attributename = attribute.getnodename(); uri childuri = celluri(uri, cellmodel, attribute_prefix+attributename); \/\/ if we are looking for public attributes, they need to match with the attributes model, otherwise we just \/\/ point to the node cell model (idea: maybe in the future point to the internal model) cellmodel attributecellmodel = findattributewithname(cellmodel, attributename); if (attributefilter.test(attributename)) { cell attributecell = daggercellcomponent.builder() .withuri(childuri) .fromnode(attribute) .withcellmodel(attributecellmodel) .build() .createcell(); attributes.addchild(attributename, attributecell); \/\/system.err.println(\"\\t\\ta[\"+i+\"]:\"+attributename+\":\"+attributecell.getvalue()); } else { } } return attributes; }","repo":"danigiri\/particle"}
{"id":2495,"comment_id":0,"comment":"\/** * creates the widgets for criteria. * * @param centre * @param root * @return *\/","code":"\/** * creates the widgets for criteria. * * @param centre * @param root * @return *\/ private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","classification":"NONSATD","isFinished":true,"code_context_2":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","code_context_10":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","code_context_20":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","repo":"fieldenms\/"}
{"id":2495,"comment_id":1,"comment":"\/\/ empty property means \"entity itself\"","code":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","classification":"NONSATD","isFinished":true,"code_context_2":"for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget;","code_context_10":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop);","code_context_20":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype));","repo":"fieldenms\/"}
{"id":2495,"comment_id":2,"comment":"\/\/ todo do not forget about money later (after money widget will be available)","code":"private list<abstractcriterionwidget> createcriteriawidgets(final icentredomaintreemanagerandenhancer centre, final class<? extends abstractentity<?>> root) { final class<?> managedtype = centre.getenhancer().getmanagedtype(root); final list<abstractcriterionwidget> criteriawidgets = new arraylist<>(); for (final string critprop : centre.getfirsttick().checkedproperties(root)) { if (!abstractdomaintree.isplaceholder(critprop)) { final boolean isentityitself = \"\".equals(critprop); \/\/ empty property means \"entity itself\" final class<?> propertytype = isentityitself ? managedtype : propertytypedeterminator.determinepropertytype(managedtype, critprop); final abstractcriterionwidget criterionwidget; if (abstractdomaintree.iscritonlysingle(managedtype, critprop)) { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitysinglecriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringsinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleansinglecriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","classification":"DESIGN","isFinished":true,"code_context_2":"} else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) {","code_context_10":"} else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget);","code_context_20":"criterionwidget = new integersinglecriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { criterionwidget = new decimalsinglecriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneysinglecriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datesinglecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the crit-only single editor type [%s] is currently unsupported.\", propertytype)); } } else { if (entityutils.isentitytype(propertytype)) { final list<pair<string, boolean>> additionalprops = dsldefaultconfig.getadditionalpropsforautocompleter(critprop); criterionwidget = new entitycriterionwidget(root, managedtype, critprop, additionalprops, getcentrecontextconfigfor(critprop)); } else if (entityutils.isstring(propertytype)) { criterionwidget = new stringcriterionwidget(root, managedtype, critprop); } else if (entityutils.isboolean(propertytype)) { criterionwidget = new booleancriterionwidget(root, managedtype, critprop); } else if (integer.class.isassignablefrom(propertytype) || long.class.isassignablefrom(propertytype)) { criterionwidget = new integercriterionwidget(root, managedtype, critprop); } else if (bigdecimal.class.isassignablefrom(propertytype)) { \/\/ todo do not forget about money later (after money widget will be available) criterionwidget = new decimalcriterionwidget(root, managedtype, critprop); } else if (money.class.isassignablefrom(propertytype)) { criterionwidget = new moneycriterionwidget(root, managedtype, critprop); } else if (entityutils.isdate(propertytype)) { criterionwidget = new datecriterionwidget(root, managedtype, critprop); } else { throw new unsupportedoperationexception(string.format(\"the multi \/ range editor type [%s] is currently unsupported.\", propertytype)); } } criteriawidgets.add(criterionwidget); } } return criteriawidgets; }","repo":"fieldenms\/"}
{"id":2538,"comment_id":0,"comment":"\/\/ nop;","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"NONSATD","isFinished":true,"code_context_2":"factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver();","code_context_10":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri);","code_context_20":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b);","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2538,"comment_id":1,"comment":"\/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!?","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); }","code_context_10":"resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\");","code_context_20":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme:","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2538,"comment_id":2,"comment":"\/\/ fixme: make this configurable","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"} class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection?","code_context_10":"baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); }","code_context_20":"constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); }","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2538,"comment_id":3,"comment":"\/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"\/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) {","code_context_10":"try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); }","code_context_20":"factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder);","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2538,"comment_id":4,"comment":"\/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) {","code_context_10":"b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) {","code_context_20":"\/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) {","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2538,"comment_id":5,"comment":"\/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) {","code_context_10":"b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) {","code_context_20":"\/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) {","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2538,"comment_id":6,"comment":"\/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/","code":"private void initializefop2x(xprocruntime runtime, xstep step, properties options) { object fopfactorybuilder = null; constructor factbuilderconstructor = null; try { factbuilderconstructor = klass.getconstructor(uri.class); } catch (nosuchmethodexception nsme) { \/\/ nop; } resolver = runtime.getresolver(); uri baseuri = step.getstep().getnode().getbaseuri(); string s = getstringprop(\"baseurl\"); if (s != null) { baseuri = baseuri.resolve(s); } try { if (resolver == null) { fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } else { \/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) { method = fclass.getmethod(\"setpagewidth\", string.class); method.invoke(fopfactorybuilder, s); } f = getfloatprop(\"targetresolution\"); if (f != null) { method = fclass.getmethod(\"settargetresolution\", float.class); method.invoke(fopfactorybuilder, f); } b = getbooleanprop(\"strictuserconfigvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"strictvalidation\"); if (b != null) { method = fclass.getmethod(\"setstrictuserconfigvalidation\", boolean.class); method.invoke(fopfactorybuilder, b); } b = getbooleanprop(\"usecache\"); if (b != null && !b) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"disablefontcache\"); method.invoke(fontmanager); } \/* fixme: s = getstringprop(\"userconfig\"); if (s != null) { fopfactory.setuserconfig(s); } *\/ method = fclass.getmethod(\"build\"); fopfactory = method.invoke(fopfactorybuilder); } catch (exception e) { throw new xprocexception(e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) {","code_context_10":"b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) {","code_context_20":"\/\/ fixme: make an org.apache.xmlgraphics.io.resourceresolver resolver!? fopfactorybuilder = factbuilderconstructor.newinstance(baseuri); } class fclass = fopfactorybuilder.getclass(); \/\/ fixme: make this configurable boolean b = false; \/* why doesn't this call work with reflection? method = fclass.getmethod(\"setstrictfovalidation\", boolean.class); method.invoke(fopfactorybuilder, b); *\/ b = getbooleanprop(\"breakindentinheritanceonreferenceareaboundary\"); if (b != null) { method = fclass.getmethod(\"setbreakindentinheritanceonreferenceareaboundary\", boolean.class); method.invoke(fopfactorybuilder, b); } float f = getfloatprop(\"sourceresolution\"); if (f != null) { method = fclass.getmethod(\"setsourceresolution\", float.class); method.invoke(fopfactorybuilder, f); } \/* fixme: s = getstringprop(\"fontbaseurl\"); if (s != null) { fopfactory.getfontmanager().setfontbaseurl(s); } *\/ b = getbooleanprop(\"base14kerningenabled\"); if (b != null) { method getfontmanager = fclass.getmethod(\"getfontmanager\"); object fontmanager = getfontmanager.invoke(fopfactorybuilder); method = fontmanager.getclass().getmethod(\"setbase14kerningenabled\", boolean.class); method.invoke(fontmanager, b); } \/* fixme: s = getstringprop(\"hyphenbaseurl\"); if (s != null) { fopfactory.sethyphenbaseurl(s); } *\/ s = getstringprop(\"pageheight\"); if (s != null) { method = fclass.getmethod(\"setpageheight\", string.class); method.invoke(fopfactorybuilder, s); } s = getstringprop(\"pagewidth\"); if (s != null) {","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2539,"comment_id":0,"comment":"\/\/ \"pdf\";","code":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) { method = fopfactory.getclass().getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(fopfactory, outputformat, out); method = fop.getclass().getmethod(\"getuseragent\"); useragent = method.invoke(fop); } else { method = fopfactory.getclass().getmethod(\"newfouseragent\"); useragent = method.invoke(fopfactory); } class uaclass = useragent.getclass(); boolean b = getbooleanprop(\"accessibility\"); if (b != null) { method = uaclass.getmethod(\"setaccessibility\", boolean.class); method.invoke(useragent, b); } string s = getstringprop(\"author\"); if (s != null) { method = uaclass.getmethod(\"setauthor\", string.class); method.invoke(useragent, s); } if (\"1.x\".equals(fopversion)) { method = uaclass.getmethod(\"setbaseurl\", string.class); method.invoke(useragent, step.getnode().getbaseuri().tostring()); s = getstringprop(\"baseurl\"); if (s != null) { method.invoke(useragent, s); } } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\"); if (b != null) { method = uaclass.getmethod(\"setconservememorypolicy\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"creationdate\"); if (s != null) { dateformat df = dateformat.getdateinstance(); date d = df.parse(s); method = uaclass.getmethod(\"setcreationdate\", date.class); method.invoke(useragent, d); } s = getstringprop(\"creator\"); if (s != null) { method = uaclass.getmethod(\"setcreator\", string.class); method.invoke(useragent, s); } s = getstringprop(\"keywords\"); if (s != null) { method = uaclass.getmethod(\"setkeywords\", string.class); method.invoke(useragent, s); } b = getbooleanprop(\"locatorenabled\"); if (b != null) { method = uaclass.getmethod(\"setlocatorenabled\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"producer\"); if (s != null) { method = uaclass.getmethod(\"setproducer\", string.class); method.invoke(useragent, s); } s = getstringprop(\"subject\"); if (s != null) { method = uaclass.getmethod(\"setsubject\", string.class); method.invoke(useragent, s); } float f = getfloatprop(\"targetresolution\"); if (f != null) { method = uaclass.getmethod(\"settargetresolution\", float.class); method.invoke(useragent, f); } s = getstringprop(\"title\"); if (s != null) { method = uaclass.getmethod(\"settitle\", string.class); method.invoke(useragent, s); } if (\"2.x\".equals(fopversion)) { method = uaclass.getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(useragent, outputformat, out); } method = fop.getclass().getmethod(\"getdefaulthandler\"); object defhandler = method.invoke(fop); transformerfactory transformerfactory = transformerfactory.newinstance(); transformer transformer = transformerfactory.newtransformer(); transformer.transform(source, new saxresult((contenthandler) defhandler)); } catch (exception e) { throw new xprocexception(step.getnode(), \"failed to process fo document with fop\", e); } }","classification":"NONSATD","isFinished":true,"code_context_2":"string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\";","code_context_10":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype);","code_context_20":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) {","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2539,"comment_id":1,"comment":"\/\/\"postscript\";","code":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) { method = fopfactory.getclass().getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(fopfactory, outputformat, out); method = fop.getclass().getmethod(\"getuseragent\"); useragent = method.invoke(fop); } else { method = fopfactory.getclass().getmethod(\"newfouseragent\"); useragent = method.invoke(fopfactory); } class uaclass = useragent.getclass(); boolean b = getbooleanprop(\"accessibility\"); if (b != null) { method = uaclass.getmethod(\"setaccessibility\", boolean.class); method.invoke(useragent, b); } string s = getstringprop(\"author\"); if (s != null) { method = uaclass.getmethod(\"setauthor\", string.class); method.invoke(useragent, s); } if (\"1.x\".equals(fopversion)) { method = uaclass.getmethod(\"setbaseurl\", string.class); method.invoke(useragent, step.getnode().getbaseuri().tostring()); s = getstringprop(\"baseurl\"); if (s != null) { method.invoke(useragent, s); } } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\"); if (b != null) { method = uaclass.getmethod(\"setconservememorypolicy\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"creationdate\"); if (s != null) { dateformat df = dateformat.getdateinstance(); date d = df.parse(s); method = uaclass.getmethod(\"setcreationdate\", date.class); method.invoke(useragent, d); } s = getstringprop(\"creator\"); if (s != null) { method = uaclass.getmethod(\"setcreator\", string.class); method.invoke(useragent, s); } s = getstringprop(\"keywords\"); if (s != null) { method = uaclass.getmethod(\"setkeywords\", string.class); method.invoke(useragent, s); } b = getbooleanprop(\"locatorenabled\"); if (b != null) { method = uaclass.getmethod(\"setlocatorenabled\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"producer\"); if (s != null) { method = uaclass.getmethod(\"setproducer\", string.class); method.invoke(useragent, s); } s = getstringprop(\"subject\"); if (s != null) { method = uaclass.getmethod(\"setsubject\", string.class); method.invoke(useragent, s); } float f = getfloatprop(\"targetresolution\"); if (f != null) { method = uaclass.getmethod(\"settargetresolution\", float.class); method.invoke(useragent, f); } s = getstringprop(\"title\"); if (s != null) { method = uaclass.getmethod(\"settitle\", string.class); method.invoke(useragent, s); } if (\"2.x\".equals(fopversion)) { method = uaclass.getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(useragent, outputformat, out); } method = fop.getclass().getmethod(\"getdefaulthandler\"); object defhandler = method.invoke(fop); transformerfactory transformerfactory = transformerfactory.newinstance(); transformer transformer = transformerfactory.newtransformer(); transformer.transform(source, new saxresult((contenthandler) defhandler)); } catch (exception e) { throw new xprocexception(step.getnode(), \"failed to process fo document with fop\", e); } }","classification":"NONSATD","isFinished":true,"code_context_2":"outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\";","code_context_10":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) {","code_context_20":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) { method = fopfactory.getclass().getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(fopfactory, outputformat, out);","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2539,"comment_id":2,"comment":"\/\/\"afp\";","code":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) { method = fopfactory.getclass().getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(fopfactory, outputformat, out); method = fop.getclass().getmethod(\"getuseragent\"); useragent = method.invoke(fop); } else { method = fopfactory.getclass().getmethod(\"newfouseragent\"); useragent = method.invoke(fopfactory); } class uaclass = useragent.getclass(); boolean b = getbooleanprop(\"accessibility\"); if (b != null) { method = uaclass.getmethod(\"setaccessibility\", boolean.class); method.invoke(useragent, b); } string s = getstringprop(\"author\"); if (s != null) { method = uaclass.getmethod(\"setauthor\", string.class); method.invoke(useragent, s); } if (\"1.x\".equals(fopversion)) { method = uaclass.getmethod(\"setbaseurl\", string.class); method.invoke(useragent, step.getnode().getbaseuri().tostring()); s = getstringprop(\"baseurl\"); if (s != null) { method.invoke(useragent, s); } } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\"); if (b != null) { method = uaclass.getmethod(\"setconservememorypolicy\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"creationdate\"); if (s != null) { dateformat df = dateformat.getdateinstance(); date d = df.parse(s); method = uaclass.getmethod(\"setcreationdate\", date.class); method.invoke(useragent, d); } s = getstringprop(\"creator\"); if (s != null) { method = uaclass.getmethod(\"setcreator\", string.class); method.invoke(useragent, s); } s = getstringprop(\"keywords\"); if (s != null) { method = uaclass.getmethod(\"setkeywords\", string.class); method.invoke(useragent, s); } b = getbooleanprop(\"locatorenabled\"); if (b != null) { method = uaclass.getmethod(\"setlocatorenabled\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"producer\"); if (s != null) { method = uaclass.getmethod(\"setproducer\", string.class); method.invoke(useragent, s); } s = getstringprop(\"subject\"); if (s != null) { method = uaclass.getmethod(\"setsubject\", string.class); method.invoke(useragent, s); } float f = getfloatprop(\"targetresolution\"); if (f != null) { method = uaclass.getmethod(\"settargetresolution\", float.class); method.invoke(useragent, f); } s = getstringprop(\"title\"); if (s != null) { method = uaclass.getmethod(\"settitle\", string.class); method.invoke(useragent, s); } if (\"2.x\".equals(fopversion)) { method = uaclass.getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(useragent, outputformat, out); } method = fop.getclass().getmethod(\"getdefaulthandler\"); object defhandler = method.invoke(fop); transformerfactory transformerfactory = transformerfactory.newinstance(); transformer transformer = transformerfactory.newtransformer(); transformer.transform(source, new saxresult((contenthandler) defhandler)); } catch (exception e) { throw new xprocexception(step.getnode(), \"failed to process fo document with fop\", e); } }","classification":"NONSATD","isFinished":true,"code_context_2":"outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\";","code_context_10":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); }","code_context_20":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) { method = fopfactory.getclass().getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(fopfactory, outputformat, out); method = fop.getclass().getmethod(\"getuseragent\"); useragent = method.invoke(fop);","repo":"fsasaki\/xmlcalabash1-print"}
{"id":2539,"comment_id":3,"comment":"\/\/ fixme: how do i do this in 2.x?","code":"public void format(xdmnode doc, outputstream out, string contenttype) { string outputformat = null; if (contenttype == null || \"application\/pdf\".equalsignorecase(contenttype)) { outputformat = \"application\/pdf\"; \/\/ \"pdf\"; } else if (\"application\/postscript\".equalsignorecase(contenttype)) { outputformat = \"application\/postscript\"; \/\/\"postscript\"; } else if (\"application\/afp\".equalsignorecase(contenttype)) { outputformat = \"application\/x-afp\"; \/\/\"afp\"; } else if (\"application\/rtf\".equalsignorecase(contenttype)) { outputformat = \"application\/rtf\"; } else if (\"text\/plain\".equalsignorecase(contenttype)) { outputformat = \"text\/plain\"; } else { throw new xprocexception(step.getnode(), \"unsupported content-type on p:xsl-formatter: \" + contenttype); } if (! (\"1.x\".equals(fopversion) || \"2.x\".equals(fopversion))) { throw new xprocexception(\"unexpected fop version: \" + fopversion); } try { inputsource fodoc = s9apiutils.xdmtoinputsource(runtime, doc); saxsource source = new saxsource(fodoc); object useragent = null; object fop = null; if (\"1.x\".equals(fopversion)) { method = fopfactory.getclass().getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(fopfactory, outputformat, out); method = fop.getclass().getmethod(\"getuseragent\"); useragent = method.invoke(fop); } else { method = fopfactory.getclass().getmethod(\"newfouseragent\"); useragent = method.invoke(fopfactory); } class uaclass = useragent.getclass(); boolean b = getbooleanprop(\"accessibility\"); if (b != null) { method = uaclass.getmethod(\"setaccessibility\", boolean.class); method.invoke(useragent, b); } string s = getstringprop(\"author\"); if (s != null) { method = uaclass.getmethod(\"setauthor\", string.class); method.invoke(useragent, s); } if (\"1.x\".equals(fopversion)) { method = uaclass.getmethod(\"setbaseurl\", string.class); method.invoke(useragent, step.getnode().getbaseuri().tostring()); s = getstringprop(\"baseurl\"); if (s != null) { method.invoke(useragent, s); } } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\"); if (b != null) { method = uaclass.getmethod(\"setconservememorypolicy\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"creationdate\"); if (s != null) { dateformat df = dateformat.getdateinstance(); date d = df.parse(s); method = uaclass.getmethod(\"setcreationdate\", date.class); method.invoke(useragent, d); } s = getstringprop(\"creator\"); if (s != null) { method = uaclass.getmethod(\"setcreator\", string.class); method.invoke(useragent, s); } s = getstringprop(\"keywords\"); if (s != null) { method = uaclass.getmethod(\"setkeywords\", string.class); method.invoke(useragent, s); } b = getbooleanprop(\"locatorenabled\"); if (b != null) { method = uaclass.getmethod(\"setlocatorenabled\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"producer\"); if (s != null) { method = uaclass.getmethod(\"setproducer\", string.class); method.invoke(useragent, s); } s = getstringprop(\"subject\"); if (s != null) { method = uaclass.getmethod(\"setsubject\", string.class); method.invoke(useragent, s); } float f = getfloatprop(\"targetresolution\"); if (f != null) { method = uaclass.getmethod(\"settargetresolution\", float.class); method.invoke(useragent, f); } s = getstringprop(\"title\"); if (s != null) { method = uaclass.getmethod(\"settitle\", string.class); method.invoke(useragent, s); } if (\"2.x\".equals(fopversion)) { method = uaclass.getmethod(\"newfop\", string.class, outputstream.class); fop = method.invoke(useragent, outputformat, out); } method = fop.getclass().getmethod(\"getdefaulthandler\"); object defhandler = method.invoke(fop); transformerfactory transformerfactory = transformerfactory.newinstance(); transformer transformer = transformerfactory.newtransformer(); transformer.transform(source, new saxresult((contenthandler) defhandler)); } catch (exception e) { throw new xprocexception(step.getnode(), \"failed to process fo document with fop\", e); } }","classification":"DEFECT","isFinished":true,"code_context_2":"} } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\");","code_context_10":"method.invoke(useragent, s); } if (\"1.x\".equals(fopversion)) { method = uaclass.getmethod(\"setbaseurl\", string.class); method.invoke(useragent, step.getnode().getbaseuri().tostring()); s = getstringprop(\"baseurl\"); if (s != null) { method.invoke(useragent, s); } } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\"); if (b != null) { method = uaclass.getmethod(\"setconservememorypolicy\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"creationdate\"); if (s != null) { dateformat df = dateformat.getdateinstance(); date d = df.parse(s);","code_context_20":"} class uaclass = useragent.getclass(); boolean b = getbooleanprop(\"accessibility\"); if (b != null) { method = uaclass.getmethod(\"setaccessibility\", boolean.class); method.invoke(useragent, b); } string s = getstringprop(\"author\"); if (s != null) { method = uaclass.getmethod(\"setauthor\", string.class); method.invoke(useragent, s); } if (\"1.x\".equals(fopversion)) { method = uaclass.getmethod(\"setbaseurl\", string.class); method.invoke(useragent, step.getnode().getbaseuri().tostring()); s = getstringprop(\"baseurl\"); if (s != null) { method.invoke(useragent, s); } } else { \/\/ fixme: how do i do this in 2.x? } b = getbooleanprop(\"conservememorypolicy\"); if (b != null) { method = uaclass.getmethod(\"setconservememorypolicy\", boolean.class); method.invoke(useragent, b); } s = getstringprop(\"creationdate\"); if (s != null) { dateformat df = dateformat.getdateinstance(); date d = df.parse(s); method = uaclass.getmethod(\"setcreationdate\", date.class); method.invoke(useragent, d); } s = getstringprop(\"creator\"); if (s != null) { method = uaclass.getmethod(\"setcreator\", string.class); method.invoke(useragent, s); } s = getstringprop(\"keywords\"); if (s != null) {","repo":"fsasaki\/xmlcalabash1-print"}
{"id":10831,"comment_id":0,"comment":"\/\/ setup webview.","code":"@override @suppresslint(\"setjavascriptenabled\") protected void oncreate(@nullable bundle savedinstancestate) { dank.dependencyinjector().inject(this); super.oncreate(savedinstancestate); setcontentview(r.layout.activity_login); butterknife.bind(this); findandsetuptoolbar(); toolbar.setbackground(null); toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() { @override public void onprogresschanged(webview view, int newprogress) { if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } } }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false); webview.getsettings().setjavascriptenabled(true); webview.getsettings().setdomstorageenabled(true); userloginhelper = reddit.get().login().loginhelper(); webview.loadurl(userloginhelper.authorizationurl()); }","classification":"NONSATD","isFinished":true,"code_context_2":"toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() {","code_context_10":"@suppresslint(\"setjavascriptenabled\") protected void oncreate(@nullable bundle savedinstancestate) { dank.dependencyinjector().inject(this); super.oncreate(savedinstancestate); setcontentview(r.layout.activity_login); butterknife.bind(this); findandsetuptoolbar(); toolbar.setbackground(null); toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() { @override public void onprogresschanged(webview view, int newprogress) { if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } });","code_context_20":"@override @suppresslint(\"setjavascriptenabled\") protected void oncreate(@nullable bundle savedinstancestate) { dank.dependencyinjector().inject(this); super.oncreate(savedinstancestate); setcontentview(r.layout.activity_login); butterknife.bind(this); findandsetuptoolbar(); toolbar.setbackground(null); toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() { @override public void onprogresschanged(webview view, int newprogress) { if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show();","repo":"federa7675\/Dawn"}
{"id":10831,"comment_id":1,"comment":"\/\/ we've detected the redirect url.","code":"@override @suppresslint(\"setjavascriptenabled\") protected void oncreate(@nullable bundle savedinstancestate) { dank.dependencyinjector().inject(this); super.oncreate(savedinstancestate); setcontentview(r.layout.activity_login); butterknife.bind(this); findandsetuptoolbar(); toolbar.setbackground(null); toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() { @override public void onprogresschanged(webview view, int newprogress) { if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } } }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false); webview.getsettings().setjavascriptenabled(true); webview.getsettings().setdomstorageenabled(true); userloginhelper = reddit.get().login().loginhelper(); webview.loadurl(userloginhelper.authorizationurl()); }","classification":"NONSATD","isFinished":true,"code_context_2":"public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true;","code_context_10":"if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } }","code_context_20":"butterknife.bind(this); findandsetuptoolbar(); toolbar.setbackground(null); toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() { @override public void onprogresschanged(webview view, int newprogress) { if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } } }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false); webview.getsettings().setjavascriptenabled(true); webview.getsettings().setdomstorageenabled(true); userloginhelper = reddit.get().login().loginhelper(); webview.loadurl(userloginhelper.authorizationurl()); }","repo":"federa7675\/Dawn"}
{"id":10831,"comment_id":2,"comment":"\/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices.","code":"@override @suppresslint(\"setjavascriptenabled\") protected void oncreate(@nullable bundle savedinstancestate) { dank.dependencyinjector().inject(this); super.oncreate(savedinstancestate); setcontentview(r.layout.activity_login); butterknife.bind(this); findandsetuptoolbar(); toolbar.setbackground(null); toolbar.settitle(r.string.login); contentviewgroup.setcliptooutline(true); \/\/ setup webview. cookiemanager.getinstance().removeallcookies(null); webview.setwebchromeclient(new webchromeclient() { @override public void onprogresschanged(webview view, int newprogress) { if (!loggedin) { boolean shouldshowprogress = newprogress < 75; setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } } }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false); webview.getsettings().setjavascriptenabled(true); webview.getsettings().setdomstorageenabled(true); userloginhelper = reddit.get().login().loginhelper(); webview.loadurl(userloginhelper.authorizationurl()); }","classification":"DEFECT","isFinished":true,"code_context_2":"} }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false);","code_context_10":"loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } } }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false); webview.getsettings().setjavascriptenabled(true); webview.getsettings().setdomstorageenabled(true); userloginhelper = reddit.get().login().loginhelper(); webview.loadurl(userloginhelper.authorizationurl()); }","code_context_20":"setprogressvisible(shouldshowprogress); } } }); webview.setwebviewclient(new webviewclient() { @override public void onpagestarted(webview view, string url, bitmap favicon) { if (url.contains(\"code=\")) { \/\/ we've detected the redirect url. webview.stoploading(); loggedin = true; handleonpermissiongranted(url); } else if (url.contains(\"error=\")) { toast.maketext(loginactivity.this, r.string.login_error_oauth_permission_rejected, toast.length_long).show(); webview.stoploading(); setresult(result_canceled); finish(); } } }); \/\/ bug workaround: webview crashes when dropdown is shown on \/\/ a nougat emulator. haven't tested on other devices. webview.clearformdata(); webview.getsettings().setsaveformdata(false); webview.getsettings().setjavascriptenabled(true); webview.getsettings().setdomstorageenabled(true); userloginhelper = reddit.get().login().loginhelper(); webview.loadurl(userloginhelper.authorizationurl()); }","repo":"federa7675\/Dawn"}
{"id":2698,"comment_id":0,"comment":"\/** * since there isn't an obvious way to get the chip version, the * hub model is used to determine which chip is being used. *\/","code":"\/** * since there isn't an obvious way to get the chip version, the * hub model is used to determine which chip is being used. *\/ private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); } }","classification":"NONSATD","isFinished":true,"code_context_2":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); } }","code_context_10":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); } }","code_context_20":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); } }","repo":"eanderso\/arcusplatform"}
{"id":2698,"comment_id":1,"comment":"\/\/todo: get chip version directly if possible.","code":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); } }","classification":"DESIGN","isFinished":true,"code_context_2":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) {","code_context_10":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); }","code_context_20":"private static void setchipversion() { \/\/todo: get chip version directly if possible. string model = irishal.getmodel(); if (model.isv2(model)) { hardware.set(\"zm5304au-cme3r\"); } else if (model.isv3(model)) { hardware.set(\"zm5101\"); } else { hardware.set(\"unknown\"); } }","repo":"eanderso\/arcusplatform"}
{"id":19228,"comment_id":0,"comment":"\/\/ this is to fix a bug in the v7 support lib. if there is no options menu and you hit menu, it will crash with a \/\/ npe @ android.support.v7.app.actionbarimplics.getthemedcontext(actionbarimplics.java:274) \/\/ this can safely be removed if we add in menu options on this screen","code":"@override public boolean onkeydown(int keycode, keyevent event) { if ((keycode == keyevent.keycode_menu) && (null == getsupportactionbar())) { \/\/ this is to fix a bug in the v7 support lib. if there is no options menu and you hit menu, it will crash with a \/\/ npe @ android.support.v7.app.actionbarimplics.getthemedcontext(actionbarimplics.java:274) \/\/ this can safely be removed if we add in menu options on this screen return true; } return super.onkeydown(keycode, event); }","classification":"DESIGN","isFinished":true,"code_context_2":"public boolean onkeydown(int keycode, keyevent event) { if ((keycode == keyevent.keycode_menu) && (null == getsupportactionbar())) { \/\/ this is to fix a bug in the v7 support lib. if there is no options menu and you hit menu, it will crash with a \/\/ npe @ android.support.v7.app.actionbarimplics.getthemedcontext(actionbarimplics.java:274) \/\/ this can safely be removed if we add in menu options on this screen return true; }","code_context_10":"@override public boolean onkeydown(int keycode, keyevent event) { if ((keycode == keyevent.keycode_menu) && (null == getsupportactionbar())) { \/\/ this is to fix a bug in the v7 support lib. if there is no options menu and you hit menu, it will crash with a \/\/ npe @ android.support.v7.app.actionbarimplics.getthemedcontext(actionbarimplics.java:274) \/\/ this can safely be removed if we add in menu options on this screen return true; } return super.onkeydown(keycode, event); }","code_context_20":"@override public boolean onkeydown(int keycode, keyevent event) { if ((keycode == keyevent.keycode_menu) && (null == getsupportactionbar())) { \/\/ this is to fix a bug in the v7 support lib. if there is no options menu and you hit menu, it will crash with a \/\/ npe @ android.support.v7.app.actionbarimplics.getthemedcontext(actionbarimplics.java:274) \/\/ this can safely be removed if we add in menu options on this screen return true; } return super.onkeydown(keycode, event); }","repo":"developers16\/GithubsSample"}
{"id":19294,"comment_id":0,"comment":"\/** * show information related to program version. * * @param showprogramversion true to show program version * @return this *\/ \/\/todo remove","code":"\/** * show information related to program version. * * @param showprogramversion true to show program version * @return this *\/ \/\/todo remove public ffprobe setshowprogramversion(final boolean showprogramversion) { this.showprogramversion = showprogramversion; return this; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public ffprobe setshowprogramversion(final boolean showprogramversion) { this.showprogramversion = showprogramversion; return this; }","code_context_10":"public ffprobe setshowprogramversion(final boolean showprogramversion) { this.showprogramversion = showprogramversion; return this; }","code_context_20":"public ffprobe setshowprogramversion(final boolean showprogramversion) { this.showprogramversion = showprogramversion; return this; }","repo":"entropycoder\/Jaffree"}
{"id":19297,"comment_id":0,"comment":"\/** * show information about all pixel formats supported by ffmpeg. * * @param showpixelformats true to show pixel formats * @return this *\/ \/\/todo remove","code":"\/** * show information about all pixel formats supported by ffmpeg. * * @param showpixelformats true to show pixel formats * @return this *\/ \/\/todo remove public ffprobe setshowpixelformats(final boolean showpixelformats) { this.showpixelformats = showpixelformats; return this; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public ffprobe setshowpixelformats(final boolean showpixelformats) { this.showpixelformats = showpixelformats; return this; }","code_context_10":"public ffprobe setshowpixelformats(final boolean showpixelformats) { this.showpixelformats = showpixelformats; return this; }","code_context_20":"public ffprobe setshowpixelformats(final boolean showpixelformats) { this.showpixelformats = showpixelformats; return this; }","repo":"entropycoder\/Jaffree"}
{"id":19398,"comment_id":0,"comment":"\/\/gen-first:event_label_iconrealizarpedidomouseclicked","code":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true); tela_realizarpedido.setlocationrelativeto(null); }","classification":"NONSATD","isFinished":true,"code_context_2":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez","code_context_10":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true); tela_realizarpedido.setlocationrelativeto(null); }","code_context_20":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true); tela_realizarpedido.setlocationrelativeto(null); }","repo":"fsilva-c\/Screens_LP2"}
{"id":19398,"comment_id":1,"comment":"\/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez","code":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true); tela_realizarpedido.setlocationrelativeto(null); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true);","code_context_10":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true); tela_realizarpedido.setlocationrelativeto(null); }","code_context_20":"private void label_iconrealizarpedidomouseclicked(java.awt.event.mouseevent evt) {\/\/gen-first:event_label_iconrealizarpedidomouseclicked \/\/ todo add your handling code here: \/\/instanciar a tela cad pessoa apenas uma vez tela_realizarpedido= new realizar_pedido(this.conta); tela_realizarpedido.setvisible(true); tela_realizarpedido.setlocationrelativeto(null); }","repo":"fsilva-c\/Screens_LP2"}
{"id":19831,"comment_id":0,"comment":"\/** * todo * * @return *\/","code":"\/** * todo * * @return *\/ public expression getobject() { return fobject; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public expression getobject() { return fobject; }","code_context_10":"public expression getobject() { return fobject; }","code_context_20":"public expression getobject() { return fobject; }","repo":"duarterafael\/Conformitate"}
{"id":19832,"comment_id":0,"comment":"\/** * todo * * @return *\/","code":"\/** * todo * * @return *\/ public mattribute getattribute() { return fattribute; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public mattribute getattribute() { return fattribute; }","code_context_10":"public mattribute getattribute() { return fattribute; }","code_context_20":"public mattribute getattribute() { return fattribute; }","repo":"duarterafael\/Conformitate"}
{"id":19833,"comment_id":0,"comment":"\/** * todo * * @return *\/","code":"\/** * todo * * @return *\/ public mrvalue getrvalue() { return frvalue; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public mrvalue getrvalue() { return frvalue; }","code_context_10":"public mrvalue getrvalue() { return frvalue; }","code_context_20":"public mrvalue getrvalue() { return frvalue; }","repo":"duarterafael\/Conformitate"}
{"id":19851,"comment_id":0,"comment":"\/** * * @param ci * @return *\/","code":"\/** * * @param ci * @return *\/ public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"NONSATD","isFinished":true,"code_context_2":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","code_context_10":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","code_context_20":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":1,"comment":"\/\/if explorer is in createmode","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"NONSATD","isFinished":true,"code_context_2":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location:","code_context_10":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) {","code_context_20":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid();","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":2,"comment":"\/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"NONSATD","isFinished":true,"code_context_2":"break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid();","code_context_10":"newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin();","code_context_20":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin();","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":3,"comment":"\/\/todo should be a specific blog interface","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"DESIGN","isFinished":true,"code_context_2":"break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin();","code_context_10":"\/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\";","code_context_20":"} break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\";","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":4,"comment":"\/\/todo change!!!","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"DESIGN","isFinished":true,"code_context_2":"break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\";","code_context_10":"} else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else {","code_context_20":"\/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } }","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":5,"comment":"\/\/res = \"content\/photoroute.xhtml\";","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break;","code_context_10":"res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin();","code_context_20":"\/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois }","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":6,"comment":"\/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else {","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"NONSATD","isFinished":true,"code_context_2":"break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin();","code_context_10":"res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } }","code_context_20":"if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":7,"comment":"\/\/}","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin();","code_context_10":"\/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } }","code_context_20":"} else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","repo":"fregaham\/KiWi"}
{"id":19851,"comment_id":8,"comment":"\/\/todo some common type for all pois","code":"public string gettagitlayoutpath() { string res = \"content\/none.xhtml\"; \/\/if explorer is in createmode switch( exploreraction.getmode() ) { case create_location: res = \"content_new\/location_new.xhtml\"; if( newlocationaction.getmode() == -1 ) { newlocationaction.begin(); } break; case create_route: res = \"content_new\/route_new.xhtml\"; if( newrouteaction.getmode() != 2 ) { newrouteaction.setmode(2); } break; default: \/\/there must be a ranking because everything that is displayable on tagit \/\/is a pointofinetrest but maybe also another type (like a route) \/\/until then we break, when contentitem has toptype (like route) \/\/init with none for( kiwiresource type : currentcontentitem.gettypes() ) { string serqlid = type.getserqlid(); if(serqlid.contains(constants.ns_fcp_core+\"location\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core+\"blogpost\")) { \/\/todo should be a specific blog interface res = \"content\/blog.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_fcp_core+\"newsitem\")) { res = \"content\/newsitem.xhtml\"; newsitemaction.begin(); break; } else if(serqlid.contains(constants.ns_tagit + \"route\")) { \/\/todo change!!! res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"} } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res);","code_context_10":"res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","code_context_20":"res = \"content\/route.xhtml\"; \/\/res = \"content\/photoroute.xhtml\"; routeaction.begin(); break; } else if(serqlid.contains(constants.ns_kiwi_core + \"user\")) { \/\/if( currentuser.getcontentitem().getid() == currentcontentitem.getid() ) { \/\/ res = \"content\/user.xhtml\"; \/\/ personaction.begin(); \/\/ break; \/\/} else { res = \"content\/person.xhtml\"; personaction.begin(); break; \/\/} } else if(serqlid.contains(constants.ns_demo+\"locatedmeeting\")) { res = \"content\/location.xhtml\"; locationaction.begin(); break; } } \/\/todo some common type for all pois } log.info(\"set layout to #0\",res); return res; }","repo":"fregaham\/KiWi"}
{"id":11778,"comment_id":0,"comment":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ \/\/ load \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/","code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ \/\/ load \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","code_context_10":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","code_context_20":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","repo":"fanx-dev\/fantom"}
{"id":11778,"comment_id":1,"comment":"\/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes)","code":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes];","code_context_10":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name);","code_context_20":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i)","repo":"fanx-dev\/fantom"}
{"id":11778,"comment_id":2,"comment":"\/\/ create type instance","code":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures","code_context_10":"{ this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null)","code_context_20":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length];","repo":"fanx-dev\/fantom"}
{"id":11778,"comment_id":3,"comment":"\/\/ add to my data structures","code":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null)","code_context_10":"this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super","code_context_20":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]);","repo":"fanx-dev\/fantom"}
{"id":11778,"comment_id":4,"comment":"\/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod)","code":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null)","code_context_10":"types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i];","code_context_20":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","repo":"fanx-dev\/fantom"}
{"id":11778,"comment_id":5,"comment":"\/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded)","code":"void load(fpod fpod) { this.fpod = fpod; this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) {","code_context_10":"\/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); }","code_context_20":"this.typesbyname = new hashmap(); \/\/ create a hollow type for each ftype (this requires two steps, \/\/ because we don't necessary have all the types created for \/\/ superclasses until this loop completes) int numtypes = fpod.types == null ? 0 : fpod.types.length; types = new classtype[numtypes]; for (int i=0; i<numtypes; ++i) { \/\/ create type instance classtype type = new classtype(this, fpod.types[i]); \/\/ add to my data structures types[i] = type; if (typesbyname.put(type.name, type) != null) throw err.make(\"invalid pod: \" + name + \" type already defined: \" + type.name); } \/\/ get typetype to use for mixin list (we need to handle case \/\/ when loading sys itself - and lookup within my own pod) type typetype = sys.typetype; if (typetype == null) typetype = (type)typesbyname.get(\"type\"); \/\/ now that everthing is mapped, we can fill in the super \/\/ class fields (unless something is wacked, this will only \/\/ use types in my pod or in pods already loaded) for (int i=0; i<numtypes; ++i) { ftype ftype = fpod.types[i]; classtype type = types[i]; type.base = type(ftype.base); object[] mixins = new object[ftype.mixins.length]; for (int j=0; j<mixins.length; ++j) mixins[j] = type(ftype.mixins[j]); type.mixins = new list(typetype, mixins).ro(); } }","repo":"fanx-dev\/fantom"}
{"id":11829,"comment_id":0,"comment":"\/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want.","code":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","classification":"DESIGN","isFinished":true,"code_context_2":"public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) {","code_context_10":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup.","code_context_20":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","repo":"dynin\/ideal"}
{"id":11829,"comment_id":1,"comment":"\/\/ todo: check that flavor is readonly or mutable.","code":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup.","code_context_10":"\/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","code_context_20":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","repo":"dynin\/ideal"}
{"id":11829,"comment_id":2,"comment":"\/\/ todo: replace this with a promotion lookup.","code":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","classification":"DESIGN","isFinished":true,"code_context_2":"\/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else {","code_context_10":"\/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","code_context_20":"@override public action to_value(action expression, origin the_origin) { if (constraints != null) { \/\/ we need to specially handled narrowed variables here \/\/ because the reference type is not narrowed. \/\/ say the variable declaration is \"string or null foo\", and it's narrowed to string. \/\/ the is_reference_type(the_type) would return a union type, which is not what we want. action narrowed_action = can_narrow(expression, constraints); if (narrowed_action != null) { return narrowed_action; } } type the_type = expression.result().type_bound(); if (common_types.is_reference_type(the_type)) { \/\/ todo: check that flavor is readonly or mutable. type value_type = common_types.get_reference_parameter(the_type); \/\/ todo: replace this with a promotion lookup. return promote(expression, value_type, the_origin); } else { return expression; } }","repo":"dynin\/ideal"}
{"id":20164,"comment_id":0,"comment":"\/\/ todo: implement disconnect and reconnect to postgresql instance.","code":"@override protected void reloadsettings( list<string> updatedsettings ) { \/\/ todo: implement disconnect and reconnect to postgresql instance. }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@override protected void reloadsettings( list<string> updatedsettings ) { \/\/ todo: implement disconnect and reconnect to postgresql instance. }","code_context_10":"@override protected void reloadsettings( list<string> updatedsettings ) { \/\/ todo: implement disconnect and reconnect to postgresql instance. }","code_context_20":"@override protected void reloadsettings( list<string> updatedsettings ) { \/\/ todo: implement disconnect and reconnect to postgresql instance. }","repo":"em3ndez\/Polypheny-DB"}
{"id":20491,"comment_id":0,"comment":"\/** * attempts to sign in or register the account specified by the login form. * if there are form errors (invalid email, missing fields, etc.), the * errors are presented and no actual login attempt is made. *\/","code":"\/** * attempts to sign in or register the account specified by the login form. * if there are form errors (invalid email, missing fields, etc.), the * errors are presented and no actual login attempt is made. *\/ private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","code_context_10":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","code_context_20":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":1,"comment":"\/\/ reset errors.","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null);","code_context_10":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required));","code_context_20":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) {","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":2,"comment":"\/\/ store values at the time of the login attempt.","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring();","code_context_10":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; }","code_context_20":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus();","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":3,"comment":"\/\/ check for a valid password.","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required));","code_context_10":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true;","code_context_20":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":4,"comment":"\/\/ check for a valid email address.","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required));","code_context_10":"final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else {","code_context_20":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try {","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":5,"comment":"\/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error.","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else {","code_context_10":"focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null;","code_context_20":"memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname);","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":6,"comment":"\/\/we show the loader and hide the form","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods","code_context_10":"if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name);","code_context_20":"final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse();","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":7,"comment":"\/\/we set the response listener with corresponding overridden methods","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override","code_context_10":"focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) {","code_context_20":"view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":8,"comment":"\/\/todo: use method usermanager.loginuser loading user from database","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"DESIGN","isFinished":true,"code_context_2":"e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname);","code_context_10":"public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); }","code_context_20":"if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","repo":"edwinperaza\/playmagnet"}
{"id":20491,"comment_id":9,"comment":"\/\/we add the request","code":"private void attemptlogin() { \/\/ reset errors. memailview.seterror(null); mpasswordview.seterror(null); \/\/ store values at the time of the login attempt. final string email = memailview.gettext().tostring(); final string password = mpasswordview.gettext().tostring(); boolean cancel = false; view focusview = null; \/\/ check for a valid password. if (textutils.isempty(password)) { mpasswordview.seterror(getstring(r.string.error_field_required)); focusview = mpasswordview; cancel = true; } \/\/ check for a valid email address. if (textutils.isempty(email)) { memailview.seterror(getstring(r.string.error_field_required)); focusview = memailview; cancel = true; } if (cancel) { \/\/ there was an error; don't attempt login and focus the first \/\/ form field with an error. focusview.requestfocus(); } else { \/\/we show the loader and hide the form showhideview(mprogressview, mloginformview, true); \/\/we set the response listener with corresponding overridden methods appresponselistener<jsonobject> responselistener = new appresponselistener<jsonobject>(getapplicationcontext()){ @override public void onresponse(jsonobject response) { context context = getapplicationcontext(); string firstname = null; string lastname = null; try { firstname = response.getstring(userrequestmanager.first_name); lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request);","code_context_10":"} @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","code_context_20":"lastname = response.getstring(userrequestmanager.last_name); } catch (jsonexception e) { e.printstacktrace(); } \/\/todo: use method usermanager.loginuser loading user from database prefsmanager.saveusercredentials(context, email, password); prefsmanager.setstringpref(context, prefsmanager.pref_user_first_name, firstname); prefsmanager.setstringpref(context, prefsmanager.pref_user_last_name, lastname); startactivityclosingallothers(draweractivity.class); onpostresponse(); } @override public void onunauthorizederror(volleyerror error) { showtoast(r.string.error_wrong_credentials); } @override public void onpostresponse(){ showhideview(mloginformview, mprogressview, true); } }; \/\/we add the request jsonobjectrequest request = userrequestmanager.userloginrequest(email, password, responselistener); volleymanager.getinstance(getapplicationcontext()).addtorequestqueue(request); } }","repo":"edwinperaza\/playmagnet"}
{"id":20492,"comment_id":0,"comment":"\/\/todo: replace this with your own logic","code":"public static boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public static boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","code_context_10":"public static boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","code_context_20":"public static boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","repo":"edwinperaza\/playmagnet"}
{"id":20609,"comment_id":0,"comment":"\/** * set the shutter with a string. *\/","code":"\/** * set the shutter with a string. *\/ private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","classification":"NONSATD","isFinished":true,"code_context_2":"private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","code_context_10":"private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","code_context_20":"private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","repo":"cquiroz\/ocs"}
{"id":20609,"comment_id":1,"comment":"\/\/ xxx: fix for older xml files saved with wrong shutter value","code":"private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","classification":"DEFECT","isFinished":true,"code_context_2":"shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next();","code_context_10":"private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","code_context_20":"private void _setshutter(string name) { shutter oldvalue = getshutter(); shutter shutter = shutter.getshutter(name, oldvalue); \/\/ xxx: fix for older xml files saved with wrong shutter value if (shutter == shutter.open && _lamps.size() == 1) { lamp lamp = _lamps.iterator().next(); if (lamp != lamp.ir_grey_body_high && lamp != lamp.ir_grey_body_low) { shutter = shutter.closed; } } setshutter(shutter); }","repo":"cquiroz\/ocs"}
{"id":20684,"comment_id":0,"comment":"\/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true));","code":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff); compressbuffer = b; } zip.setlevel(deflater.default_compression); break; } coff += rd; } byte[] tmp = frame.data; frame.data = compressbuffer; compressbuffer = tmp; frame.enddata = coff; }","classification":"NONSATD","isFinished":true,"code_context_2":"compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ?","code_context_10":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1);","code_context_20":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff); compressbuffer = b;","repo":"costinm\/tomcat"}
{"id":20684,"comment_id":1,"comment":"\/\/ last byte for flush ?","code":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff); compressbuffer = b; } zip.setlevel(deflater.default_compression); break; } coff += rd; } byte[] tmp = frame.data; frame.data = compressbuffer; compressbuffer = tmp; frame.enddata = coff; }","classification":"DESIGN","isFinished":true,"code_context_2":"\/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start;","code_context_10":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) {","code_context_20":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff); compressbuffer = b; } zip.setlevel(deflater.default_compression);","repo":"costinm\/tomcat"}
{"id":20684,"comment_id":2,"comment":"\/\/ needsinput needs to be called - we're done with this frame ?","code":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff); compressbuffer = b; } zip.setlevel(deflater.default_compression); break; } coff += rd; } byte[] tmp = frame.data; frame.data = compressbuffer; compressbuffer = tmp; frame.enddata = coff; }","classification":"DESIGN","isFinished":true,"code_context_2":"int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed);","code_context_10":"} \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff);","code_context_20":"@override public synchronized void compress(spdyframe frame, int start) throws ioexception { init(frame.version); if (compressbuffer == null) { compressbuffer = new byte[frame.data.length]; } \/\/system.out.println(hexdumplistener.gethexdump(frame.data, 0, frame.enddata, true, true)); deflater zip = zipout; \/\/ last byte for flush ? zip.setinput(frame.data, start, frame.enddata - start - 1); int coff = start; zip.setlevel(deflater.default_compression); while (true) { int rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); if (rd == 0) { \/\/ needsinput needs to be called - we're done with this frame ? zip.setinput(frame.data, frame.enddata - 1, 1); zip.setlevel(deflater.best_speed); while (true) { rd = zip.deflate(compressbuffer, coff, compressbuffer.length - coff); coff += rd; if (rd == 0) { break; } byte[] b = new byte[compressbuffer.length * 2]; system.arraycopy(compressbuffer, 0, b, 0, coff); compressbuffer = b; } zip.setlevel(deflater.default_compression); break; } coff += rd; } byte[] tmp = frame.data; frame.data = compressbuffer; compressbuffer = tmp;","repo":"costinm\/tomcat"}
{"id":12650,"comment_id":0,"comment":"\/\/ workaround for failing tests. when these tests are executed in \/\/ separate test methods, only the first one passes successfully.","code":"@test public void test() throws exception { \/\/ workaround for failing tests. when these tests are executed in \/\/ separate test methods, only the first one passes successfully. testlogdebug(); testlogerror(); testlogwarn(); testloginfo(); testlogtrace(); testlogdebugwiththrowable(); testlogerrorwiththrowable(); testlogwarnwiththrowable(); testloginfowiththrowable(); testlogtracewiththrowable(); }","classification":"TEST","isFinished":true,"code_context_2":"@test public void test() throws exception { \/\/ workaround for failing tests. when these tests are executed in \/\/ separate test methods, only the first one passes successfully. testlogdebug(); testlogerror();","code_context_10":"@test public void test() throws exception { \/\/ workaround for failing tests. when these tests are executed in \/\/ separate test methods, only the first one passes successfully. testlogdebug(); testlogerror(); testlogwarn(); testloginfo(); testlogtrace(); testlogdebugwiththrowable(); testlogerrorwiththrowable(); testlogwarnwiththrowable(); testloginfowiththrowable(); testlogtracewiththrowable();","code_context_20":"@test public void test() throws exception { \/\/ workaround for failing tests. when these tests are executed in \/\/ separate test methods, only the first one passes successfully. testlogdebug(); testlogerror(); testlogwarn(); testloginfo(); testlogtrace(); testlogdebugwiththrowable(); testlogerrorwiththrowable(); testlogwarnwiththrowable(); testloginfowiththrowable(); testlogtracewiththrowable(); }","repo":"delchev\/cloud-dirigible"}
{"id":20936,"comment_id":0,"comment":"\/\/metodo para adicionar funcionario produtividade","code":"\/\/metodo para adicionar funcionario produtividade public static void addfuncprodutividade() { funcionarios[1] = new funcprodutividade(joptionpane.showinputdialog(\"numero do bi do funcionario produtividade\"), joptionpane.showinputdialog(\"data de ingresso do funcionario produtividade\"), double.parsedouble(joptionpane.showinputdialog(\"salario do funcionario produtividade\")), integer.parseint(joptionpane.showinputdialog(\"unidade produzida do funcionario produtividade\")), double.parsedouble(joptionpane.showinputdialog(\"valor da unidade produzida funcionario produtividade\"))); }","classification":"DESIGN","isFinished":true,"code_context_2":"public static void addfuncprodutividade() { funcionarios[1] = new funcprodutividade(joptionpane.showinputdialog(\"numero do bi do funcionario produtividade\"), joptionpane.showinputdialog(\"data de ingresso do funcionario produtividade\"), double.parsedouble(joptionpane.showinputdialog(\"salario do funcionario produtividade\")), integer.parseint(joptionpane.showinputdialog(\"unidade produzida do funcionario produtividade\")), double.parsedouble(joptionpane.showinputdialog(\"valor da unidade produzida funcionario produtividade\"))); }","code_context_10":"public static void addfuncprodutividade() { funcionarios[1] = new funcprodutividade(joptionpane.showinputdialog(\"numero do bi do funcionario produtividade\"), joptionpane.showinputdialog(\"data de ingresso do funcionario produtividade\"), double.parsedouble(joptionpane.showinputdialog(\"salario do funcionario produtividade\")), integer.parseint(joptionpane.showinputdialog(\"unidade produzida do funcionario produtividade\")), double.parsedouble(joptionpane.showinputdialog(\"valor da unidade produzida funcionario produtividade\"))); }","code_context_20":"public static void addfuncprodutividade() { funcionarios[1] = new funcprodutividade(joptionpane.showinputdialog(\"numero do bi do funcionario produtividade\"), joptionpane.showinputdialog(\"data de ingresso do funcionario produtividade\"), double.parsedouble(joptionpane.showinputdialog(\"salario do funcionario produtividade\")), integer.parseint(joptionpane.showinputdialog(\"unidade produzida do funcionario produtividade\")), double.parsedouble(joptionpane.showinputdialog(\"valor da unidade produzida funcionario produtividade\"))); }","repo":"fernandogomesfg\/Algoritmos-em-Java"}
{"id":20938,"comment_id":0,"comment":"\/\/metodo para imprimir salario do funcionario produtividade","code":"\/\/metodo para imprimir salario do funcionario produtividade public static void printsalarioprodutividade() { for (int i = 0; i < funcionarios.length; i++) { if (funcionarios[i] instanceof funcprodutividade) { system.out.println(funcionarios[i].calcularremuneracao()); } } }","classification":"DESIGN","isFinished":true,"code_context_2":"public static void printsalarioprodutividade() { for (int i = 0; i < funcionarios.length; i++) { if (funcionarios[i] instanceof funcprodutividade) { system.out.println(funcionarios[i].calcularremuneracao()); } } }","code_context_10":"public static void printsalarioprodutividade() { for (int i = 0; i < funcionarios.length; i++) { if (funcionarios[i] instanceof funcprodutividade) { system.out.println(funcionarios[i].calcularremuneracao()); } } }","code_context_20":"public static void printsalarioprodutividade() { for (int i = 0; i < funcionarios.length; i++) { if (funcionarios[i] instanceof funcprodutividade) { system.out.println(funcionarios[i].calcularremuneracao()); } } }","repo":"fernandogomesfg\/Algoritmos-em-Java"}
{"id":12906,"comment_id":0,"comment":"\/\/*todo consider use list with initial cap size","code":"private void distribute(distribution[] distributionlist, int iterationnum) { distribution[] busy = null; \/\/*todo consider use list with initial cap size for (int inc = 0; inc < distributionlist.length; inc++) { if (distributionlist[inc] == null) continue; if (!distributionlist[inc].getistepdecorator().offerqueuesubjectupdate(distributionlist[inc].getdata(), distributionlist[inc].getsubject())) { if (busy == null) { busy = new distribution[distributionlist.length]; } busy[inc] = distributionlist[inc]; logger.debug(\"distribution not succeeded. moving to deceleration mode for subject: \" + busy[inc].getsubject() + \". iteration number - \" + iterationnum + \"\/\" + maximum_offers_retries); } } if (!isempty(busy)) { \/\/* ***** deceleration mode ***** if (iterationnum >= maximum_offers_retries) { logger.debug(\"deceleration mode failed after \" + iterationnum + \"\/\" + maximum_offers_retries + \" retries. moving back to normal distribution\"); for (distribution dist : distributionlist) { if (dist == null) continue; dist.getistepdecorator().queuesubjectupdate(dist.getdata(), dist.getsubject()); } } try { logger.debug(\"retarding the distribution for \" + wait_period_milli * iterationnum + \" milliseconds\"); thread.sleep(wait_period_milli * iterationnum); } catch (interruptedexception e) { throw new steppingsystemexception(\"distribution timeout failed\"); } distribute(busy, ++iterationnum); } }","classification":"DESIGN","isFinished":true,"code_context_2":"private void distribute(distribution[] distributionlist, int iterationnum) { distribution[] busy = null; \/\/*todo consider use list with initial cap size for (int inc = 0; inc < distributionlist.length; inc++) { if (distributionlist[inc] == null)","code_context_10":"private void distribute(distribution[] distributionlist, int iterationnum) { distribution[] busy = null; \/\/*todo consider use list with initial cap size for (int inc = 0; inc < distributionlist.length; inc++) { if (distributionlist[inc] == null) continue; if (!distributionlist[inc].getistepdecorator().offerqueuesubjectupdate(distributionlist[inc].getdata(), distributionlist[inc].getsubject())) { if (busy == null) { busy = new distribution[distributionlist.length]; } busy[inc] = distributionlist[inc]; logger.debug(\"distribution not succeeded. moving to deceleration mode for subject: \" + busy[inc].getsubject() + \". iteration number - \" + iterationnum + \"\/\" + maximum_offers_retries); }","code_context_20":"private void distribute(distribution[] distributionlist, int iterationnum) { distribution[] busy = null; \/\/*todo consider use list with initial cap size for (int inc = 0; inc < distributionlist.length; inc++) { if (distributionlist[inc] == null) continue; if (!distributionlist[inc].getistepdecorator().offerqueuesubjectupdate(distributionlist[inc].getdata(), distributionlist[inc].getsubject())) { if (busy == null) { busy = new distribution[distributionlist.length]; } busy[inc] = distributionlist[inc]; logger.debug(\"distribution not succeeded. moving to deceleration mode for subject: \" + busy[inc].getsubject() + \". iteration number - \" + iterationnum + \"\/\" + maximum_offers_retries); } } if (!isempty(busy)) { \/\/* ***** deceleration mode ***** if (iterationnum >= maximum_offers_retries) { logger.debug(\"deceleration mode failed after \" + iterationnum + \"\/\" + maximum_offers_retries + \" retries. moving back to normal distribution\"); for (distribution dist : distributionlist) { if (dist == null) continue; dist.getistepdecorator().queuesubjectupdate(dist.getdata(), dist.getsubject()); }","repo":"gabibeyo\/stepping"}
{"id":12906,"comment_id":1,"comment":"\/\/* ***** deceleration mode *****","code":"private void distribute(distribution[] distributionlist, int iterationnum) { distribution[] busy = null; \/\/*todo consider use list with initial cap size for (int inc = 0; inc < distributionlist.length; inc++) { if (distributionlist[inc] == null) continue; if (!distributionlist[inc].getistepdecorator().offerqueuesubjectupdate(distributionlist[inc].getdata(), distributionlist[inc].getsubject())) { if (busy == null) { busy = new distribution[distributionlist.length]; } busy[inc] = distributionlist[inc]; logger.debug(\"distribution not succeeded. moving to deceleration mode for subject: \" + busy[inc].getsubject() + \". iteration number - \" + iterationnum + \"\/\" + maximum_offers_retries); } } if (!isempty(busy)) { \/\/* ***** deceleration mode ***** if (iterationnum >= maximum_offers_retries) { logger.debug(\"deceleration mode failed after \" + iterationnum + \"\/\" + maximum_offers_retries + \" retries. moving back to normal distribution\"); for (distribution dist : distributionlist) { if (dist == null) continue; dist.getistepdecorator().queuesubjectupdate(dist.getdata(), dist.getsubject()); } } try { logger.debug(\"retarding the distribution for \" + wait_period_milli * iterationnum + \" milliseconds\"); thread.sleep(wait_period_milli * iterationnum); } catch (interruptedexception e) { throw new steppingsystemexception(\"distribution timeout failed\"); } distribute(busy, ++iterationnum); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} if (!isempty(busy)) { \/\/* ***** deceleration mode ***** if (iterationnum >= maximum_offers_retries) { logger.debug(\"deceleration mode failed after \" + iterationnum + \"\/\" + maximum_offers_retries + \" retries. moving back to normal distribution\");","code_context_10":"continue; if (!distributionlist[inc].getistepdecorator().offerqueuesubjectupdate(distributionlist[inc].getdata(), distributionlist[inc].getsubject())) { if (busy == null) { busy = new distribution[distributionlist.length]; } busy[inc] = distributionlist[inc]; logger.debug(\"distribution not succeeded. moving to deceleration mode for subject: \" + busy[inc].getsubject() + \". iteration number - \" + iterationnum + \"\/\" + maximum_offers_retries); } } if (!isempty(busy)) { \/\/* ***** deceleration mode ***** if (iterationnum >= maximum_offers_retries) { logger.debug(\"deceleration mode failed after \" + iterationnum + \"\/\" + maximum_offers_retries + \" retries. moving back to normal distribution\"); for (distribution dist : distributionlist) { if (dist == null) continue; dist.getistepdecorator().queuesubjectupdate(dist.getdata(), dist.getsubject()); } } try { logger.debug(\"retarding the distribution for \" + wait_period_milli * iterationnum + \" milliseconds\");","code_context_20":"private void distribute(distribution[] distributionlist, int iterationnum) { distribution[] busy = null; \/\/*todo consider use list with initial cap size for (int inc = 0; inc < distributionlist.length; inc++) { if (distributionlist[inc] == null) continue; if (!distributionlist[inc].getistepdecorator().offerqueuesubjectupdate(distributionlist[inc].getdata(), distributionlist[inc].getsubject())) { if (busy == null) { busy = new distribution[distributionlist.length]; } busy[inc] = distributionlist[inc]; logger.debug(\"distribution not succeeded. moving to deceleration mode for subject: \" + busy[inc].getsubject() + \". iteration number - \" + iterationnum + \"\/\" + maximum_offers_retries); } } if (!isempty(busy)) { \/\/* ***** deceleration mode ***** if (iterationnum >= maximum_offers_retries) { logger.debug(\"deceleration mode failed after \" + iterationnum + \"\/\" + maximum_offers_retries + \" retries. moving back to normal distribution\"); for (distribution dist : distributionlist) { if (dist == null) continue; dist.getistepdecorator().queuesubjectupdate(dist.getdata(), dist.getsubject()); } } try { logger.debug(\"retarding the distribution for \" + wait_period_milli * iterationnum + \" milliseconds\"); thread.sleep(wait_period_milli * iterationnum); } catch (interruptedexception e) { throw new steppingsystemexception(\"distribution timeout failed\"); } distribute(busy, ++iterationnum); } }","repo":"gabibeyo\/stepping"}
{"id":13058,"comment_id":0,"comment":"\/\/ probably incorrect - comparing object[] arrays with arrays.equals","code":"@override public boolean equals(object o) { if (this == o) return true; if (!(o instanceof checkchainmap)) return false; if (!super.equals(o)) return false; checkchainmap<?, ?> that = (checkchainmap<?, ?>) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals return arrays.equals(table, that.table); }","classification":"DESIGN","isFinished":true,"code_context_2":"if (!super.equals(o)) return false; checkchainmap<?, ?> that = (checkchainmap<?, ?>) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals return arrays.equals(table, that.table); }","code_context_10":"@override public boolean equals(object o) { if (this == o) return true; if (!(o instanceof checkchainmap)) return false; if (!super.equals(o)) return false; checkchainmap<?, ?> that = (checkchainmap<?, ?>) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals return arrays.equals(table, that.table); }","code_context_20":"@override public boolean equals(object o) { if (this == o) return true; if (!(o instanceof checkchainmap)) return false; if (!super.equals(o)) return false; checkchainmap<?, ?> that = (checkchainmap<?, ?>) o; \/\/ probably incorrect - comparing object[] arrays with arrays.equals return arrays.equals(table, that.table); }","repo":"finefuture\/gazelle"}
{"id":13286,"comment_id":0,"comment":"\/\/todo: replace this with your own logic","code":"private boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","classification":"DESIGN","isFinished":true,"code_context_2":"private boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","code_context_10":"private boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","code_context_20":"private boolean isemailvalid(string email) { \/\/todo: replace this with your own logic return email.contains(\"@\"); }","repo":"eZubia\/scrum-team"}
{"id":13333,"comment_id":0,"comment":"\/** * returns the index of the entry whose extracted value matches the given target, or -1 if no matching * entry is found. note that if multiple matching entries exist, the returned value is specified by * inclusionmode instead of being undefined. * * todo: reconsider whether this should be hidden from view * * @param target the value sought * @param mode controls which index to return if multiple matching entries are found * @return the index, or -1 if none are found *\/","code":"\/** * returns the index of the entry whose extracted value matches the given target, or -1 if no matching * entry is found. note that if multiple matching entries exist, the returned value is specified by * inclusionmode instead of being undefined. * * todo: reconsider whether this should be hidden from view * * @param target the value sought * @param mode controls which index to return if multiple matching entries are found * @return the index, or -1 if none are found *\/ public int find(v target, inclusionmode mode) { return find(target, mode, matchrequirement.exact_only); }","classification":"DESIGN","isFinished":true,"code_context_2":"public int find(v target, inclusionmode mode) { return find(target, mode, matchrequirement.exact_only); }","code_context_10":"public int find(v target, inclusionmode mode) { return find(target, mode, matchrequirement.exact_only); }","code_context_20":"public int find(v target, inclusionmode mode) { return find(target, mode, matchrequirement.exact_only); }","repo":"devjn\/fesimplegeoprox-android-maps"}
{"id":13381,"comment_id":0,"comment":"\/** * close the message channel. *\/","code":"\/** * close the message channel. *\/ public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","code_context_10":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","code_context_20":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","repo":"fhg-fokus-nubomedia\/signaling-plane"}
{"id":13381,"comment_id":1,"comment":"\/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response","code":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug))","code_context_10":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); }","code_context_20":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream);","repo":"fhg-fokus-nubomedia\/signaling-plane"}
{"id":13381,"comment_id":2,"comment":"\/\/ no need to close myclientinputstream since myparser.close() above will do it","code":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug))","code_context_10":"} catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } }","code_context_20":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); }","repo":"fhg-fokus-nubomedia\/signaling-plane"}
{"id":13381,"comment_id":3,"comment":"\/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap","code":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug))","code_context_10":"if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) {","code_context_20":"logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","repo":"fhg-fokus-nubomedia\/signaling-plane"}
{"id":13381,"comment_id":4,"comment":"\/\/ issue 358 : remove socket and semaphore on close to avoid leaking","code":"public void close(boolean removesocket, boolean stopkeepalivetask) { isrunning = false; \/\/ we need to close everything because the socket may be closed by the other end \/\/ like in lb scenarios sending options and killing the socket after it gets the response if (mysock != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing socket \" + key); try { mysock.close(); mysock = null; } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing socket \" + ex); } } if(myparser != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","classification":"NONSATD","isFinished":true,"code_context_2":"if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) {","code_context_10":"} catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } }","code_context_20":"if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing my parser \" + myparser); myparser.close(); } \/\/ no need to close myclientinputstream since myparser.close() above will do it if(myclientoutputstream != null) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing client output stream \" + myclientoutputstream); try { myclientoutputstream.close(); } catch (ioexception ex) { if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"error closing client output stream\" + ex); } } if(removesocket) { \/\/ remove the \"tcp:\" part of the key to cleanup the iohandler hashmap string iohandlerkey = key.substring(4); if (logger.isloggingenabled(logwriter.trace_debug)) logger.logdebug(\"closing tcp socket \" + iohandlerkey); \/\/ issue 358 : remove socket and semaphore on close to avoid leaking sipstack.iohandler.removesocket(iohandlerkey); if (logger.isloggingenabled(logwriter.trace_debug)) { logger.logdebug(\"closing message channel (key = \" + key +\")\" + this); } } else { if (logger.isloggingenabled(logwriter.trace_debug)) { string iohandlerkey = key.substring(4); logger.logdebug(\"not removing socket key from the cached map since it has already been updated by the iohandler.sendbytes \" + iohandlerkey); } } if(stopkeepalivetask) { cancelpingkeepalivetimeouttaskifstarted(); } }","repo":"fhg-fokus-nubomedia\/signaling-plane"}
{"id":21575,"comment_id":0,"comment":"\/\/ todo add more types","code":"private string gettypestring(int type, int length) { if (type == datatype.int.gettype()) { return \"int\"; } if (type == datatype.float.gettype()) { return \"float\"; } if (type == datatype.double.gettype()) { return \"double\"; } if (type == datatype.date.gettype()) { return \"date\"; } if (type == datatype.char.gettype()) { return \"char(\" + length + \")\"; } if (type == datatype.varchar.gettype()) { return \"varchar(\" + length + \")\"; } \/\/ todo add more types return null; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"return \"varchar(\" + length + \")\"; } \/\/ todo add more types return null; }","code_context_10":"} if (type == datatype.date.gettype()) { return \"date\"; } if (type == datatype.char.gettype()) { return \"char(\" + length + \")\"; } if (type == datatype.varchar.gettype()) { return \"varchar(\" + length + \")\"; } \/\/ todo add more types return null; }","code_context_20":"private string gettypestring(int type, int length) { if (type == datatype.int.gettype()) { return \"int\"; } if (type == datatype.float.gettype()) { return \"float\"; } if (type == datatype.double.gettype()) { return \"double\"; } if (type == datatype.date.gettype()) { return \"date\"; } if (type == datatype.char.gettype()) { return \"char(\" + length + \")\"; } if (type == datatype.varchar.gettype()) { return \"varchar(\" + length + \")\"; } \/\/ todo add more types return null; }","repo":"dbiir\/pard"}
{"id":13395,"comment_id":0,"comment":"\/\/ todo similar to equijoin, but not subtracting joincolumnslength","code":"public static int getpreopsoutputsize(component component, schema schema, tablealiasname tan) { if (component instanceof thetajoincomponent) throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan); else if (parents.length == 1) return getpreopsoutputsize(parents[0], schema, tan); else if (parents.length == 2) { final component firstparent = parents[0]; final component secondparent = parents[1]; final int joincolumnslength = firstparent.gethashindexes().size(); return getpreopsoutputsize(firstparent, schema, tan) + getpreopsoutputsize(secondparent, schema, tan) - joincolumnslength; } throw new runtimeexception(\"more than two parents for a component \" + component); }","classification":"DESIGN","isFinished":true,"code_context_2":"throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null)","code_context_10":"public static int getpreopsoutputsize(component component, schema schema, tablealiasname tan) { if (component instanceof thetajoincomponent) throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan); else if (parents.length == 1) return getpreopsoutputsize(parents[0], schema, tan); else if (parents.length == 2) { final component firstparent = parents[0]; final component secondparent = parents[1];","code_context_20":"public static int getpreopsoutputsize(component component, schema schema, tablealiasname tan) { if (component instanceof thetajoincomponent) throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan); else if (parents.length == 1) return getpreopsoutputsize(parents[0], schema, tan); else if (parents.length == 2) { final component firstparent = parents[0]; final component secondparent = parents[1]; final int joincolumnslength = firstparent.gethashindexes().size(); return getpreopsoutputsize(firstparent, schema, tan) + getpreopsoutputsize(secondparent, schema, tan) - joincolumnslength; } throw new runtimeexception(\"more than two parents for a component \" + component); }","repo":"epfldata\/squall"}
{"id":13395,"comment_id":1,"comment":"\/\/ this is a datasourcecomponent","code":"public static int getpreopsoutputsize(component component, schema schema, tablealiasname tan) { if (component instanceof thetajoincomponent) throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan); else if (parents.length == 1) return getpreopsoutputsize(parents[0], schema, tan); else if (parents.length == 2) { final component firstparent = parents[0]; final component secondparent = parents[1]; final int joincolumnslength = firstparent.gethashindexes().size(); return getpreopsoutputsize(firstparent, schema, tan) + getpreopsoutputsize(secondparent, schema, tan) - joincolumnslength; } throw new runtimeexception(\"more than two parents for a component \" + component); }","classification":"NONSATD","isFinished":true,"code_context_2":"final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan);","code_context_10":"public static int getpreopsoutputsize(component component, schema schema, tablealiasname tan) { if (component instanceof thetajoincomponent) throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan); else if (parents.length == 1) return getpreopsoutputsize(parents[0], schema, tan); else if (parents.length == 2) { final component firstparent = parents[0]; final component secondparent = parents[1]; final int joincolumnslength = firstparent.gethashindexes().size(); return getpreopsoutputsize(firstparent, schema, tan) + getpreopsoutputsize(secondparent, schema, tan)","code_context_20":"public static int getpreopsoutputsize(component component, schema schema, tablealiasname tan) { if (component instanceof thetajoincomponent) throw new runtimeexception( \"sql generator with theta does not work for now!\"); \/\/ todo similar to equijoin, but not subtracting joincolumnslength final component[] parents = component.getparents(); if (parents == null) \/\/ this is a datasourcecomponent return getpreopsoutputsize((datasourcecomponent) component, schema, tan); else if (parents.length == 1) return getpreopsoutputsize(parents[0], schema, tan); else if (parents.length == 2) { final component firstparent = parents[0]; final component secondparent = parents[1]; final int joincolumnslength = firstparent.gethashindexes().size(); return getpreopsoutputsize(firstparent, schema, tan) + getpreopsoutputsize(secondparent, schema, tan) - joincolumnslength; } throw new runtimeexception(\"more than two parents for a component \" + component); }","repo":"epfldata\/squall"}
{"id":21617,"comment_id":0,"comment":"\/\/ given","code":"@test @parameters({ \"clustername\", \"ambariuser\", \"ambaripassword\", \"emailneeded\", \"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","classification":"NONSATD","isFinished":true,"code_context_2":"@optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id);","code_context_10":"@test @parameters({ \"clustername\", \"ambariuser\", \"ambaripassword\", \"emailneeded\", \"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed","code_context_20":"@test @parameters({ \"clustername\", \"ambariuser\", \"ambaripassword\", \"emailneeded\", \"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","repo":"doktoric\/test123"}
{"id":21617,"comment_id":1,"comment":"\/\/ when \/\/ todo email needed","code":"@test @parameters({ \"clustername\", \"ambariuser\", \"ambaripassword\", \"emailneeded\", \"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","classification":"DESIGN","isFinished":true,"code_context_2":"itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map,","code_context_10":"@optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","code_context_20":"@test @parameters({ \"clustername\", \"ambariuser\", \"ambaripassword\", \"emailneeded\", \"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","repo":"doktoric\/test123"}
{"id":21617,"comment_id":2,"comment":"\/\/ then","code":"@test @parameters({ \"clustername\", \"ambariuser\", \"ambaripassword\", \"emailneeded\", \"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","classification":"NONSATD","isFinished":true,"code_context_2":"client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword);","code_context_10":"integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","code_context_20":"\"enablesecurity\", \"kerberosmasterkey\", \"kerberosadmin\", \"kerberospassword\", \"runrecipesonhosts\" }) public void testclustercreation(@optional(\"it-cluster\") string clustername, @optional(\"admin\") string ambariuser, @optional(\"admin123!@#\") string ambaripassword, @optional(\"false\") boolean emailneeded, @optional(\"false\") boolean enablesecurity, @optional string kerberosmasterkey, @optional string kerberosadmin, @optional string kerberospassword, @optional(\"\") string runrecipesonhosts) throws exception { \/\/ given integrationtestcontext itcontext = getitcontext(); string stackidstr = itcontext.getcontextparam(cloudbreakitcontextconstants.stack_id); integer stackid = integer.valueof(stackidstr); integer blueprintid = integer.valueof(itcontext.getcontextparam(cloudbreakitcontextconstants.blueprint_id)); list<hostgroup> hostgroups = itcontext.getcontextparam(cloudbreakitcontextconstants.hostgroup_id, list.class); list<map<string, object>> map = converthostgroups(hostgroups, runrecipesonhosts); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_user_id, ambariuser); itcontext.putcontextparam(cloudbreakitcontextconstants.ambari_password_id, ambaripassword); \/\/ when \/\/ todo email needed cloudbreakclient client = getclient(); client.postcluster(clustername, ambariuser, ambaripassword, blueprintid, \"cluster for integration test\", integer.valueof(stackid), map, enablesecurity, kerberosmasterkey, kerberosadmin, kerberospassword); \/\/ then cloudbreakutil.waitandcheckstackstatus(itcontext, stackidstr, \"available\"); cloudbreakutil.checkclusteravailability(client, stackidstr, ambariuser, ambaripassword); }","repo":"doktoric\/test123"}
{"id":13832,"comment_id":0,"comment":"\/\/ todo: use some sort of map instead","code":"private void reloaditemslist() { documentpartsdatasource.ensureconnectionisopen(); list<documentcollectionitem> documentcollectionitems = new arraylist<>(); log.d(\"doccollectionactivity\", \"checking filetypes to add\"); for (filetype filetype : sectionsprovider.getfiletypesforsectionmodel(sectionmodel)) { log.d(\"doccollectionactivity\", \"checking filetype: \" + filetype); \/\/ todo: use some sort of map instead boolean ispresent = false; for (documentpart documentpart : documentpartsdatasource.getdocumentpartsforsection(sectionmodel)) { if (documentpart.gettype().equals(filetype)) { documentcollectionitem docdata = new documentcollectionitem(getstring(filetype.getnameresourceid()), getstring(r.string.document_collection_list_item_present_title), filetype, true); documentcollectionitems.add(docdata); ispresent = true; break; } } if (!ispresent) { log.d(\"doccollectionactivity\", \"file type not found in documentparts, add as to-do\"); documentcollectionitem docdata = new documentcollectionitem(getstring(filetype.getnameresourceid()), getstring(r.string.document_collection_list_item_not_present_title), filetype, false); documentcollectionitems.add(docdata); } } listview = (listview) findviewbyid(r.id.documentlist); documentcollectionviewadapter listviewadapter = new documentcollectionviewadapter(this, r.layout.fragment_document, documentcollectionitems); listview.setadapter(listviewadapter); }","classification":"DESIGN","isFinished":true,"code_context_2":"for (filetype filetype : sectionsprovider.getfiletypesforsectionmodel(sectionmodel)) { log.d(\"doccollectionactivity\", \"checking filetype: \" + filetype); \/\/ todo: use some sort of map instead boolean ispresent = false; for (documentpart documentpart : documentpartsdatasource.getdocumentpartsforsection(sectionmodel)) {","code_context_10":"private void reloaditemslist() { documentpartsdatasource.ensureconnectionisopen(); list<documentcollectionitem> documentcollectionitems = new arraylist<>(); log.d(\"doccollectionactivity\", \"checking filetypes to add\"); for (filetype filetype : sectionsprovider.getfiletypesforsectionmodel(sectionmodel)) { log.d(\"doccollectionactivity\", \"checking filetype: \" + filetype); \/\/ todo: use some sort of map instead boolean ispresent = false; for (documentpart documentpart : documentpartsdatasource.getdocumentpartsforsection(sectionmodel)) { if (documentpart.gettype().equals(filetype)) { documentcollectionitem docdata = new documentcollectionitem(getstring(filetype.getnameresourceid()), getstring(r.string.document_collection_list_item_present_title), filetype, true); documentcollectionitems.add(docdata); ispresent = true; break; } } if (!ispresent) {","code_context_20":"private void reloaditemslist() { documentpartsdatasource.ensureconnectionisopen(); list<documentcollectionitem> documentcollectionitems = new arraylist<>(); log.d(\"doccollectionactivity\", \"checking filetypes to add\"); for (filetype filetype : sectionsprovider.getfiletypesforsectionmodel(sectionmodel)) { log.d(\"doccollectionactivity\", \"checking filetype: \" + filetype); \/\/ todo: use some sort of map instead boolean ispresent = false; for (documentpart documentpart : documentpartsdatasource.getdocumentpartsforsection(sectionmodel)) { if (documentpart.gettype().equals(filetype)) { documentcollectionitem docdata = new documentcollectionitem(getstring(filetype.getnameresourceid()), getstring(r.string.document_collection_list_item_present_title), filetype, true); documentcollectionitems.add(docdata); ispresent = true; break; } } if (!ispresent) { log.d(\"doccollectionactivity\", \"file type not found in documentparts, add as to-do\"); documentcollectionitem docdata = new documentcollectionitem(getstring(filetype.getnameresourceid()), getstring(r.string.document_collection_list_item_not_present_title), filetype, false); documentcollectionitems.add(docdata); } } listview = (listview) findviewbyid(r.id.documentlist); documentcollectionviewadapter listviewadapter = new documentcollectionviewadapter(this, r.layout.fragment_document, documentcollectionitems); listview.setadapter(listviewadapter); }","repo":"devolksbank\/NL-Help-U"}
{"id":22238,"comment_id":0,"comment":"\/\/ todo: support for tabs?","code":"public static string replaceleadingspaceswithnbsps (string str) { \/\/ todo: support for tabs? matcher m = _leadingspacespattern.matcher(str); stringbuffer buf = new stringbuffer(); while (m.find()) { int spacecount = m.group(1).length(); m.appendreplacement(buf, awutil.repeatedstring(\"&nbsp;\", spacecount)); } m.appendtail(buf); return buf.tostring(); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"public static string replaceleadingspaceswithnbsps (string str) { \/\/ todo: support for tabs? matcher m = _leadingspacespattern.matcher(str); stringbuffer buf = new stringbuffer();","code_context_10":"public static string replaceleadingspaceswithnbsps (string str) { \/\/ todo: support for tabs? matcher m = _leadingspacespattern.matcher(str); stringbuffer buf = new stringbuffer(); while (m.find()) { int spacecount = m.group(1).length(); m.appendreplacement(buf, awutil.repeatedstring(\"&nbsp;\", spacecount)); } m.appendtail(buf); return buf.tostring(); }","code_context_20":"public static string replaceleadingspaceswithnbsps (string str) { \/\/ todo: support for tabs? matcher m = _leadingspacespattern.matcher(str); stringbuffer buf = new stringbuffer(); while (m.find()) { int spacecount = m.group(1).length(); m.appendreplacement(buf, awutil.repeatedstring(\"&nbsp;\", spacecount)); } m.appendtail(buf); return buf.tostring(); }","repo":"fbarthez\/aribaweb"}
{"id":14274,"comment_id":0,"comment":"\/* * \/\/ note: this is wrong ? even though the ibis has left, the * ibisidentifier may still be floating around in the system... we should * just have some timeout on the cache entries instead... * * public void left(ibis.ipl.ibisidentifier id) { super.left(id); * synchronized(addresses) { addresses.remove(id); } } * * public void died(ibis.ipl.ibisidentifier id) { super.died(id); * synchronized(addresses) { addresses.remove(id); } } *\/","code":"\/* * \/\/ note: this is wrong ? even though the ibis has left, the * ibisidentifier may still be floating around in the system... we should * just have some timeout on the cache entries instead... * * public void left(ibis.ipl.ibisidentifier id) { super.left(id); * synchronized(addresses) { addresses.remove(id); } } * * public void died(ibis.ipl.ibisidentifier id) { super.died(id); * synchronized(addresses) { addresses.remove(id); } } *\/ ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","classification":"DEFECT","isFinished":true,"code_context_2":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","code_context_10":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","code_context_20":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","repo":"dadepo\/ipl"}
{"id":14274,"comment_id":1,"comment":"\/\/ read receiveport type from input, to produce a \/\/ better error message.","code":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","classification":"NONSATD","isFinished":true,"code_context_2":"rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in);","code_context_10":"sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; }","code_context_20":"int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception(","repo":"dadepo\/ipl"}
{"id":14274,"comment_id":2,"comment":"\/\/ and try again if we did not reach the timeout...","code":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","classification":"NONSATD","isFinished":true,"code_context_2":"case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) {","code_context_10":"+ message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\");","code_context_20":"+ \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) {","repo":"dadepo\/ipl"}
{"id":14274,"comment_id":3,"comment":"\/\/ ignored","code":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","classification":"NONSATD","isFinished":true,"code_context_2":"} } catch (throwable e) { \/\/ ignored } try {","code_context_10":"} } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100);","code_context_20":"if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","repo":"dadepo\/ipl"}
{"id":14274,"comment_id":4,"comment":"\/\/ ignored","code":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","classification":"NONSATD","isFinished":true,"code_context_2":"} } catch (throwable e) { \/\/ ignored } try {","code_context_10":"} } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100);","code_context_20":"if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","repo":"dadepo\/ipl"}
{"id":14274,"comment_id":5,"comment":"\/\/ ignore","code":"ibissocket connect(tcpsendport sp, ibis.ipl.impl.receiveportidentifier rip, int timeout, boolean filltimeout) throws ioexception { ibisidentifier id = (ibisidentifier) rip.ibisidentifier(); string name = rip.name(); ibissocketaddress idaddr; synchronized (addresses) { idaddr = addresses.get(id); if (idaddr == null) { idaddr = new ibissocketaddress(id.getimplementationdata()); addresses.put(id, idaddr); } } long starttime = system.currenttimemillis(); if (logger.isdebugenabled()) { logger.debug(\"--> creating socket for connection to \" + name + \" at \" + idaddr); } porttype sendporttype = sp.getporttype(); do { dataoutputstream out = null; ibissocket s = null; int result = -1; sp.printmanagementproperties(system.out); try { s = factory.createclientsocket(idaddr, timeout, filltimeout, sp.managementproperties()); s.settcpnodelay(true); out = new dataoutputstream(new bufferedarrayoutputstream( s.getoutputstream())); out.writeutf(name); sp.getident().writeto(out); sendporttype.writeto(out); out.flush(); result = s.getinputstream().read(); switch (result) { case receiveport.accepted: return s; case receiveport.already_connected: throw new alreadyconnectedexception(\"already connected\", rip); case receiveport.type_mismatch: \/\/ read receiveport type from input, to produce a \/\/ better error message. datainputstream in = new datainputstream(s.getinputstream()); porttype rtp = new porttype(in); capabilityset s1 = rtp.unmatchedcapabilities(sendporttype); capabilityset s2 = sendporttype.unmatchedcapabilities(rtp); string message = \"\"; if (s1.size() != 0) { message = message + \"\\nunmatched receiveport capabilities: \" + s1.tostring() + \".\"; } if (s2.size() != 0) { message = message + \"\\nunmatched sendport capabilities: \" + s2.tostring() + \".\"; } throw new portmismatchexception( \"cannot connect ports of different port types.\" + message, rip); case receiveport.denied: throw new connectionrefusedexception( \"receiver denied connection\", rip); case receiveport.no_many_to_x: throw new connectionrefusedexception( \"receiver already has a connection and neither manytoone not manytomany \" + \"is set\", rip); case receiveport.not_present: case receiveport.disabled: \/\/ and try again if we did not reach the timeout... if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","classification":"NONSATD","isFinished":true,"code_context_2":"} } catch (throwable e) { \/\/ ignored } try {","code_context_10":"} } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100);","code_context_20":"if (timeout > 0 && system.currenttimemillis() > starttime + timeout) { throw new connectiontimedoutexception( \"could not connect\", rip); } break; case -1: throw new ioexception(\"encountered eof in tcpibis.connect\"); default: throw new ioexception(\"illegal opcode in tcpibis.connect\"); } } catch (sockettimeoutexception e) { throw new connectiontimedoutexception(\"could not connect\", rip); } finally { if (result != receiveport.accepted) { try { if (out != null) { out.close(); } } catch (throwable e) { \/\/ ignored } try { s.close(); } catch (throwable e) { \/\/ ignored } } } try { thread.sleep(100); } catch (interruptedexception e) { \/\/ ignore } } while (true); }","repo":"dadepo\/ipl"}
{"id":22476,"comment_id":0,"comment":"\/\/ todo: i18n: externalize. \/\/ todo: enhance: print link to updated shop.","code":"@allowedmethod @transactional(transactiontype.write) public string update() { log.debug(\"---------------- update()\"); saveorupdate(); \/\/ todo: i18n: externalize. \/\/ todo: enhance: print link to updated shop. addactionmessage(\"shop <strong>\" + escape(shop.getname()) + \"<\/strong> updated successfully.\"); return result_redirect_view; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"log.debug(\"---------------- update()\"); saveorupdate(); \/\/ todo: i18n: externalize. \/\/ todo: enhance: print link to updated shop. addactionmessage(\"shop <strong>\" + escape(shop.getname()) + \"<\/strong> updated successfully.\"); return result_redirect_view;","code_context_10":"@allowedmethod @transactional(transactiontype.write) public string update() { log.debug(\"---------------- update()\"); saveorupdate(); \/\/ todo: i18n: externalize. \/\/ todo: enhance: print link to updated shop. addactionmessage(\"shop <strong>\" + escape(shop.getname()) + \"<\/strong> updated successfully.\"); return result_redirect_view; }","code_context_20":"@allowedmethod @transactional(transactiontype.write) public string update() { log.debug(\"---------------- update()\"); saveorupdate(); \/\/ todo: i18n: externalize. \/\/ todo: enhance: print link to updated shop. addactionmessage(\"shop <strong>\" + escape(shop.getname()) + \"<\/strong> updated successfully.\"); return result_redirect_view; }","repo":"doanhoa93\/struts2shop"}
{"id":30694,"comment_id":0,"comment":"\/\/ todo: do not use until it is able to choose the best cardinality","code":"\/\/ todo: do not use until it is able to choose the best cardinality public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","classification":"DESIGN","isFinished":true,"code_context_2":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","code_context_10":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","code_context_20":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","repo":"fernandoj92\/mvca-parkinson"}
{"id":30694,"comment_id":1,"comment":"\/\/ seleccionamos el directorio en el que se van a recoger todos los","code":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\"));","code_context_10":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\");","code_context_20":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname());","repo":"fernandoj92\/mvca-parkinson"}
{"id":30694,"comment_id":2,"comment":"\/\/create the discretedataset","code":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\");","code_context_10":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm);","code_context_20":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","repo":"fernandoj92\/mvca-parkinson"}
{"id":30694,"comment_id":3,"comment":"\/\/ learn the ltm","code":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format","code_context_10":"string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } }","code_context_20":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","repo":"fernandoj92\/mvca-parkinson"}
{"id":30694,"comment_id":4,"comment":"\/\/ save it in bif format","code":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm);","code_context_10":"try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","code_context_20":"public static void learnandsaveallmodels(){ \/\/ seleccionamos el directorio en el que se van a recoger todos los string input_path = \"data\/automatic_learn\/\"; file[] inputfiles = new file(input_path).listfiles(x -> x.getname().endswith(\".arff\")); string output_path = \"results\/automatic_learn\/lcm\/\"; for (file inputfile : inputfiles) { try { if (inputfile.isfile()) { \/\/create the discretedataset discretedataset data = new discretedataset(datafileloader.loaddata(input_path + inputfile.getname(), discretevariable.class)); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"------------------------------------------------------------------------------\"); system.out.println(\"############## \"+ data.getname() + \" ############## \\n\"); \/\/ learn the ltm ltm ltm = learnbestlcmvaryingcardinality(data); \/\/ save it in bif format newbifwriter writer = new newbifwriter(new fileoutputstream(output_path + \"lcm_\" + filenameutils.removeextension(inputfile.getname()) + \".bif\"), false); writer.write(ltm); } }catch(exception e){ system.out.println(\"error with \" + inputfile.getname()); e.printstacktrace(); } } }","repo":"fernandoj92\/mvca-parkinson"}
{"id":22578,"comment_id":0,"comment":"\/** sends the level finish packet. *\/","code":"\/** sends the level finish packet. *\/ public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","code_context_10":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","code_context_20":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","repo":"derekdinan\/CTFServer"}
{"id":22578,"comment_id":1,"comment":"\/\/ for thread safety","code":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr =","code_context_10":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation();","code_context_20":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(),","repo":"derekdinan\/CTFServer"}
{"id":22578,"comment_id":2,"comment":"\/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level)","code":"public void sendlevelfinish() { taskqueue.gettaskqueue() .push( new task() { public void execute() { try { \/\/ for thread safety final level level = world.getworld().getlevel(); packetbuilder bldr = new packetbuilder( persistingpacketmanager.getpacketmanager().getoutgoingpacket(4)); bldr.putshort(\"width\", level.getwidth()); bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer()));","code_context_10":"session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","code_context_20":"bldr.putshort(\"height\", level.getheight()); bldr.putshort(\"depth\", level.getdepth()); session.send(bldr.topacket()); position spawn = level.getspawnposition(); rotation r = level.getspawnrotation(); sendspawn( (byte) -1, session.getplayer().nameid, session.getplayer().getcoloredname(), session.getplayer().getteamname(), session.getplayer().getname(), session.getplayer().getlistname(), session.getplayer().getskinurl(), spawn.getx(), spawn.gety(), spawn.getz(), (byte) r.getrotation(), (byte) r.getlook(), false, true); \/\/ now load the player's game (todo in the future do this in parallel with loading \/\/ the \/\/ level) savedgamemanager.getsavedgamemanager() .queuepersistencerequest(new loadpersistencerequest(session.getplayer())); session.setready(); world.getworld().completeregistration(session); } catch (exception ex) { server.log(ex); } } }); }","repo":"derekdinan\/CTFServer"}
{"id":30787,"comment_id":0,"comment":"\/\/--- drk > note: assuming here that caja.load creates frames in order of calls...if not, this method could be trouble.","code":"\/\/--- drk > note: assuming here that caja.load creates frames in order of calls...if not, this method could be trouble. private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","classification":"DEFECT","isFinished":true,"code_context_2":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","code_context_10":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","code_context_20":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","repo":"dougkoellmer\/swarm"}
{"id":30787,"comment_id":1,"comment":"\/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject);","code":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","classification":"DESIGN","isFinished":true,"code_context_2":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","code_context_10":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","code_context_20":"private void onframeload(javascriptobject cajaframeobject) { \/\/--- this is to catch the theoretical case where we call start and stop then start really fast, \/\/--- but the load call from the first start didn't finish before the first stop, and comes in between \/\/--- the first stop and the second start...note this is just done based on my thoughts on possible fringe \/\/--- cases, not because i actually noticed this behavior. \/\/this.stop(); \/\/m_cajaframeobjects.push(cajaframeobject); }","repo":"dougkoellmer\/swarm"}
{"id":14459,"comment_id":0,"comment":"\/** * metodo encargado de asignar un\/a enfermero\/a a un registro de vacunacion * * este metodo hace uso del metodo searchforvaccinationregister() para buscar el registro * y a continuacion asignar el\/la enfermero\/a * * @param patient el paciente al cual se le quiere asignar el\/la enfermero\/a * @param nurse el\/la enfermero\/a a asignar *\/","code":"\/** * metodo encargado de asignar un\/a enfermero\/a a un registro de vacunacion * * este metodo hace uso del metodo searchforvaccinationregister() para buscar el registro * y a continuacion asignar el\/la enfermero\/a * * @param patient el paciente al cual se le quiere asignar el\/la enfermero\/a * @param nurse el\/la enfermero\/a a asignar *\/ public void assignnursetoregister(patient patient, nurse nurse) { vaccinationregister registertoassingnurse = searchforvaccinationregister(patient); registertoassingnurse.assignnurse(nurse); }","classification":"NONSATD","isFinished":true,"code_context_2":"public void assignnursetoregister(patient patient, nurse nurse) { vaccinationregister registertoassingnurse = searchforvaccinationregister(patient); registertoassingnurse.assignnurse(nurse); }","code_context_10":"public void assignnursetoregister(patient patient, nurse nurse) { vaccinationregister registertoassingnurse = searchforvaccinationregister(patient); registertoassingnurse.assignnurse(nurse); }","code_context_20":"public void assignnursetoregister(patient patient, nurse nurse) { vaccinationregister registertoassingnurse = searchforvaccinationregister(patient); registertoassingnurse.assignnurse(nurse); }","repo":"deividgdt\/UNED_II"}
{"id":22775,"comment_id":0,"comment":"\/** * find the edges and return as a list of points. the method uses a * dfs search through all points in the image with values > 0 to link * adjacent sequential points into edges. * the method then uses method mergeadjacentendpoints. * note that the later 2 methods are not needed if the edges will be used * in a corner detector only, but if the edges are to be used to * find inflection points in scale space maps, those methods help to * provide more complete shapes and better matches between the same object * in another scale space map. * * @return *\/","code":"\/** * find the edges and return as a list of points. the method uses a * dfs search through all points in the image with values > 0 to link * adjacent sequential points into edges. * the method then uses method mergeadjacentendpoints. * note that the later 2 methods are not needed if the edges will be used * in a corner detector only, but if the edges are to be used to * find inflection points in scale space maps, those methods help to * provide more complete shapes and better matches between the same object * in another scale space map. * * @return *\/ public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","code_context_10":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","code_context_20":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":1,"comment":"\/\/ dfs search for sequential neighbors.","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1;","code_context_10":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } }","code_context_20":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx();","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":2,"comment":"\/\/for now, choosing to look only at the blue","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) {","code_context_10":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>();","code_context_20":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) {","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":3,"comment":"\/\/ for each neighbor v of u","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) {","code_context_10":"long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue;","code_context_20":"for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) {","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":4,"comment":"\/\/ if u is not in an edge already, create a new one","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray();","code_context_10":"if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1;","code_context_20":"boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":5,"comment":"\/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1;","code_context_10":"if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide","code_context_20":"continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100;","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":6,"comment":"\/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"DESIGN","isFinished":true,"code_context_2":"appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true;","code_context_10":"\/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges","code_context_20":"if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size()));","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":7,"comment":"\/\/ count the number of points in edges","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine,","code_context_10":"\/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size()));","code_context_20":"int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size();","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":8,"comment":"\/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not","code_context_10":"long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())});","code_context_20":"stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)});","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":9,"comment":"\/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not.","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"} *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\");","code_context_10":"lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)});","code_context_20":"log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":10,"comment":"\/\/todo: this may need to change","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"DESIGN","isFinished":true,"code_context_2":"log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size();","code_context_10":"\/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output);","code_context_20":"lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22775,"comment_id":11,"comment":"\/\/prunespurs(output);","code":"public list<pairintarray> findedges() { \/\/ dfs search for sequential neighbors. stack<pairint> stack = new stack<pairint>(); int thresh0 = 1; for (int i = 0; i < img.getwidth(); i++) { for (int j = 0; j < img.getheight(); j++) { \/\/for now, choosing to look only at the blue int bpix = img.getvalue(i, j); if (bpix >= thresh0) { stack.add(new pairint(i, j)); } } } numberofpixelsabovethreshold = stack.size(); log.log(level.fine, \"number of pixels that exceed threshhold={0}\", long.tostring(numberofpixelsabovethreshold)); list<pairintarray> output = new arraylist<pairintarray>(); int[] unodeedgeidx = new int[img.getwidth() * img.getheight()]; arrays.fill(unodeedgeidx, -1); while (!stack.isempty()) { pairint unode = stack.pop(); int ux = unode.getx(); int uy = unode.gety(); int uidx = (uy * img.getwidth()) + ux; boolean foundneighbor = false; \/\/ for each neighbor v of u for (int vx = (ux - 1); vx < (ux + 2); vx++) { if (foundneighbor) { break; } if (vx < 0 || vx > (img.getwidth() - 1)) { continue; } for (int vy = (uy - 1); vy < (uy + 2); vy++) { if (vy < 0 || vy > (img.getheight() - 1)) { continue; } int vidx = (vy * img.getwidth()) + vx; if (unodeedgeidx[vidx] != -1 || (uidx == vidx)) { continue; } if (img.getvalue(vx, vy) < thresh0) { continue; } \/\/ if u is not in an edge already, create a new one if (unodeedgeidx[uidx] == -1) { pairintarray edge = new pairintarray(); edge.add(ux, uy); unodeedgeidx[uidx] = output.size(); output.add(edge); } \/\/ keep the curve points ordered \/\/ add v to the edge u if u is the last node in it's edge pairintarray appendtonode = output.get(unodeedgeidx[uidx]); int aidx = appendtonode.getn() - 1; if ((appendtonode.getx(aidx) != ux) || (appendtonode.gety(aidx) != uy)) { continue; } appendtonode.add(vx, vy); unodeedgeidx[vidx] = unodeedgeidx[uidx]; \/\/todo: do we only want 1 neighbor from the 9 as a continuation? \/\/ yes for now, but this requires edges be only 1 pixel wide \/\/ inserting back at the top of the stack assures that the \/\/ search continues next from an associated point stack.add(new pairint(vx, vy)); foundneighbor = true; break; } } } log.fine(output.size() + \" edges after dfs\"); int nitermax = 100; int n, sz, lastsize; \/\/ count the number of points in edges long sum = countpixelsinedges(output); log.log(level.fine, \"==> {0} pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum), long.tostring(numberofpixelsabovethreshold)}); log.log(level.fine, \"there are {0} edges\", integer.tostring(output.size())); output = mergeadjacentendpoints(output); log.log(level.fine, \"{0} edges after merge adjacent\", integer.tostring(output.size())); \/* \/\/not necessary now that lines from cannyedgefilter are 1 pix width. miscellaneouscurvehelper ch = new miscellaneouscurvehelper(); n = 0; sz = output.size(); lastsize = integer.max_value; while ((sz < lastsize) && (n < nitermax)) { lastsize = sz; output = ch.pruneandincludeadjacentcurves(output, img.getwidth()); sz = output.size(); log.log(level.fine, \"{0}) {1} edges after prune overlapping\", new object[]{integer.tostring(n), integer.tostring(sz)}); n++; } *\/ \/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","classification":"NONSATD","isFinished":true,"code_context_2":"new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output;","code_context_10":"\/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","code_context_20":"\/\/ this helps to merge edges (that is extracted curves) at adjacent \/\/ points that resemble an intersection of the lines, but it's not \/\/ necessarily useful if only interested in corners and not inflection \/\/ points because the curvature is determined correctly \/\/ whether the curves are merged or not. output = connectclosestpointsifcantrim(output); log.fine(output.size() + \" edges after connect closest\"); output = fillingaps(output); log.log(level.fine, \"{0} edges after fill in gaps\", new object[]{integer.tostring(output.size())}); \/\/todo: this may need to change removeedgesshorterthan(output, edgesizelowerlimit); sz = output.size(); log.log(level.fine, \"{0} edges after removing those shorter\", new object[]{integer.tostring(sz)}); long sum2 = countpixelsinedges(output); log.log(level.fine, \"==> {0}) pixels are in edges out of {1} pixels > threshhold\", new object[]{long.tostring(sum2), long.tostring(numberofpixelsabovethreshold)}); \/\/prunespurs(output); adjustedgestowardsbrightpixels(output); return output; }","repo":"dukson\/curvature-scale-space-corners-and-transformations"}
{"id":22779,"comment_id":0,"comment":"\/\/todo refactor logic \/\/select direction","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"DESIGN","isFinished":true,"code_context_2":"int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\");","code_context_10":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: {","code_context_20":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right;","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":1,"comment":"\/\/start on the left side","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: {","code_context_10":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break;","code_context_20":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) {","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":2,"comment":"\/\/start on the right side","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: {","code_context_10":"} break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: {","code_context_20":"if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) {","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":3,"comment":"\/\/move x","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) {","code_context_10":"} break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot));","code_context_20":"} if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) {","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":4,"comment":"\/\/select direction again","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end","code_context_10":"} break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); }","code_context_20":"this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); }","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":5,"comment":"\/\/if start above end","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: {","code_context_10":"} abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; }","code_context_20":"break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: {","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":6,"comment":"\/\/ abstractdirection = abstractrobot.direction.down;","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) {","code_context_10":"switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done","code_context_20":"abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); }","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":7,"comment":"\/\/if start below end","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"} if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: {","code_context_10":"} break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; }","code_context_20":"system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":8,"comment":"\/\/done","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break;","code_context_10":"\/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); }","code_context_20":"switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":9,"comment":"\/\/ abstractdirection = abstractrobot.direction.up;","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y","code_context_10":"switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","code_context_20":"break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","repo":"defoltbmd\/SimpleRobot"}
{"id":22779,"comment_id":10,"comment":"\/\/move y","code":"public void fillpath(directionrobot robot, int endx, int endy) { int width = robot.getx() - endx; int height = robot.gety() - endy; \/\/todo refactor logic \/\/select direction abstractrobot.direction abstractdirection = robot.getdirection(); system.out.println(\"first!\"); if (width < 0) { \/\/start on the left side switch (abstractdirection) { case up: { this.addcomand(new rotaterightcomand(robot)); } break; case left: { this.addcomand(new rotaterightcomand(robot)); this.addcomand(new rotaterightcomand(robot)); } break; case down: { this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.right; } if (width > 0) { \/\/start on the right side switch (abstractdirection) { case up: { this.addcomand(new rotateleftcomand(robot)); } break; case down: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); this.addcomand(new rotateleftcomand(robot)); } break; } abstractdirection = abstractrobot.direction.left; } \/\/move x system.out.println(width); for (int i = 0; i < abs(width); i++) { this.addcomand(new moveforwardcomand(robot)); } \/\/select direction again if (height < 0) { \/\/if start above end switch (abstractdirection) { case left: { this.addcomand(new rotateleftcomand(robot)); } break; case right: { this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) {","code_context_10":"this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","code_context_20":"this.addcomand(new rotaterightcomand(robot)); } break; } \/\/ abstractdirection = abstractrobot.direction.down; } if (height > 0) { \/\/if start below end switch (abstractdirection) { case left: { this.addcomand(new rotaterightcomand(robot)); } break; case right: { this.addcomand(new rotateleftcomand(robot)); \/\/done } break; } \/\/ abstractdirection = abstractrobot.direction.up; } \/\/move y system.out.println(height); for (int i = 0; i < abs(height); i++) { this.addcomand(new moveforwardcomand(robot)); } }","repo":"defoltbmd\/SimpleRobot"}
{"id":14669,"comment_id":0,"comment":"\/* sync the (now obsolete) policy fields with the * jscrollpane. *\/","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"NONSATD","isFinished":true,"code_context_2":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy();","code_context_10":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/","code_context_20":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); }","repo":"dannyhx\/artisynth_core"}
{"id":14669,"comment_id":1,"comment":"\/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"NONSATD","isFinished":true,"code_context_2":"int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null;","code_context_10":"{ \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); }","code_context_20":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) {","repo":"dannyhx\/artisynth_core"}
{"id":14669,"comment_id":2,"comment":"\/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize();","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"DEFECT","isFinished":true,"code_context_2":"if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview();","code_context_10":"int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets.","code_context_20":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/","repo":"dannyhx\/artisynth_core"}
{"id":14669,"comment_id":3,"comment":"\/* if there's a viewport add its preferredsize. *\/","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"NONSATD","isFinished":true,"code_context_2":"view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width;","code_context_10":"dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right;","code_context_20":"jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height;","repo":"dannyhx\/artisynth_core"}
{"id":14669,"comment_id":4,"comment":"\/* if there's a jscrollpane.viewportborder, add its insets. *\/","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"NONSATD","isFinished":true,"code_context_2":"prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) {","code_context_10":"\/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) {","code_context_20":"\/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth","repo":"dannyhx\/artisynth_core"}
{"id":14669,"comment_id":5,"comment":"\/* if a header exists and it's visible, factor its * preferred size in. *\/","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"NONSATD","isFinished":true,"code_context_2":"prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width;","code_context_10":"prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: *","code_context_20":"extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the","repo":"dannyhx\/artisynth_core"}
{"id":14669,"comment_id":6,"comment":"\/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/","code":"public dimension preferredlayoutsize(container parent) { \/* sync the (now obsolete) policy fields with the * jscrollpane. *\/ jscrollpane scrollpane = (jscrollpane)parent; vsbpolicy = scrollpane.getverticalscrollbarpolicy(); hsbpolicy = scrollpane.gethorizontalscrollbarpolicy(); insets insets = parent.getinsets(); int prefwidth = insets.left + insets.right; int prefheight = insets.top + insets.bottom; \/* note that viewport.getviewsize() is equivalent to * viewport.getview().getpreferredsize() modulo a null * view or a view whose size was explicitly set. *\/ dimension extentsize = null; dimension viewsize = null; component view = null; if (viewport != null) { extentsize = viewport.getpreferredsize(); \/\/bug fix: always use the preferred size for the client. \/\/viewsize = viewport.getviewsize(); viewsize = viewport.getview().getpreferredsize(); view = viewport.getview(); } \/* if there's a viewport add its preferredsize. *\/ if (extentsize != null) { prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportwidth(); } if (canscroll && (viewsize.width > extentsize.width)) { prefheight += hsb.getpreferredsize().height; } } } dimension dim = new dimension(prefwidth, prefheight); return dim; }","classification":"NONSATD","isFinished":true,"code_context_2":"prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) {","code_context_10":"} \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) {","code_context_20":"prefwidth += extentsize.width; prefheight += extentsize.height; } \/* if there's a jscrollpane.viewportborder, add its insets. *\/ border viewportborder = scrollpane.getviewportborder(); if (viewportborder != null) { insets vpbinsets = viewportborder.getborderinsets(parent); prefwidth += vpbinsets.left + vpbinsets.right; prefheight += vpbinsets.top + vpbinsets.bottom; } \/* if a header exists and it's visible, factor its * preferred size in. *\/ if ((rowhead != null) && rowhead.isvisible()) { prefwidth += rowhead.getpreferredsize().width; } if ((colhead != null) && colhead.isvisible()) { prefheight += colhead.getpreferredsize().height; } \/* if a scrollbar is going to appear, factor its preferred size in. * if the scrollbars policy is as_needed, this can be a little * tricky: * * - if the view is a scrollable then scrollabletracksviewportwidth * and scrollabletracksviewportheight can be used to effectively * disable scrolling (if they're true) in their respective dimensions. * * - assuming that a scrollbar hasn't been disabled by the * previous constraint, we need to decide if the scrollbar is going * to appear to correctly compute the jscrollpanes preferred size. * to do this we compare the preferredsize of the viewport (the * extentsize) to the preferredsize of the view. although we're * not responsible for laying out the view we'll assume that the * jviewport will always give it its preferredsize. *\/ if ((vsb != null) && (vsbpolicy != vertical_scrollbar_never)) { if (vsbpolicy == vertical_scrollbar_always) { prefwidth += vsb.getpreferredsize().width; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true; if (view instanceof scrollable) { canscroll = !((scrollable)view).getscrollabletracksviewportheight(); } if (canscroll && (viewsize.height > extentsize.height)) { prefwidth += vsb.getpreferredsize().width; } } } if ((hsb != null) && (hsbpolicy != horizontal_scrollbar_never)) { if (hsbpolicy == horizontal_scrollbar_always) { prefheight += hsb.getpreferredsize().height; } else if ((viewsize != null) && (extentsize != null)) { boolean canscroll = true;","repo":"dannyhx\/artisynth_core"}
{"id":31217,"comment_id":0,"comment":"\/\/ todo subscribe to network events instead of using this","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"DESIGN","isFinished":true,"code_context_2":"return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext(","code_context_10":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly","code_context_20":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":1,"comment":"\/\/ todo remove this but with caution","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file","code_context_10":"if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server","code_context_20":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() {","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":2,"comment":"\/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"TEST","isFinished":true,"code_context_2":"\/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs =","code_context_10":"} \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server","code_context_20":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress();","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":3,"comment":"\/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\";","code_context_10":"toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() {","code_context_20":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } }","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":4,"comment":"\/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() {","code_context_10":"\/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace();","code_context_20":"\/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver);","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":5,"comment":"\/\/ create a keystore containing our trusted cas","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype);","code_context_10":"server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\");","code_context_20":"} else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) {","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":6,"comment":"\/\/ create a trustmanager that trusts the cas in our keystore","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm);","code_context_10":"certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace();","code_context_20":"} catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try {","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":7,"comment":"\/\/ create an sslcontext that uses our trustmanager","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null);","code_context_10":"} \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace();","code_context_20":"if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput);","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":8,"comment":"\/\/ create a keystore containing our trusted cas","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype);","code_context_10":"server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\");","code_context_20":"} else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) {","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":9,"comment":"\/\/ create a trustmanager that trusts the cas in our keystore","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm);","code_context_10":"certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace();","code_context_20":"} catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try {","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":10,"comment":"\/\/ create an sslcontext that uses our trustmanager","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null);","code_context_10":"} \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace();","code_context_20":"if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput);","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":11,"comment":"\/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false;","code_context_10":"} catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone","code_context_20":"\/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); }","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":12,"comment":"\/\/ generate or retrieve an id for this phone","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"\"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) {","code_context_10":"\/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); }","code_context_20":"e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit();","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":13,"comment":"\/\/ to get historycount","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone","code_context_10":"sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else {","code_context_20":"analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000);","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":14,"comment":"\/\/ generate or retrieve an historycount for this phone","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) {","code_context_10":"editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); }","code_context_20":"results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace();","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":15,"comment":"\/\/ check if retrieve historycount succeeded","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception();","code_context_10":"boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); }","code_context_20":"editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":16,"comment":"\/\/ make sure server is initialized!","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try {","code_context_10":"editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\");","code_context_20":"a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) {","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":17,"comment":"\/\/ find a way to switch to no wait","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"} }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try {","code_context_10":"} } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) {","code_context_20":"\/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null;","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":18,"comment":"\/\/ set the app to run test for","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app","code_context_10":"try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) {","code_context_20":"config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":19,"comment":"\/\/ run the test on this.app","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"NONSATD","isFinished":true,"code_context_2":"\/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); }","code_context_10":"} catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults();","code_context_20":"config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","repo":"dng24\/wehe-android"}
{"id":31217,"comment_id":20,"comment":"\/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking","code":"@override protected void doinbackground(string... args) { if (iscancelled()) { return null; } \/\/ todo subscribe to network events instead of using this if (!networkavailable) { toast.maketext( context, getstring(r.string.toast_nonetwork), toast.length_long).show(); } else { \/\/ todo remove this but with caution updateuibean = new updateuibean(); \/\/ todo to test different configs change the files poiting by config_file \/\/ avoid changing the code directly config.readconfigfile(replayconstants.config_file, context); sharedpreferences sharedprefs = preferencemanager.getdefaultsharedpreferences(context); try { \/\/ metadata here is user's network type device used geolocation if permitted etc \/\/ google storage forbids to store user related data \/\/ so we send that data to a private server server = sharedprefs.getstring(\"pref_server\", \"wehe2.meddle.mobi\"); metadataserver = \"wehe-metadata.meddle.mobi\"; \/\/ we first resolve the ip of the server and then communicate with the server \/\/ using ip only, because we have multiple server under same domain and we want \/\/ the client not to switch server during a test run \/\/ todo come up with a better way to handle inet related queries, since this \/\/ introduced inefficiency final inetaddress[] address = {null, null}; new thread() { public void run() { while (!(address[0] instanceof inet4address || address[0] instanceof inet6address)) { try { server = inetaddress.getbyname(server).gethostaddress(); address[0] = inetaddress.getbyname(server); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); new thread() { public void run() { while (!(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { metadataserver = inetaddress.getbyname(metadataserver).gethostaddress(); address[1] = inetaddress.getbyname(metadataserver); } catch (unknownhostexception e) { log.w(\"getreplayserverip\", \"get ip of replay server failed!\"); e.printstacktrace(); } } } }.start(); int maxwaittime = 5000; int currentwaittime = 500; while (!(address[0] instanceof inet4address || address[0] instanceof inet6address) && !(address[1] instanceof inet4address || address[1] instanceof inet6address)) { try { if (currentwaittime <= maxwaittime) { thread.sleep(currentwaittime); currentwaittime += 500; } else { toast.maketext(context, r.string.server_unavailable, toast.length_long).show(); return null; } } catch (interruptedexception e) { e.printstacktrace(); } } if (address[0] instanceof inet6address) server = \"[\" + server + \"]\"; if (address[1] instanceof inet6address) metadataserver = \"[\" + metadataserver + \"]\"; try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.main)) { ca = cf.generatecertificate(cainput); system.out.println(\"main=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"main\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); sslsocketfactory = context.getsocketfactory(); hostnameverifier = (hostname, session) -> true; } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } try { certificatefactory cf = certificatefactory.getinstance(\"x.509\"); certificate ca; try (inputstream cainput = getresources().openrawresource(r.raw.metadata)) { ca = cf.generatecertificate(cainput); system.out.println(\"metadata=\" + ((x509certificate) ca).getissuerdn()); } \/\/ create a keystore containing our trusted cas string keystoretype = keystore.getdefaulttype(); keystore keystore = keystore.getinstance(keystoretype); keystore.load(null, null); keystore.setcertificateentry(\"metadata\", ca); \/\/ create a trustmanager that trusts the cas in our keystore string tmfalgorithm = trustmanagerfactory.getdefaultalgorithm(); trustmanagerfactory tmf = trustmanagerfactory.getinstance(tmfalgorithm); tmf.init(keystore); \/\/ create an sslcontext that uses our trustmanager sslcontext context = sslcontext.getinstance(\"tls\"); context.init(null, tmf.gettrustmanagers(), null); metadatasocketfactory = context.getsocketfactory(); } catch (certificateexception e) { e.printstacktrace(); } catch (nosuchalgorithmexception e) { e.printstacktrace(); } catch (keystoreexception e) { e.printstacktrace(); } catch (keymanagementexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } log.d(\"getreplayserverip\", \"server ip: \" + server); } catch (nullpointerexception e) { e.printstacktrace(); log.w(\"getreplayserverip\", \"invalid ip address!\"); } \/\/ extract data that was sent by previous activity. in our case, list of \/\/ apps, server and timing enabletiming = \"true\"; dotest = false; int port = integer.valueof(config.get(\"result_port\")); analyzerserverurl = (\"https:\/\/\" + server + \":\" + port + \"\/results\"); date = new date(); results = new jsonarray(); log.d(\"result channel\", \"path: \" + server + \" port: \" + port); confirmationreplays = sharedprefs.getboolean(\"confirmationreplays\", true); \/\/ generate or retrieve an id for this phone boolean hasid = sharedprefs.getboolean(\"hasid\", false); if (!hasid) { randomid = new randomstring(10).nextstring(); sharedpreferences.editor editor = sharedprefs.edit(); editor.putboolean(\"hasid\", true); editor.putstring(\"id\", randomid); editor.apply(); } else { randomid = sharedprefs.getstring(\"id\", null); } a_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_area\", \"10\"))); ks2pvalue_threshold = integer.parseint(objects.requirenonnull(sharedprefs.getstring(\"pref_threshold_ks2p\", \"5\"))); confirmationreplays = sharedprefs.getboolean(\"pref_multiple_tests\", true); \/\/ to get historycount settings = getsharedpreferences(status, context.mode_private); \/\/ generate or retrieve an historycount for this phone boolean hashistorycount = settings.getboolean(\"hashistorycount\", false); if (!hashistorycount) { historycount = 0; sharedpreferences.editor editor = settings.edit(); editor.putboolean(\"hashistorycount\", true); editor.putint(\"historycount\", historycount); editor.apply(); } else { historycount = settings.getint(\"historycount\", -1); } \/\/ check if retrieve historycount succeeded if (historycount == -1) throw new runtimeexception(); config.set(\"timing\", enabletiming); \/\/ make sure server is initialized! while (server == null) { try { thread.sleep(1000); } catch (interruptedexception e) { e.printstacktrace(); } } config.set(\"server\", server); config.set(\"jitter\", \"true\"); config.set(\"publicip\", \"\"); new thread(new runnable() { public void run() { config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","classification":"DESIGN","isFinished":true,"code_context_2":"runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null;","code_context_10":"} log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; }","code_context_20":"config.set(\"publicip\", getpublicip(\"80\")); } }).start(); \/\/ find a way to switch to no wait while (config.get(\"publicip\").equals(\"\")) { try { thread.sleep(500); } catch (interruptedexception e) { e.printstacktrace(); } } log.d(\"replay\", \"public ip: \" + config.get(\"publicip\")); } for (applicationbean app : selectedapps) { rerun = false; \/\/ set the app to run test for this.app = app; \/\/ run the test on this.app runtest(); } \/\/ keep checking if the user exited from replayactivity or not \/\/ todo find a better way stop the tests immediately without continues checking if (iscancelled()) { return null; } if (results.length() > 0) { log.d(\"result channel\", \"storing results\"); saveresults(); } if (iscancelled()) { return null; } showfinishdialog(); log.d(\"result channel\", \"exiting normally\"); return null; }","repo":"dng24\/wehe-android"}
{"id":23038,"comment_id":0,"comment":"\/\/ we have more space than necessary; resize to give each at least \/\/ preferred size","code":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } }","classification":"NONSATD","isFinished":true,"code_context_2":"int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph","code_context_10":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width;","code_context_20":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more }","repo":"g-pechorin\/flexdock"}
{"id":23038,"comment_id":1,"comment":"\/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more","code":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"+ ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize();","code_context_10":"int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5)));","code_context_20":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } }","repo":"g-pechorin\/flexdock"}
{"id":23038,"comment_id":2,"comment":"\/\/ we have more space than necessary; resize to give each at least \/\/ preferred size","code":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } }","classification":"NONSATD","isFinished":true,"code_context_2":"int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph","code_context_10":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width;","code_context_20":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more }","repo":"g-pechorin\/flexdock"}
{"id":23038,"comment_id":3,"comment":"\/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more","code":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"+ ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize();","code_context_10":"int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5)));","code_context_20":"@override public void resettopreferredsizes() { insets i = getinsets(); if (getorientation() == vertical_split) { int h = getheight() - i.top - i.bottom - getdividersize(); int toph = gettopcomponent().getpreferredsize().height; int bottomh = getbottomcomponent().getpreferredsize().height; int extraspace = h - toph - bottomh; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.top + toph + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } else { int w = getwidth() - i.left - i.right - getdividersize(); int lefth = getleftcomponent().getpreferredsize().width; int righth = getrightcomponent().getpreferredsize().width; int extraspace = w - lefth - righth; \/\/ we have more space than necessary; resize to give each at least \/\/ preferred size if (extraspace >= 0) { setdividerlocation(i.left + lefth + ((int) (extraspace * getresizeweight() + .5))); } \/\/ todo implement shrinking excess space to ensure that one has \/\/ preferred and nothing more } }","repo":"g-pechorin\/flexdock"}
{"id":23040,"comment_id":0,"comment":"\/\/ update each of the widgets with the remote adapter","code":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]); appwidgetmanager.updateappwidget(appwidgetids[i], layout); } super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","classification":"NONSATD","isFinished":true,"code_context_2":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]);","code_context_10":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]); appwidgetmanager.updateappwidget(appwidgetids[i], layout); } super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","code_context_20":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]); appwidgetmanager.updateappwidget(appwidgetids[i], layout); } super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","repo":"ekux44\/LampShade"}
{"id":23040,"comment_id":1,"comment":"\/\/ wtf another google bug?","code":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]); appwidgetmanager.updateappwidget(appwidgetids[i], layout); } super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","classification":"DEFECT","isFinished":true,"code_context_2":"} super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","code_context_10":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]); appwidgetmanager.updateappwidget(appwidgetids[i], layout); } super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","code_context_20":"@override public void onupdate(context context, appwidgetmanager appwidgetmanager, int[] appwidgetids) { \/\/ update each of the widgets with the remote adapter for (int i = 0; i < appwidgetids.length; ++i) { remoteviews layout = buildlayout(context, appwidgetids[i]); appwidgetmanager.updateappwidget(appwidgetids[i], layout); } super.onupdate(context, appwidgetmanager, appwidgetids); \/\/ wtf another google bug? this.onenabled(context); }","repo":"ekux44\/LampShade"}
{"id":14870,"comment_id":0,"comment":"\/** * handles changes to the apn database. *\/","code":"\/** * handles changes to the apn database. *\/ private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","classification":"NONSATD","isFinished":true,"code_context_2":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","code_context_10":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","code_context_20":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","repo":"efortuna\/AndroidSDKClone"}
{"id":14870,"comment_id":1,"comment":"\/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd","code":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","classification":"NONSATD","isFinished":true,"code_context_2":"overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); }","code_context_10":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed);","code_context_20":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","repo":"efortuna\/AndroidSDKClone"}
{"id":14870,"comment_id":2,"comment":"\/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator.","code":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","classification":"DESIGN","isFinished":true,"code_context_2":"((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist();","code_context_10":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","code_context_20":"private void onapnchanged() { dctconstants.state overallstate = getoverallstate(); boolean isdisconnected = (overallstate == dctconstants.state.idle || overallstate == dctconstants.state.failed); if (mphone instanceof gsmphone) { \/\/ the \"current\" may no longer be valid. mms depends on this to send properly. tbd ((gsmphone)mphone).updatecurrentcarrierinprovider(); } \/\/ todo: it'd be nice to only do this if the changed entrie(s) \/\/ match the current operator. if (dbg) log(\"onapnchanged: createallapnlist and cleanupallconnections\"); createallapnlist(); setinitialattachapn(); cleanupallconnections(!isdisconnected, phone.reason_apn_changed); if (isdisconnected) { setupdataonconnectableapns(phone.reason_apn_changed); } }","repo":"efortuna\/AndroidSDKClone"}
{"id":14914,"comment_id":0,"comment":"\/\/ type is a normal class.","code":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; } else if (type instanceof wildcardtype) { return getrawtype(((wildcardtype)type).getupperbounds()[0]); } else { string classname = type == null ? \"null\" : type.getclass().getname(); throw new illegalargumentexception(\"expected a class, parameterizedtype, or \" + \"genericarraytype, but <\" + type + \"> is of type \" + classname); } }","classification":"NONSATD","isFinished":true,"code_context_2":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; }","code_context_10":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype;","code_context_20":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; }","repo":"foolite\/panda"}
{"id":14914,"comment_id":1,"comment":"\/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists.","code":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; } else if (type instanceof wildcardtype) { return getrawtype(((wildcardtype)type).getupperbounds()[0]); } else { string classname = type == null ? \"null\" : type.getclass().getname(); throw new illegalargumentexception(\"expected a class, parameterizedtype, or \" + \"genericarraytype, but <\" + type + \"> is of type \" + classname); } }","classification":"NONSATD","isFinished":true,"code_context_2":"else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class);","code_context_10":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple.","code_context_20":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; } else if (type instanceof wildcardtype) { return getrawtype(((wildcardtype)type).getupperbounds()[0]); } else { string classname = type == null ? \"null\" : type.getclass().getname(); throw new illegalargumentexception(\"expected a class, parameterizedtype, or \" + \"genericarraytype, but <\" + type + \"> is of type \" + classname);","repo":"foolite\/panda"}
{"id":14914,"comment_id":2,"comment":"\/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay","code":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; } else if (type instanceof wildcardtype) { return getrawtype(((wildcardtype)type).getupperbounds()[0]); } else { string classname = type == null ? \"null\" : type.getclass().getname(); throw new illegalargumentexception(\"expected a class, parameterizedtype, or \" + \"genericarraytype, but <\" + type + \"> is of type \" + classname); } }","classification":"DESIGN","isFinished":true,"code_context_2":"} else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; }","code_context_10":"\/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; } else if (type instanceof wildcardtype) { return getrawtype(((wildcardtype)type).getupperbounds()[0]); } else { string classname = type == null ? \"null\" : type.getclass().getname(); throw new illegalargumentexception(\"expected a class, parameterizedtype, or \" + \"genericarraytype, but <\" + type + \"> is of type \" + classname); }","code_context_20":"public static class<?> getrawtype(type type) { if (type instanceof class<?>) { \/\/ type is a normal class. return (class<?>)type; } else if (type instanceof parameterizedtype) { parameterizedtype parameterizedtype = (parameterizedtype)type; \/\/ i'm not exactly sure why getrawtype() returns type instead of class. \/\/ neal isn't either but suspects some pathological case related \/\/ to nested classes exists. type rawtype = parameterizedtype.getrawtype(); asserts.istrue(rawtype instanceof class); return (class<?>)rawtype; } else if (type instanceof genericarraytype) { type componenttype = ((genericarraytype)type).getgenericcomponenttype(); return array.newinstance(getrawtype(componenttype), 0).getclass(); } else if (type instanceof typevariable) { \/\/ we could use the variable's bounds, but that won't work if there are multiple. \/\/ having a raw type that's more general than necessary is okay return object.class; } else if (type instanceof wildcardtype) { return getrawtype(((wildcardtype)type).getupperbounds()[0]); } else { string classname = type == null ? \"null\" : type.getclass().getname(); throw new illegalargumentexception(\"expected a class, parameterizedtype, or \" + \"genericarraytype, but <\" + type + \"> is of type \" + classname); } }","repo":"foolite\/panda"}
{"id":31546,"comment_id":0,"comment":"\/\/ empty the database table before downloading up-to-date content","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used)","code_context_10":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) {","code_context_20":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes);","repo":"elimu-ai\/content-provider"}
{"id":31546,"comment_id":1,"comment":"\/\/ todo: also delete corresponding audio files (only those that are no longer used)","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"\/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid());","code_context_10":"log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication();","code_context_20":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e);","repo":"elimu-ai\/content-provider"}
{"id":31546,"comment_id":2,"comment":"\/\/ check if the corresponding audio file has already been downloaded","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile);","code_context_10":"public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length);","code_context_20":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists());","repo":"elimu-ai\/content-provider"}
{"id":31546,"comment_id":3,"comment":"\/\/ download file bytes","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl();","code_context_10":"audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) {","code_context_20":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); }","repo":"elimu-ai\/content-provider"}
{"id":31546,"comment_id":4,"comment":"\/\/ store the downloaded file in the external storage directory","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile);","code_context_10":"file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); }","code_context_20":"log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show();","repo":"elimu-ai\/content-provider"}
{"id":31546,"comment_id":5,"comment":"\/\/ store the audio in the database","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid());","code_context_10":"try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone);","code_context_20":"log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","repo":"elimu-ai\/content-provider"}
{"id":31546,"comment_id":6,"comment":"\/\/ update the ui","code":"private void processresponsebody(list<audiogson> audiogsons) { log.i(getclass().getname(), \"processresponsebody\"); executorservice executorservice = executors.newsinglethreadexecutor(); executorservice.execute(new runnable() { @override public void run() { log.i(getclass().getname(), \"run\"); roomdb roomdb = roomdb.getdatabase(getcontext()); audiodao audiodao = roomdb.audiodao(); \/\/ empty the database table before downloading up-to-date content audiodao.deleteall(); \/\/ todo: also delete corresponding audio files (only those that are no longer used) for (audiogson audiogson : audiogsons) { log.i(getclass().getname(), \"audiogson.getid(): \" + audiogson.getid()); audio audio = gsontoroomconverter.getaudio(audiogson); \/\/ check if the corresponding audio file has already been downloaded file audiofile = filehelper.getaudiofile(audiogson, getcontext()); log.i(getclass().getname(), \"audiofile: \" + audiofile); log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); if (!audiofile.exists()) { \/\/ download file bytes baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","classification":"NONSATD","isFinished":true,"code_context_2":"log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size());","code_context_10":"log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","code_context_20":"baseapplication baseapplication = (baseapplication) getactivity().getapplication(); string downloadurl = baseapplication.getbaseurl() + audiogson.getbytesurl(); log.i(getclass().getname(), \"downloadurl: \" + downloadurl); byte[] bytes = multimediadownloader.downloadfilebytes(downloadurl); log.i(getclass().getname(), \"bytes.length: \" + bytes.length); \/\/ store the downloaded file in the external storage directory try { fileoutputstream fileoutputstream = new fileoutputstream(audiofile); fileoutputstream.write(bytes); } catch (filenotfoundexception e) { log.e(getclass().getname(), null, e); } catch (ioexception e) { log.e(getclass().getname(), null, e); } log.i(getclass().getname(), \"audiofile.exists(): \" + audiofile.exists()); } \/\/ store the audio in the database audiodao.insert(audio); log.i(getclass().getname(), \"stored audio in database with id \" + audio.getid()); } \/\/ update the ui list<audio> audios = audiodao.loadall(); log.i(getclass().getname(), \"audios.size(): \" + audios.size()); getactivity().runonuithread(() -> { textview.settext(\"audios.size(): \" + audios.size()); snackbar.make(textview, \"audios.size(): \" + audios.size(), snackbar.length_long).show(); progressbar.setvisibility(view.gone); }); } }); }","repo":"elimu-ai\/content-provider"}
{"id":15539,"comment_id":0,"comment":"\/** todo: generated by frameweb. should be documented. *\/","code":"\/** todo: generated by frameweb. should be documented. *\/ @override protected entitymanager getentitymanager() { return entitymanager; }","classification":"DOCUMENTATION","isFinished":true,"code_context_2":"@override protected entitymanager getentitymanager() { return entitymanager; }","code_context_10":"@override protected entitymanager getentitymanager() { return entitymanager; }","code_context_20":"@override protected entitymanager getentitymanager() { return entitymanager; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":15540,"comment_id":0,"comment":"\/** todo: generated by frameweb. should be documented. *\/","code":"\/** todo: generated by frameweb. should be documented. *\/ @override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","classification":"DOCUMENTATION","isFinished":true,"code_context_2":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","code_context_10":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","code_context_20":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":15540,"comment_id":1,"comment":"\/\/ fixme: auto-generated method stub","code":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","classification":"DESIGN","isFinished":true,"code_context_2":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","code_context_10":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","code_context_20":"@override public list<book> getbookbytitle(string title) { \/\/ fixme: auto-generated method stub return null; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":15541,"comment_id":0,"comment":"\/** todo: generated by frameweb. should be documented. *\/","code":"\/** todo: generated by frameweb. should be documented. *\/ @override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","classification":"DOCUMENTATION","isFinished":true,"code_context_2":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","code_context_10":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","code_context_20":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":15541,"comment_id":1,"comment":"\/\/ fixme: auto-generated method stub","code":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","code_context_10":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","code_context_20":"@override public list<book> getbookbyauthor(string author) { \/\/ fixme: auto-generated method stub return null; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":15542,"comment_id":0,"comment":"\/** todo: generated by frameweb. should be documented. *\/","code":"\/** todo: generated by frameweb. should be documented. *\/ @override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","classification":"DOCUMENTATION","isFinished":true,"code_context_2":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","code_context_10":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","code_context_20":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":15542,"comment_id":1,"comment":"\/\/ fixme: auto-generated method stub","code":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","code_context_10":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","code_context_20":"@override public list<book> getbooklist() { \/\/ fixme: auto-generated method stub return null; }","repo":"dwws-ufes\/2020-doeLivro"}
{"id":31991,"comment_id":0,"comment":"\/\/## todo: maybe support multiple files here?","code":"public static void dogoto( string strqualifedtype ) { editorutilities.showwaitcursor( true ); try { itype type = typesystem.getbyfullnameifvalid( strqualifedtype ); if( type == null ) { return; } ifile[] sourcefiles = type.getsourcefiles(); if( sourcefiles != null && sourcefiles.length > 0 ) { \/\/## todo: maybe support multiple files here? ifile sourcefile = sourcefiles[0]; labframe.instance().openfile( pathutil.create( sourcefile.touri() ) ); } } catch( exception e ) { throw new runtimeexception( e ); } finally { editorutilities.showwaitcursor( false ); } }","classification":"DESIGN","isFinished":true,"code_context_2":"if( sourcefiles != null && sourcefiles.length > 0 ) { \/\/## todo: maybe support multiple files here? ifile sourcefile = sourcefiles[0]; labframe.instance().openfile( pathutil.create( sourcefile.touri() ) );","code_context_10":"try { itype type = typesystem.getbyfullnameifvalid( strqualifedtype ); if( type == null ) { return; } ifile[] sourcefiles = type.getsourcefiles(); if( sourcefiles != null && sourcefiles.length > 0 ) { \/\/## todo: maybe support multiple files here? ifile sourcefile = sourcefiles[0]; labframe.instance().openfile( pathutil.create( sourcefile.touri() ) ); } } catch( exception e ) { throw new runtimeexception( e ); } finally {","code_context_20":"public static void dogoto( string strqualifedtype ) { editorutilities.showwaitcursor( true ); try { itype type = typesystem.getbyfullnameifvalid( strqualifedtype ); if( type == null ) { return; } ifile[] sourcefiles = type.getsourcefiles(); if( sourcefiles != null && sourcefiles.length > 0 ) { \/\/## todo: maybe support multiple files here? ifile sourcefile = sourcefiles[0]; labframe.instance().openfile( pathutil.create( sourcefile.touri() ) ); } } catch( exception e ) { throw new runtimeexception( e ); } finally { editorutilities.showwaitcursor( false ); } }","repo":"dmcreyno\/gosu-lang"}
{"id":15967,"comment_id":0,"comment":"\/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call.","code":"public table<locator, string, string> getmetadatavalues(set<locator> locators) { columnfamily cf = cassandramodel.cf_metric_metadata; boolean isbatch = locators.size() > 1; table<locator, string, string> metatable = hashbasedtable.create(); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, string>> query = keyspace .preparequery(cf) .getkeyslice(locators) .execute(); for (row<locator, string> row : query.getresult()) { columnlist<string> columns = row.getcolumns(); for (column<string> column : columns) { string metavalue = column.getvalue(stringmetadataserializer.get()); string metakey = column.getname(); metatable.put(row.getkey(), metakey, metavalue); } } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return metatable; }","classification":"DESIGN","isFinished":true,"code_context_2":"timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, string>> query = keyspace .preparequery(cf)","code_context_10":"public table<locator, string, string> getmetadatavalues(set<locator> locators) { columnfamily cf = cassandramodel.cf_metric_metadata; boolean isbatch = locators.size() > 1; table<locator, string, string> metatable = hashbasedtable.create(); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, string>> query = keyspace .preparequery(cf) .getkeyslice(locators) .execute(); for (row<locator, string> row : query.getresult()) { columnlist<string> columns = row.getcolumns(); for (column<string> column : columns) { string metavalue = column.getvalue(stringmetadataserializer.get()); string metakey = column.getname(); metatable.put(row.getkey(), metakey, metavalue);","code_context_20":"public table<locator, string, string> getmetadatavalues(set<locator> locators) { columnfamily cf = cassandramodel.cf_metric_metadata; boolean isbatch = locators.size() > 1; table<locator, string, string> metatable = hashbasedtable.create(); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, string>> query = keyspace .preparequery(cf) .getkeyslice(locators) .execute(); for (row<locator, string> row : query.getresult()) { columnlist<string> columns = row.getcolumns(); for (column<string> column : columns) { string metavalue = column.getvalue(stringmetadataserializer.get()); string metakey = column.getname(); metatable.put(row.getkey(), metakey, metavalue); } } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e);","repo":"dlobue\/blueflood"}
{"id":15967,"comment_id":1,"comment":"\/\/ todo: not really sure what happens when one of the keys is not found.","code":"public table<locator, string, string> getmetadatavalues(set<locator> locators) { columnfamily cf = cassandramodel.cf_metric_metadata; boolean isbatch = locators.size() > 1; table<locator, string, string> metatable = hashbasedtable.create(); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, string>> query = keyspace .preparequery(cf) .getkeyslice(locators) .execute(); for (row<locator, string> row : query.getresult()) { columnlist<string> columns = row.getcolumns(); for (column<string> column : columns) { string metavalue = column.getvalue(stringmetadataserializer.get()); string metakey = column.getname(); metatable.put(row.getkey(), metakey, metavalue); } } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return metatable; }","classification":"DESIGN","isFinished":true,"code_context_2":"} } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else {","code_context_10":".execute(); for (row<locator, string> row : query.getresult()) { columnlist<string> columns = row.getcolumns(); for (column<string> column : columns) { string metavalue = column.getvalue(stringmetadataserializer.get()); string metakey = column.getname(); metatable.put(row.getkey(), metakey, metavalue); } } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return metatable;","code_context_20":"columnfamily cf = cassandramodel.cf_metric_metadata; boolean isbatch = locators.size() > 1; table<locator, string, string> metatable = hashbasedtable.create(); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, string>> query = keyspace .preparequery(cf) .getkeyslice(locators) .execute(); for (row<locator, string> row : query.getresult()) { columnlist<string> columns = row.getcolumns(); for (column<string> column : columns) { string metavalue = column.getvalue(stringmetadataserializer.get()); string metakey = column.getname(); metatable.put(row.getkey(), metakey, metavalue); } } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return metatable; }","repo":"dlobue\/blueflood"}
{"id":15968,"comment_id":0,"comment":"\/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call.","code":"private map<locator, columnlist<long>> getcolumnsfromdb(list<locator> locators, columnfamily<locator, long> cf, range range) { if (range.getstart() > range.getstop()) { throw new runtimeexception(string.format(\"invalid rollup range: \", range.tostring())); } boolean isbatch = locators.size() != 1; final map<locator, columnlist<long>> columns = new hashmap<locator, columnlist<long>>(); final rangebuilder rangebuilder = new rangebuilder().setstart(range.getstart()).setend(range.getstop()); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf) .getkeyslice(locators) .withcolumnrange(rangebuilder.build()) .execute(); for (row<locator, long> row : query.getresult()) { columns.put(row.getkey(), row.getcolumns()); } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return columns; }","classification":"DESIGN","isFinished":true,"code_context_2":"timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf)","code_context_10":"private map<locator, columnlist<long>> getcolumnsfromdb(list<locator> locators, columnfamily<locator, long> cf, range range) { if (range.getstart() > range.getstop()) { throw new runtimeexception(string.format(\"invalid rollup range: \", range.tostring())); } boolean isbatch = locators.size() != 1; final map<locator, columnlist<long>> columns = new hashmap<locator, columnlist<long>>(); final rangebuilder rangebuilder = new rangebuilder().setstart(range.getstart()).setend(range.getstop()); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf) .getkeyslice(locators) .withcolumnrange(rangebuilder.build()) .execute(); for (row<locator, long> row : query.getresult()) { columns.put(row.getkey(), row.getcolumns()); } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found.","code_context_20":"private map<locator, columnlist<long>> getcolumnsfromdb(list<locator> locators, columnfamily<locator, long> cf, range range) { if (range.getstart() > range.getstop()) { throw new runtimeexception(string.format(\"invalid rollup range: \", range.tostring())); } boolean isbatch = locators.size() != 1; final map<locator, columnlist<long>> columns = new hashmap<locator, columnlist<long>>(); final rangebuilder rangebuilder = new rangebuilder().setstart(range.getstart()).setend(range.getstop()); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf) .getkeyslice(locators) .withcolumnrange(rangebuilder.build()) .execute(); for (row<locator, long> row : query.getresult()) { columns.put(row.getkey(), row.getcolumns()); } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return columns;","repo":"dlobue\/blueflood"}
{"id":15968,"comment_id":1,"comment":"\/\/ todo: not really sure what happens when one of the keys is not found.","code":"private map<locator, columnlist<long>> getcolumnsfromdb(list<locator> locators, columnfamily<locator, long> cf, range range) { if (range.getstart() > range.getstop()) { throw new runtimeexception(string.format(\"invalid rollup range: \", range.tostring())); } boolean isbatch = locators.size() != 1; final map<locator, columnlist<long>> columns = new hashmap<locator, columnlist<long>>(); final rangebuilder rangebuilder = new rangebuilder().setstart(range.getstart()).setend(range.getstop()); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf) .getkeyslice(locators) .withcolumnrange(rangebuilder.build()) .execute(); for (row<locator, long> row : query.getresult()) { columns.put(row.getkey(), row.getcolumns()); } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return columns; }","classification":"DESIGN","isFinished":true,"code_context_2":"} } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else {","code_context_10":"\/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf) .getkeyslice(locators) .withcolumnrange(rangebuilder.build()) .execute(); for (row<locator, long> row : query.getresult()) { columns.put(row.getkey(), row.getcolumns()); } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return columns;","code_context_20":"range range) { if (range.getstart() > range.getstop()) { throw new runtimeexception(string.format(\"invalid rollup range: \", range.tostring())); } boolean isbatch = locators.size() != 1; final map<locator, columnlist<long>> columns = new hashmap<locator, columnlist<long>>(); final rangebuilder rangebuilder = new rangebuilder().setstart(range.getstart()).setend(range.getstop()); timer.context ctx = isbatch ? instrumentation.getbatchreadtimercontext(cf) : instrumentation.getreadtimercontext(cf); try { \/\/ we don't paginate this call. so we should make sure the number of reads is tolerable. \/\/ todo: think about paginating this call. operationresult<rows<locator, long>> query = keyspace .preparequery(cf) .getkeyslice(locators) .withcolumnrange(rangebuilder.build()) .execute(); for (row<locator, long> row : query.getresult()) { columns.put(row.getkey(), row.getcolumns()); } } catch (connectionexception e) { if (e instanceof notfoundexception) { \/\/ todo: not really sure what happens when one of the keys is not found. instrumentation.marknotfound(cf); } else { if (isbatch) { instrumentation.markbatchreaderror(e); } else { instrumentation.markreaderror(e); } } log.warn((isbatch ? \"batch \" : \"\") + \" read query failed for column family \" + cf.getname(), e); } finally { ctx.stop(); } return columns; }","repo":"dlobue\/blueflood"}
{"id":16078,"comment_id":0,"comment":"\/\/ to do: code goes here.","code":"void setupdatadialog_componentadded(java.awt.event.containerevent event) { \/\/ to do: code goes here. }","classification":"IMPLEMENTATION","isFinished":true,"code_context_2":"void setupdatadialog_componentadded(java.awt.event.containerevent event) { \/\/ to do: code goes here. }","code_context_10":"void setupdatadialog_componentadded(java.awt.event.containerevent event) { \/\/ to do: code goes here. }","code_context_20":"void setupdatadialog_componentadded(java.awt.event.containerevent event) { \/\/ to do: code goes here. }","repo":"fluffynukeit\/mdsplus"}
{"id":7897,"comment_id":0,"comment":"\/\/ todo auto-generated catch block","code":"private set<string> getspammyitems() { set<string> spammyitems = null; try { spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems; }","classification":"DESIGN","isFinished":true,"code_context_2":"spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent.","code_context_10":"private set<string> getspammyitems() { set<string> spammyitems = null; try { spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems; }","code_context_20":"private set<string> getspammyitems() { set<string> spammyitems = null; try { spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems; }","repo":"edmazur\/everquest-robot-stanvern"}
{"id":7897,"comment_id":1,"comment":"\/\/ todo: do something intelligent.","code":"private set<string> getspammyitems() { set<string> spammyitems = null; try { spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems; }","classification":"DESIGN","isFinished":true,"code_context_2":"\/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems;","code_context_10":"private set<string> getspammyitems() { set<string> spammyitems = null; try { spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems; }","code_context_20":"private set<string> getspammyitems() { set<string> spammyitems = null; try { spammyitems = new hashset<string>(files.readalllines(spammy_item_file.topath())); } catch (ioexception e) { \/\/ todo auto-generated catch block e.printstacktrace(); \/\/ todo: do something intelligent. } return spammyitems; }","repo":"edmazur\/everquest-robot-stanvern"}
{"id":7896,"comment_id":0,"comment":"\/\/ todo: do something intelligent.","code":"public void initialize() { payloadtriebuilder<item> itemsbynamebuilder = payloadtrie.builder(); itemsbynamebuilder .ignorecase() .ignoreoverlaps(); spammyitems = getspammyitems(); bufferedreader bufferedreader; try { bufferedreader = new bufferedreader(new filereader(item_file)); string line = null; while ((line = bufferedreader.readline()) != null) { string[] parts = line.split(\"\\t\"); string name = parts[0]; string url = parts[1]; item item = new item(name, url); itemsbynamebuilder.addkeyword(normalize(name), item); } } catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } itemsbyname = itemsbynamebuilder.build(); }","classification":"DESIGN","isFinished":true,"code_context_2":"} catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace();","code_context_10":"string line = null; while ((line = bufferedreader.readline()) != null) { string[] parts = line.split(\"\\t\"); string name = parts[0]; string url = parts[1]; item item = new item(name, url); itemsbynamebuilder.addkeyword(normalize(name), item); } } catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } itemsbyname = itemsbynamebuilder.build(); }","code_context_20":"public void initialize() { payloadtriebuilder<item> itemsbynamebuilder = payloadtrie.builder(); itemsbynamebuilder .ignorecase() .ignoreoverlaps(); spammyitems = getspammyitems(); bufferedreader bufferedreader; try { bufferedreader = new bufferedreader(new filereader(item_file)); string line = null; while ((line = bufferedreader.readline()) != null) { string[] parts = line.split(\"\\t\"); string name = parts[0]; string url = parts[1]; item item = new item(name, url); itemsbynamebuilder.addkeyword(normalize(name), item); } } catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } itemsbyname = itemsbynamebuilder.build(); }","repo":"edmazur\/everquest-robot-stanvern"}
{"id":7896,"comment_id":1,"comment":"\/\/ todo: do something intelligent.","code":"public void initialize() { payloadtriebuilder<item> itemsbynamebuilder = payloadtrie.builder(); itemsbynamebuilder .ignorecase() .ignoreoverlaps(); spammyitems = getspammyitems(); bufferedreader bufferedreader; try { bufferedreader = new bufferedreader(new filereader(item_file)); string line = null; while ((line = bufferedreader.readline()) != null) { string[] parts = line.split(\"\\t\"); string name = parts[0]; string url = parts[1]; item item = new item(name, url); itemsbynamebuilder.addkeyword(normalize(name), item); } } catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } itemsbyname = itemsbynamebuilder.build(); }","classification":"DESIGN","isFinished":true,"code_context_2":"} catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace();","code_context_10":"string line = null; while ((line = bufferedreader.readline()) != null) { string[] parts = line.split(\"\\t\"); string name = parts[0]; string url = parts[1]; item item = new item(name, url); itemsbynamebuilder.addkeyword(normalize(name), item); } } catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } itemsbyname = itemsbynamebuilder.build(); }","code_context_20":"public void initialize() { payloadtriebuilder<item> itemsbynamebuilder = payloadtrie.builder(); itemsbynamebuilder .ignorecase() .ignoreoverlaps(); spammyitems = getspammyitems(); bufferedreader bufferedreader; try { bufferedreader = new bufferedreader(new filereader(item_file)); string line = null; while ((line = bufferedreader.readline()) != null) { string[] parts = line.split(\"\\t\"); string name = parts[0]; string url = parts[1]; item item = new item(name, url); itemsbynamebuilder.addkeyword(normalize(name), item); } } catch (filenotfoundexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } catch (ioexception e) { e.printstacktrace(); \/\/ todo: do something intelligent. } itemsbyname = itemsbynamebuilder.build(); }","repo":"edmazur\/everquest-robot-stanvern"}
{"id":24462,"comment_id":0,"comment":"\/\/ fixme [vistall] support other borders?","code":"private void borderschanged() { jcomponent component = (jcomponent)toawtcomponent(); component.setborder(jbui.borders.empty()); collection<borderinfo> borders = dataobject().getborders(); map<borderposition, integer> emptyborders = new linkedhashmap<>(); for (borderinfo border : borders) { if (border.getborderstyle() == borderstyle.empty) { emptyborders.put(border.getborderposition(), border.getwidth()); } } if (!emptyborders.isempty()) { component.setborder(jbui.borders.empty(getbordersize(emptyborders, borderposition.top), getbordersize(emptyborders, borderposition.left), getbordersize(emptyborders, borderposition.bottom), getbordersize(emptyborders, borderposition.right))); return; } \/\/ fixme [vistall] support other borders? }","classification":"DEFECT","isFinished":true,"code_context_2":"return; } \/\/ fixme [vistall] support other borders? }","code_context_10":"for (borderinfo border : borders) { if (border.getborderstyle() == borderstyle.empty) { emptyborders.put(border.getborderposition(), border.getwidth()); } } if (!emptyborders.isempty()) { component.setborder(jbui.borders.empty(getbordersize(emptyborders, borderposition.top), getbordersize(emptyborders, borderposition.left), getbordersize(emptyborders, borderposition.bottom), getbordersize(emptyborders, borderposition.right))); return; } \/\/ fixme [vistall] support other borders? }","code_context_20":"private void borderschanged() { jcomponent component = (jcomponent)toawtcomponent(); component.setborder(jbui.borders.empty()); collection<borderinfo> borders = dataobject().getborders(); map<borderposition, integer> emptyborders = new linkedhashmap<>(); for (borderinfo border : borders) { if (border.getborderstyle() == borderstyle.empty) { emptyborders.put(border.getborderposition(), border.getwidth()); } } if (!emptyborders.isempty()) { component.setborder(jbui.borders.empty(getbordersize(emptyborders, borderposition.top), getbordersize(emptyborders, borderposition.left), getbordersize(emptyborders, borderposition.bottom), getbordersize(emptyborders, borderposition.right))); return; } \/\/ fixme [vistall] support other borders? }","repo":"consulo\/consulo"}
{"id":24543,"comment_id":0,"comment":"\/** * an internal method used to save the stream to the target. * * @param ref * @param fos * @throws jaxbexception *\/","code":"\/** * an internal method used to save the stream to the target. * * @param ref * @param fos * @throws jaxbexception *\/ private void savexml(reference ref, fileoutputstream fos) throws jaxbexception { jaxbcontext ctx = jaxbcontext.newinstance(com.digi_dmx.gen.context.class); marshaller m = ctx.createmarshaller(); m.setproperty(marshaller.jaxb_formatted_output, true); m.setproperty(marshaller.jaxb_encoding, default_encoding); com.digi_dmx.gen.context save = new com.digi_dmx.gen.context(); save.setfactory(ref.getfactoryclassname()); save.setclazz(ref.getclassname()); enumeration<refaddr> all = ref.getall(); while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","classification":"NONSATD","isFinished":true,"code_context_2":"private void savexml(reference ref, fileoutputstream fos) throws jaxbexception { jaxbcontext ctx = jaxbcontext.newinstance(com.digi_dmx.gen.context.class); marshaller m = ctx.createmarshaller(); m.setproperty(marshaller.jaxb_formatted_output, true); m.setproperty(marshaller.jaxb_encoding, default_encoding); com.digi_dmx.gen.context save = new com.digi_dmx.gen.context(); save.setfactory(ref.getfactoryclassname()); save.setclazz(ref.getclassname()); enumeration<refaddr> all = ref.getall(); while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","code_context_10":"private void savexml(reference ref, fileoutputstream fos) throws jaxbexception { jaxbcontext ctx = jaxbcontext.newinstance(com.digi_dmx.gen.context.class); marshaller m = ctx.createmarshaller(); m.setproperty(marshaller.jaxb_formatted_output, true); m.setproperty(marshaller.jaxb_encoding, default_encoding); com.digi_dmx.gen.context save = new com.digi_dmx.gen.context(); save.setfactory(ref.getfactoryclassname()); save.setclazz(ref.getclassname()); enumeration<refaddr> all = ref.getall(); while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","code_context_20":"private void savexml(reference ref, fileoutputstream fos) throws jaxbexception { jaxbcontext ctx = jaxbcontext.newinstance(com.digi_dmx.gen.context.class); marshaller m = ctx.createmarshaller(); m.setproperty(marshaller.jaxb_formatted_output, true); m.setproperty(marshaller.jaxb_encoding, default_encoding); com.digi_dmx.gen.context save = new com.digi_dmx.gen.context(); save.setfactory(ref.getfactoryclassname()); save.setclazz(ref.getclassname()); enumeration<refaddr> all = ref.getall(); while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","repo":"ebardes\/EasyJNDI"}
{"id":24543,"comment_id":1,"comment":"\/\/ this hurts my soul","code":"private void savexml(reference ref, fileoutputstream fos) throws jaxbexception { jaxbcontext ctx = jaxbcontext.newinstance(com.digi_dmx.gen.context.class); marshaller m = ctx.createmarshaller(); m.setproperty(marshaller.jaxb_formatted_output, true); m.setproperty(marshaller.jaxb_encoding, default_encoding); com.digi_dmx.gen.context save = new com.digi_dmx.gen.context(); save.setfactory(ref.getfactoryclassname()); save.setclazz(ref.getclassname()); enumeration<refaddr> all = ref.getall(); while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","classification":"DESIGN","isFinished":true,"code_context_2":"attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos);","code_context_10":"while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","code_context_20":"private void savexml(reference ref, fileoutputstream fos) throws jaxbexception { jaxbcontext ctx = jaxbcontext.newinstance(com.digi_dmx.gen.context.class); marshaller m = ctx.createmarshaller(); m.setproperty(marshaller.jaxb_formatted_output, true); m.setproperty(marshaller.jaxb_encoding, default_encoding); com.digi_dmx.gen.context save = new com.digi_dmx.gen.context(); save.setfactory(ref.getfactoryclassname()); save.setclazz(ref.getclassname()); enumeration<refaddr> all = ref.getall(); while (all.hasmoreelements()) { refaddr refaddr = all.nextelement(); attr attr = new attr(); attr.setname(refaddr.gettype()); object content = refaddr.getcontent(); if (content != null) { attr.setvalue(content.tostring()); } save.addattr(attr); \/\/ this hurts my soul } m.marshal(save, fos); }","repo":"ebardes\/EasyJNDI"}
